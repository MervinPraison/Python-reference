{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Intro\n",
    "- high level overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools of data engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database - mysql, postgresql\n",
    "\n",
    "Data processing - clean, aggregate, join data, scale up processing\n",
    "- ie. Apache Spark, Hive\n",
    "\n",
    "Scheduling - plan jobs with specific intervals, resolve dependency requirements of jobs\n",
    "- Apache Airflow, oozie, simple bash tool like cron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloud computing\n",
    "\n",
    "Cloud service providers - in 2018, AWS 32%, Azure 17%, Google 10%\n",
    "\n",
    "Services - storage, computation, databases\n",
    "\n",
    "1. Storage - AWS S3, Azure Blob Storage, Google Cloud Storage\n",
    "1. Computation - perform calculations, ie. hosting a web server\n",
    "- AWS EC2, Azure Virtual Machines, Google compute engine\n",
    "1. Databases (db) - AWS RDS, Azure SQL db, Google Cloud SQL\n",
    "\n",
    "SQL - Star schema\n",
    "definition - consists of one or more fact tables referencing any number of dimension tables\n",
    "- analytical databases like Redshift optimize\n",
    "- facts = things that happened (ie. product orders)\n",
    "- dimensions = info on the world (ie. customer information)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples: join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample join statment\n",
    "SELECT * FROM \"Customer\"\n",
    "INNER JOIN \"Order\"\n",
    "ON \"customer_id\" = \"Customer\".\"id\";\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL - use pandas to query the db using read_sql() fxn\n",
    "# Complete the SELECT statement\n",
    "data = pd.read_sql(\"\"\"\n",
    "SELECT first_name, last_name FROM \"Customer\"\n",
    "ORDER BY last_name, first_name\n",
    "\"\"\", db_engine)\n",
    "\n",
    "# Show the first 3 rows of the DataFrame\n",
    "print(data.head(3))\n",
    "\n",
    "# Show the info of the DataFrame\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the SELECT statement. Yields df\n",
    "data = pd.read_sql(\"\"\"\n",
    "SELECT * FROM \"Customer\"\n",
    "INNER JOIN \"Order\"\n",
    "ON \"Order\".\"customer_id\"=\"Customer\".\"id\"\n",
    "\"\"\", db_engine)\n",
    "\n",
    "# Show the id column of data\n",
    "print(data.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel computing\n",
    "- gains - memory and processing power (but mostly memory)\n",
    "- risks - overhead due to communication\n",
    "    - task needs to be large\n",
    "    - need several processing units\n",
    "    - parallel slowdown = speed does not increase linearly\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiprocessing.Pool API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: low level code\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# accepts tuple\n",
    "def take_mean_age(year_and_group):\n",
    "    year, group = year_and_group\n",
    "    return pd.DataFrame({\"Age\": group['Age'].mean()}, index=[year])\n",
    "\n",
    "with Pool(4) as p:\n",
    "    results = p.map(take_mean_age, athlete_events.groupby('Year'))\n",
    "    \n",
    "results_df = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply a function over multiple cores\n",
    "@print_timing\n",
    "def parallel_apply(apply_func, groups, nb_cores):\n",
    "    with Pool(nb_cores) as p:\n",
    "        results = p.map(apply_func, groups)\n",
    "    return pd.concat(results)\n",
    "\n",
    "# Parallel apply using 1 core\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 1)\n",
    "\n",
    "# Parallel apply using 2 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 2)\n",
    "\n",
    "# Parallel apply using 4 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 4)\n",
    "\n",
    "'''\n",
    "Compare processing times based on 1, 2, or 4 cores\n",
    "Processing time: 922.2915172576904\n",
    "Processing time: 525.2659320831299\n",
    "Processing time: 317.6119327545166\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dask = high level API for dataframe abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: high level code using dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# partition dataframe into 4\n",
    "athlete_events_dask = dd.from_pandas(athlete_events, npartitions = 4)\n",
    "\n",
    "# run parallel computations on each partition\n",
    "results_df = athlete_events_dask.groupby('Year').Age.mean().compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel computing frameworks\n",
    "- Apache hadoop\n",
    "    1. HDFS - distributed file system\n",
    "        - cloud-managed storage systems like Amazon S3 replace HDFS\n",
    "    1. hadoop MapReduce\n",
    "        - flaws - hard to write MapReduce jobs\n",
    "        - Hive addresses this, uses Hive SQL\n",
    "        - looks like SQL, but converted to MapReduce\n",
    "    1. Spark\n",
    "        - more popular choice for data processing\n",
    "        - advantage - keeps as much processing as possible in memory\n",
    "        - distributes data processing b/n clusters of computers\n",
    "        - Spark architecture relies on RDD (resilient distributed datasets). Data distributed b/n multiple nodes. No named columns. Like list of tuples\n",
    "\n",
    "- Spark\n",
    "    - transformations: .map(), .filter()\n",
    "    - actions: .count(), .first()\n",
    "    - PySpark is the python interface to Spark\n",
    "        - R or Scala also interface to Spark\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "methods\n",
    "- .printSchema(): helps print the schema of a Spark DataFrame.\n",
    "- .groupBy(): grouping statement for an aggregation.\n",
    "- .mean(): take the mean over each group.\n",
    "- .show(): show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using athlete_events_spark df\n",
    "# Print the type of athlete_events_spark\n",
    "print(type(athlete_events_spark))\n",
    "\n",
    "# Print the schema of athlete_events_spark\n",
    "print(athlete_events_spark.printSchema())\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age'))\n",
    "\n",
    "# Group by the Year, and find the mean Age, then show result\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age').show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running PySpark files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a PySpark file using spark-submit. This tool can help you submit your application to a spark cluster.\n",
    "\n",
    "For the sake of this exercise, you're going to work with a local Spark instance running on 4 threads. The file you need to submit is in /home/repl/spark-script.py. \n",
    "\n",
    "Feel free to read the file:\n",
    "cat /home/repl/spark-script.py\n",
    "\n",
    "You can use spark-submit as follows:\n",
    "\n",
    "spark-submit \\\n",
    "  --master local[4] \\\n",
    "  /home/repl/spark-script.py\n",
    "\n",
    "What does this output? Note that it may take a few seconds to get your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out file\n",
    "repl:~$ cat /home/repl/spark-script.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    athlete_events_spark = (spark\n",
    "        .read\n",
    "        .csv(\"/home/repl/datasets/athlete_events.csv\",\n",
    "             header=True,\n",
    "             inferSchema=True,\n",
    "             escape='\"'))\n",
    "\n",
    "    athlete_events_spark = (athlete_events_spark\n",
    "        .withColumn(\"Height\",\n",
    "                    athlete_events_spark.Height.cast(\"integer\")))\n",
    "\n",
    "    print(athlete_events_spark\n",
    "        .groupBy('Year')\n",
    "        .mean('Height')\n",
    "        .orderBy('Year')\n",
    "        .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use spark-submit to run a PySpark file\n",
    "\n",
    "repl:~$ spark-submit \\\n",
    ">   --master local[4] \\\n",
    ">   /home/repl/spark-script.py\n",
    "Picked up _JAVA_OPTIONS: -Xmx512m\n",
    "Picked up _JAVA_OPTIONS: -Xmx512m\n",
    "21/10/27 04:20:15 WARN NativeCodeLoader:Unable to load native-hadoop library foryour platform... using builtin-java classes where applicable\n",
    "+----+------------------+\n",
    "|Year|       avg(Height)|\n",
    "+----+------------------+\n",
    "|1896| 172.7391304347826|\n",
    "|1900|176.63793103448276|\n",
    "|1904| 175.7887323943662|\n",
    "|1906|178.20622568093384|\n",
    "|1908|177.54315789473685|\n",
    "|1912| 177.4479889042996|\n",
    "|1920| 175.7522816166884|\n",
    "|1924|174.96303901437372|\n",
    "|1928| 175.1620512820513|\n",
    "|1932|174.22011541632315|\n",
    "|1936| 175.7239932885906|\n",
    "|1948|176.17279726261762|\n",
    "|1952|174.13893967093236|\n",
    "|1956|173.90096798212957|\n",
    "|1960|173.14128595600675|\n",
    "|1964|  173.448573701557|\n",
    "|1968| 173.9458648072826|\n",
    "|1972|174.56536284096757|\n",
    "|1976|174.92052773737794|\n",
    "|1980|175.52748832195473|\n",
    "+----+------------------+\n",
    "only showing top 20 rows\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow scheduling frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to schedule a pipeline\n",
    "- Manually\n",
    "- cron - Linux scheduling tool\n",
    "- Spotify's Luigi - allows definition of DAGs for complex pipelines\n",
    "- Apache Airflow - developed by Airbnb\n",
    "    - de-facto workflow scheduling framework\n",
    "    \n",
    "\n",
    "Example - 1st job for csv file, 2nd job to pull in and clean from an API, 3rd job joins data from csv and API together\n",
    "\n",
    "DAG = Directed Acyclic Graphs\n",
    "- Visualize dependencies through DAGs\n",
    "- set of nodes - each node is a task to execute\n",
    "- directed edges = dependencies b/n tasks\n",
    "- no cycles\n",
    "\n",
    "Airflow example\n",
    "- First create DAG object using the `DAG` class\n",
    "- Define operations\n",
    "    - Then use an Operator to define each of the jobs. Several kinds of operators exist in Airflow. There are simple ones like BashOperator and PythonOperator that execute bash or Python code, respectively. Then there are ways to write your own operator, like the SparkJobOperator or StartClusterOperator in the example.\n",
    "- Setup dependency flow\n",
    "    - Finally, we define the connections between these operators using `.set_downstream()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow DAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "# Create the DAG object\n",
    "dag = DAG(dag_id=\"example_dag\", ...,\n",
    "          schedule_interval=\"0 * * * *\")\n",
    "\n",
    "# Task definitions\n",
    "start_cluster = StartClusterOperator(task_id=\"start_cluster\", dag=dag)\n",
    "ingest_customer_data = SparkJobOperator(\n",
    "    task_id=\"ingest_customer_data\", dag=dag)\n",
    "ingest_product_data = SparkJobOperator(task_id=\"ingest_product_data\", dag=dag)\n",
    "enrich_customer_data = PythonOperator(task_id=\"enrich_customer_data\", ..., dag=dag)\n",
    "\n",
    "# Complete the downstream flow\n",
    "start_cluster.set_downstream(ingest_customer_data)\n",
    "ingest_customer_data.set_downstream(enrich_customer_data)\n",
    "ingest_product_data.set_downstream(enrich_customer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the DAG needs to run on every hour at minute 0. Fill in the schedule_interval keyword argument using the crontab notation. For example, every hour at minute N would be N * * * *. Remember, you need to run at minute 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DAG object\n",
    "dag = DAG(dag_id=\"car_factory_simulation\",\n",
    "          default_args={\"owner\": \"airflow\",\"start_date\": airflow.utils.dates.days_ago(2)},\n",
    "          schedule_interval=\"0 * * * *\")\n",
    "\n",
    "# Task definitions\n",
    "assemble_frame = BashOperator(task_id=\"assemble_frame\", bash_command='echo \"Assembling frame\"', dag=dag)\n",
    "place_tires = BashOperator(task_id=\"place_tires\", bash_command='echo \"Placing tires\"', dag=dag)\n",
    "assemble_body = BashOperator(task_id=\"assemble_body\", bash_command='echo \"Assembling body\"', dag=dag)\n",
    "apply_paint = BashOperator(task_id=\"apply_paint\", bash_command='echo \"Applying paint\"', dag=dag)\n",
    "\n",
    "# Complete the downstream flow\n",
    "assemble_frame.set_downstream(place_tires)\n",
    "assemble_frame.set_downstream(assemble_body)\n",
    "assemble_body.set_downstream(apply_paint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
