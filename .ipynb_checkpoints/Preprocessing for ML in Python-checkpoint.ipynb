{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for ML in Python\n",
    "1. Intro to Data Preprocessing\n",
    "2. Standardizing Data\n",
    "3. Feature Engineering\n",
    "4. Selecting features for modeling\n",
    "5. Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Intro to Data Preprocessing\n",
    "- after cleaning and EDA\n",
    "- = prepping data for modeling\n",
    "\n",
    "Pandas review:\n",
    "- df.columns\n",
    "- df.dtypes\n",
    "- df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing data\n",
    "\n",
    "# Drop all rows\n",
    "df.dropna()\n",
    "\n",
    "# Drop specific rows (default rows) using index labels\n",
    "df.drop([1,2,3])\n",
    "\n",
    "# Drop specific columns\n",
    "df.drop(\"A\", axis=1)\n",
    "\n",
    "# Subset based on value\n",
    "df[df[\"B\"] == 7]\n",
    "# Subset non-null values\n",
    "df_subset = df[df['column_name'].notnull()]\n",
    "\n",
    "# Count NaN in column \"B\"\n",
    "df[\"B\"].isnull().sum()\n",
    "\n",
    "# Filter out NaN in \"B\" column\n",
    "df[df[\"B\"].notnull()]\n",
    "\n",
    "# drop columns with at least 3 NaN, axes 0/row or 1/column\n",
    "df.dropna(axis=1, thresh=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 pandas data types\n",
    "- object: string/mixed types\n",
    "- int64: integer\n",
    "- float64: float\n",
    "- datetype64 (or timedelta): datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column type\n",
    "df['C'] = df['C'].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Training and Test Sets - Stratified sampling\n",
    "- 100 samples, 80 class 1 and 20 class 2\n",
    "- Training set: 75 samples, 60 class 1 and 15 class 2\n",
    "- Test set: 25 samples, 20 class 1 and 5 class 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sampling with sklearn\n",
    "\n",
    "# check stratification of column/feature\n",
    "y['labels'].value_counts()\n",
    "# note: stratify parameter\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, stratify=y)\n",
    "y_train['labels'].value_counts()\n",
    "y_test['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another stratified sampling example\n",
    "\n",
    "# Create a data with all columns except category_desc\n",
    "volunteer_X = volunteer.drop('category_desc', axis=1)\n",
    "\n",
    "# Create a category_desc labels dataset\n",
    "volunteer_y = volunteer[['category_desc']]\n",
    "\n",
    "# Use stratified sampling to split up the dataset according to the volunteer_y dataset\n",
    "# stratify arg = target feature\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    volunteer_X, volunteer_y, stratify=volunteer_y)\n",
    "\n",
    "# Print out the category_desc counts on the training y labels\n",
    "print(y_train['category_desc'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Standardizing Data\n",
    "- preprocessing method to normalize data\n",
    "\n",
    "2 methods discussed:\n",
    "- log normalization\n",
    "- feature scaling\n",
    "\n",
    "When to standardize\n",
    "- model in linear space (knn, linear regression, kmeans clustering)\n",
    "- features have high variance\n",
    "- features that are continuous and on different scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Log normalization\n",
    "- standardizes data with columns that have high variance\n",
    "- applies log transformation\n",
    "- advantages:\n",
    "    - captures relative changes\n",
    "    - magnitude of change\n",
    "    - keeps positive values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log normalization in python\n",
    "import numpy as np\n",
    "\n",
    "# check dataframe\n",
    "df\n",
    "df.var()\n",
    "\n",
    "# apply log normalization to column 2\n",
    "df['col2_log'] = np.log(df['col2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: log normalization\n",
    "import numpy as np\n",
    "\n",
    "# Print out the variance of the Proline column\n",
    "print(wine['Proline'].var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine['Proline_log'] = np.log(wine['Proline'])\n",
    "\n",
    "# Check the variance of the Proline column again\n",
    "print(wine['Proline_log'].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Scaling data for feature comparison\n",
    "- scaling does: 1. mean=0, 2. variance = 1\n",
    "- normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn has a scaler, but better to use StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_transform()\n",
    "Using this will fit the method to the data and transform in a single step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler()\n",
    "# note: used .fit_transform()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), \n",
    "                         columns=df.columns)\n",
    "print(df_scaled)\n",
    "# variance should be the same\n",
    "print(df.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: scale 3 columns to use in a linear model \n",
    "\n",
    "#Import StandardScaler from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Take a subset of the DataFrame you want to scale \n",
    "wine_subset = wine[['Ash','Alcalinity of ash', 'Magnesium']]\n",
    "\n",
    "# Apply the scaler to the DataFrame subset\n",
    "wine_subset_scaled = ss.fit_transform(wine_subset)ear model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Standardized data and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: k-nearest neighbors on unscaled data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# preprocessing first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "knn = kNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example2: knn on scaled data\n",
    "\n",
    "# Create the scaling method.\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Apply the scaling method to the dataset used for modeling.\n",
    "X_scaled = ss.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data.\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data.\n",
    "print(knn.score(X_test, y_test))\n",
    "\n",
    "# increased accuracy with scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "- Feature engineering = creation of new features based on existing features\n",
    "- gives insight into relationships b/n features\n",
    "- extract and expand data\n",
    "- it is dataset dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Encoding categorical variables - binarization\n",
    "- in pandas: \n",
    "- in scikit learn: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in pandas\n",
    "print(users['subscribed'])\n",
    "\n",
    "users['sub_enc'] = users['subscribed'].apply(\n",
    "    lambda val:1 if val=='y' else 0)\n",
    "\n",
    "# look at columns side by side\n",
    "print(users[['subscribed', 'sub_enc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform() in scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "users['sub_enc_le'] = le.fit_transform(users['subscribed'])\n",
    "\n",
    "print(users[['subscribed', 'sub_enc_le']])\n",
    "\n",
    "# can use this in pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding - use for 2+ variables in a column\n",
    "# use get_dummies()\n",
    "\n",
    "print(users['fav_color'])\n",
    "print(pd.get_dummies(users['fav_color']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: binary encoding categorical variables, .fit_transform()\n",
    "\n",
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking['Accessible_enc'] = enc.fit_transform(hiking['Accessible'])\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking[['Accessible', 'Accessible_enc']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: one-hot encoding for 2+ categories (ie. yes, no, maybe)\n",
    "# get_dummies()\n",
    "\n",
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(volunteer['category_desc'])\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "print(category_enc.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Engineering numerical features\n",
    "- aggregate stats, dates, and how to add value to numerical features\n",
    "- common method of feature engineering: take aggregate of a set of numbers to use in place of features (ie. mean, median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate stats: create a mean column\n",
    "\n",
    "# df of cities on rows, columns are temp for each day\n",
    "print(df)\n",
    "# make a list of columns\n",
    "columns = ['day1','day2','day3']\n",
    "# axis=1 to operate across a row\n",
    "df['mean'] = df.apply(lambda row: row[columns].mean(), axis=1)\n",
    "# creates a single mean value column\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate stats: dates and timestamps\n",
    "\n",
    "# collection of dates\n",
    "print(df)\n",
    "# extract month by converting to a datetime column\n",
    "df['date_converted'] = pd.to_datetime(df['date'])\n",
    "df['month'] = df['date_converted'].apply(lambda row: row.month)\n",
    "print(df)\n",
    "print(df[['date','month']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Text classification - text feature engineering\n",
    "Methods:\n",
    "- extract part of string or number\n",
    "- NLP methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from strings using regular expressions (regex)\n",
    "import re\n",
    "my_string = 'temperature: 75.6 F'\n",
    "\n",
    "# d=digits, + = as many as possible\n",
    "# \\. = period\n",
    "pattern = re.compile('\\d+\\.\\d+')\n",
    "temp = re.match(pattern, my_string)\n",
    "print(float(temp.group(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing text: Tf idf\n",
    "# Tf idf vectors = term frequency + inverse document frequency\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(documents.head())\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "text_tfidf = tfidf_vec.fit_transform(documents)\n",
    "\n",
    "# Naive Bayes text classifier\n",
    "# uses conditional probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: regex string feature extraction\n",
    "\n",
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    pattern = re.compile(r\"\\d+\\.\\d+\")\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.match(pattern, length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking['Length'].apply(lambda row: return_mileage(row))\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: tf/idf string vectorization\n",
    "\n",
    "# Take the title text\n",
    "title_text = volunteer['title']\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
    "\n",
    "# Use vectors to predict 'category_desc' column\n",
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = volunteer[\"category_desc\"]\n",
    "# Note: toarray() method on tf/idf vector for sklearn format\n",
    "# stratify parameter = y, since the class distribution is uneven\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    text_tfidf.toarray(), y, stratify=y)\n",
    "\n",
    "# Fit the model to the training data with Naive Bayes' fit()\n",
    "nb.fit(train_X, train_y)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(test_X, test_y))\n",
    "\n",
    "# out: 0.47096774193548385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Selection for modeling\n",
    "- drop redundant features\n",
    "- work with text vectors\n",
    "- use PCA to reduce number of features and decrease overall variance\n",
    "- iterative process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Removing redundant features\n",
    "- noisy features\n",
    "- strongly correlated features\n",
    "    - linear models assume feature independence\n",
    "    - use Pearson correlation coefficient\n",
    "- duplicated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlated features\n",
    "df.corr()\n",
    "# column A and B have a correlation of 1, so \n",
    "# prob drop one of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: drop redundant features\n",
    "\n",
    "# Create a list of redundant column names to drop\n",
    "to_drop = [\"category_desc\", \"created_date\", \"locality\", \"region\", \"vol_requests\"]\n",
    "\n",
    "# Drop those columns from the dataset\n",
    "volunteer = volunteer.drop(to_drop, axis=1)\n",
    "\n",
    "# Print out the head of the new dataset\n",
    "print(volunteer.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlated features\n",
    "\n",
    "# Print out the column correlations of the wine dataset\n",
    "print(wine.corr())\n",
    "\n",
    "# Take a minute to find the column where the correlation value is greater than 0.75 at least twice\n",
    "to_drop = \"Flavanoids\"\n",
    "\n",
    "# Drop that column from the DataFrame\n",
    "wine = wine.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Selecting features using text vectors\n",
    "- example: you can take something like top 20% of weighted words across the vector\n",
    "- iterate through different subsets of the tf-idf vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out words and weights on a per document basis\n",
    "\n",
    "# view word weights\n",
    "# vector of location descriptions\n",
    "# row data: word weights and index ie. '200': 0,...'ahead': 3\n",
    "print(tfidf_vec.vocabulary_)\n",
    "\n",
    "# view 4th row\n",
    "print(text_tfidf[3].data)\n",
    "# get indices of words that are weighted\n",
    "print(text_tfidf[3].indices)\n",
    "# reverse key value pairs in the vocabulary\n",
    "vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
    "print(vocab)\n",
    "\n",
    "# zip the row indices and weights and turn into a dictionary\n",
    "zipped_row = dict(zip(text_tfidf[3].indices, text_tfidf[3].data))\n",
    "print(zipped_row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function: looking at word weights\n",
    "\n",
    "def return_weights(vocab, vector, vector_index):\n",
    "    \"\"\"\n",
    "    Function performs row zipping to a dictionary.\n",
    "    Return a dictionary mapping the word to its score.\n",
    "    args:\n",
    "        vocab - reversed vocab list\n",
    "        vector - the vector\n",
    "        vector_index - the row we want\n",
    "    \"\"\"\n",
    "    zipped = dict(zip(vector[vector_index].indices,\n",
    "                     vector[vector_index].data))\n",
    "    return {vocab[i]:zipped[i] for i in vector[vector_index].\n",
    "           indices}\n",
    "\n",
    "# pass in reversed vocab list, the text tfidf, and index for 4th row\n",
    "# Result: mapping of scores to words\n",
    "print(return_weights(vocab, text_tfidf, 3))\n",
    "\n",
    "# you can sort by score or eliminate words below a certain threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore text vectors - part 1\n",
    "# Add to return_weights() function and return a list of numbers \n",
    "# within the function.\n",
    "\n",
    "# Add in the rest of the parameters\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, \n",
    "                   top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, \n",
    "                      vector[vector_index].data))\n",
    "    \n",
    "    # Let's transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in \n",
    "                               vector[vector_index].indices})\n",
    "    \n",
    "    # Let's sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[\n",
    "        :top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "# vector_index=8 grabs the 9th row\n",
    "# top_n=3 grabs top 3 weighted words\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, \n",
    "                     text_tfidf, 8, 3))\n",
    "\n",
    "# out\n",
    "[189, 942, 466]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore text vectors - part 2\n",
    "# Write another function to collect the top words across\n",
    "# all documents, extract them, return a list of word indices\n",
    "# and use that list to filter the text_tfidf vector.\n",
    "\n",
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Here we'll call the function from the previous exercise, \n",
    "        # and extend the list we're creating\n",
    "        filtered = return_weights(vocab, original_vocab, \n",
    "                                vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "    # Return the list in a set, so we don't get duplicate word \n",
    "    # indices\n",
    "    return set(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_,\n",
    "text_tfidf, 3)\n",
    "\n",
    "# By converting filtered_words back to a list, \n",
    "# we can use it to filter the columns in the text vector\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]\n",
    "\n",
    "# next, train a model using the filtered vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.a Training Naive Bayes with feature selection\n",
    "- rerun the Naive Bayes text classification model with selection choices from previous exercise on volunteer dataset's title and category_desc columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset according to the class distribution of category_desc, using the filtered_text vector\n",
    "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), volunteer['category_desc'], stratify=y)\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb.fit(train_X, train_y)\n",
    "\n",
    "# Print out the model's accuracy\n",
    "print(nb.score(test_X, test_y))\n",
    "\n",
    "# out\n",
    "# 0.4838709677419355\n",
    "# the title field is a very small text field used to demonstrate\n",
    "# filtering vectors, so accuracy not great here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Dimensionality reduction with PCA\n",
    "- unsupervised learning method\n",
    "- combines/decomposes a feature space\n",
    "- it's a feature extraction method - we'll use it here to reduce the feature space\n",
    "\n",
    "PCA - principal component analysis\n",
    "- linear transformation to uncorrelated space\n",
    "- captures as much variance as possible in each component\n",
    "\n",
    "PCA caveats\n",
    "- difficult to interpret components beyond the ones that explain the most variance\n",
    "- black box method\n",
    "- end of preprocessing journey - difficult to feature work post-pca except eliminating components that don't explain much variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA in scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "df_pca = pca.fit_transform(df)\n",
    "\n",
    "# print out new pca transformed vector \n",
    "print(df_pca)\n",
    "# print explained variance ratio\n",
    "# See the %age of variance explained by the component\n",
    "# Look to drop components that don't explain much of the variance\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PCA\n",
    "# wine dataset to try and improve model accuracy\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set up PCA and the X vector for diminsionality reduction\n",
    "pca = PCA()\n",
    "wine_X = wine.drop(\"Type\", axis=1)\n",
    "\n",
    "# Apply PCA to the wine dataset X vector\n",
    "transformed_X = pca.fit_transform(wine_X)\n",
    "\n",
    "# Look at the percentage of variance explained by the different \n",
    "# components\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# out\n",
    "# [9.98091230e-01 1.73591562e-03 9.49589576e-05 5.02173562e-05\n",
    "#  1.23636847e-05 8.46213034e-06 2.80681456e-06 1.52308053e-06\n",
    "#  1.12783044e-06 7.21415811e-07 3.78060267e-07 2.12013755e-07\n",
    "#  8.25392788e-08]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA - train a model using pca transformed vector\n",
    "\n",
    "# Split the transformed X and the y labels into training and \n",
    "# test sets\n",
    "X_wine_train, X_wine_test, y_wine_train, y_wine_test = \n",
    "train_test_split(transformed_X, y)\n",
    "\n",
    "# Fit knn to the training data\n",
    "knn.fit(X_wine_train, y_wine_train)\n",
    "\n",
    "# Score knn on the test data and print it out\n",
    "knn.score(X_wine_test, y_wine_test)\n",
    "\n",
    "# out\n",
    "# 0.6666666666666666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Putting it all together\n",
    "- Entire preprocessing workflow\n",
    "- UFO dataset and preprocessing\n",
    "\n",
    "Important concepts:\n",
    "- missing data: dropna() and notnull()\n",
    "- types: astype()\n",
    "- stratified sampling: train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.a Check and change column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and change column types\n",
    "\n",
    "# Check the column types\n",
    "print(ufo.dtypes)\n",
    "\n",
    "# Change the type of seconds to int\n",
    "ufo[\"seconds\"] = ufo['seconds'].astype('float')\n",
    "\n",
    "# Change the date column to type datetime\n",
    "ufo[\"date\"] = pd.to_datetime(ufo['date'])\n",
    "\n",
    "# Check the column types\n",
    "print(ufo[['seconds','date']].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.b Drop missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many values are missing in the length_of_time, state, \n",
    "# and type columns\n",
    "print(ufo[['length_of_time', 'state', 'type']].isnull.sum())\n",
    "\n",
    "# Keep only rows where length_of_time, state, and type are not null\n",
    "ufo_no_missing = ufo[ufo[\"length_of_time\"].notnull() & \n",
    "          ufo[\"state\"].notnull() & \n",
    "          ufo[\"type\"].notnull()]\n",
    "\n",
    "# Print out the shape of the new dataset\n",
    "print(ufo_no_missing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Categorical variables and standardization\n",
    "- one-hot encoding with pd.get_dummies()\n",
    "\n",
    "Standardization: \n",
    "- var()\n",
    "- np.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numbers from UFO['length_of_time'] string with regex\n",
    "\n",
    "def return_minutes(time_string):\n",
    "\n",
    "    # Use \\d+ to grab digits\n",
    "    pattern = re.compile(r\"\\d+\")\n",
    "    \n",
    "    # Use match on the pattern and column\n",
    "    num = re.match(pattern, time_string)\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "        \n",
    "# Apply the extraction to the length_of_time column\n",
    "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(lambda val: \n",
    "                                             return_minutes(val))\n",
    "\n",
    "# Take a look at the head of both of the columns\n",
    "print(ufo[['length_of_time','minutes']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify features for standardization by looking at variance\n",
    "\n",
    "# Check the variance of the seconds and minutes columns\n",
    "print(ufo['seconds'].var(), ufo['minutes'].var())\n",
    "# out:\n",
    "# seconds    424087.417474\n",
    "# minutes       117.546372\n",
    "# dtype: float64\n",
    "# note: variance of seconds is really high\n",
    "\n",
    "# Log normalize the seconds column\n",
    "ufo[\"seconds_log\"] = np.log(ufo['seconds'])\n",
    "\n",
    "# Print out the variance of just the seconds_log column\n",
    "print(ufo['seconds_log'].var())\n",
    "# out\n",
    "# 1.1223923881183004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Engineering new features\n",
    "- extract month from date field\n",
    "- extract digits from length_of_time field\n",
    "- vectorize text from desc field\n",
    "\n",
    "Review tips:\n",
    "- dates: .month or .hour attributes\n",
    "- regex: \\d and .group()\n",
    "- text: tf-idf and TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.a Encoding categorical variables\n",
    "- use binary and one-hot code methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to encode us values as 1 and others as 0\n",
    "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda val:\n",
    "                                            1 if val=='us' else 0)\n",
    "\n",
    "# Print the number of unique type values\n",
    "print(len(ufo['type'].unique()))\n",
    "\n",
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo['type'])\n",
    "\n",
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo = pd.concat([ufo, type_set], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.b Features from dates\n",
    "- feature engineering with month and year extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 rows of the date column\n",
    "print(ufo['date'].head(5))\n",
    "\n",
    "# Extract the month from the date column\n",
    "ufo[\"month\"] = ufo[\"date\"].apply(lambda x:x.month)\n",
    "\n",
    "# Extract the year from the date column\n",
    "ufo[\"year\"] = ufo[\"date\"].apply(lambda x:x.year)\n",
    "\n",
    "# Take a look at the head of all three columns\n",
    "print(ufo[['date','month','year']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.c Text vectorization\n",
    "- transform desc column into tf/idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the head of the desc field\n",
    "print(ufo['desc'].head)\n",
    "\n",
    "# Create the tfidf vectorizer object\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "# Use vec's fit_transform method on the desc field\n",
    "desc_tfidf = vec.fit_transform(ufo['desc'])\n",
    "\n",
    "# Look at the number of columns this creates\n",
    "print(desc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Feature selection and modeling\n",
    "- redundant features\n",
    "- text vector\n",
    "- iterate preprocessing and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.a Selecting the ideal dataset\n",
    "Let's get rid of some of the unnecessary features. Because we have an encoded country column, country_enc, keep it and drop other columns related to location: city, country, lat, long, state.\n",
    "\n",
    "We have columns related to month and year, so we don't need the date or recorded columns.\n",
    "\n",
    "We vectorized desc, so we don't need it anymore. For now we'll keep type.\n",
    "\n",
    "We'll keep seconds_log and drop seconds and minutes.\n",
    "\n",
    "Let's also get rid of the length_of_time column, which is unnecessary after extracting minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation between the seconds, seconds_log, and \n",
    "# minutes columns\n",
    "print(ufo[['seconds', 'seconds_log', 'minutes']].corr())\n",
    "\n",
    "# Make a list of features to drop\n",
    "to_drop = ['city','country','lat','long','state','date',\n",
    "'recorded','desc','seconds','minutes','length_of_time']\n",
    "\n",
    "# Drop those features\n",
    "ufo_dropped = ufo.drop(to_drop, axis=1)\n",
    "\n",
    "# Let's also filter some words out of the text vector we created\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_,\n",
    "desc_tfidf, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.b Modeling the UFO dataset - Part 1\n",
    "- build a k-nearest neighbor model to predict which country the UFO sighting took place in\n",
    "- X dataset has the log-normalized seconds column, the one-hot encoded type columns, as well as the month and year when the sighting took place\n",
    "- y labels are the encoded country column, where 1 is us and 0 is ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the features in the X set of data\n",
    "print(X.columns)\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y,\n",
    "stratify=y)\n",
    "\n",
    "# Fit knn to the training sets\n",
    "knn.fit(train_X, train_y)\n",
    "\n",
    "# Print the score of knn on the test sets\n",
    "print(knn.score(test_X, test_y))\n",
    "\n",
    "# out\n",
    "# 0.8843683083511777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.c Modeling the UFO dataset - Part 2\n",
    "- build a model using the text vector we created, desc_tfidf, using the filtered_words list to create a filtered text vector\n",
    "- see if we can predict the type of the sighting based on the text. We'll use a Naive Bayes model for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the list of filtered words we created to filter the text vector\n",
    "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y \n",
    "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
    "\n",
    "# Fit nb to the training sets\n",
    "nb.fit(train_X, train_y)\n",
    "\n",
    "# Print the score of nb on the test sets\n",
    "print(nb.score(test_X, test_y))\n",
    "\n",
    "# out\n",
    "# 0.14989293361884368\n",
    "\n",
    "# since accuracy is poor, need to iterate and figure what\n",
    "# subset of text improves the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
