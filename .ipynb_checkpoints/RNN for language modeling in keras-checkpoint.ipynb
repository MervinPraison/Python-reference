{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks for Language Modeling in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "David Cecchini\n",
    "Data Scientist\n",
    "\n",
    "I am a Data Scientist focusing my work and research on using machine learning on text data. I entered the field when I co-founded a startup company in the field of RegTech that automatically collect, classify and distribute regulations on highly regulated markets, and am currently a Ph.D. student at Tsinghua-Berkeley Shenzhen Institute, a partner program from Tsinghua University from China and UC Berkeley from the USA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction to the course\n",
    " - Hi, my name is David. I'm a Data Scientist that focuses on text data for real world applications, and I am proud to be your instructor in this course where you will be introduced to four different applications of language models using Recurrent Neural Networks with python.\n",
    "\n",
    "2. Text data is available online\n",
    " - So, why learn to model language (or text) data? Well, we know that Data Science models require data to be effective, and one kind of data that is available on the Internet is text. From news articles to tweets, the volume of text data is increasing fast and is freely accessible to anyone with an Internet connection.\n",
    "\n",
    "3. Applications of machine learning to text data\n",
    " - So, what can Data Scientists do with all this data? In this course we will introduce 4 applications: \n",
    "    1. sentiment analysis\n",
    "    2. multi-class classification\n",
    "    3. text generation\n",
    "    4. machine neural translation\n",
    "\n",
    "4. Sentiment analysis\n",
    " - If you have an online customer interaction, you may be interested in knowing how your customers feel towards your brand or product. To do that, you can use sentiment analysis models and classify their messages into positive or negative.\n",
    "\n",
    "5. Multi-class classification\n",
    " - build a recommender system and need to categorize news articles into a set of pre-defined categories.\n",
    "\n",
    "6. Text generation\n",
    " - Also, it is possible to generate text automatically using a specific writing style, or automatically reply to messages.\n",
    "\n",
    "7. Neural machine translation\n",
    " - Lastly, it is also possible to create models that translate from one language to another.\n",
    "\n",
    "8. Recurrent Neural Networks\n",
    " - All these applications are possible with a type of Deep Learning architecture called Recurrent Neural Networks. So what is different about RNN architectures, and why do we use it?\n",
    " - The main advantages to use RNN for text data is that \n",
    "     1. it reduces the number of parameters of the model (by avoiding one-hot encoding) and \n",
    "     2. it shares weights between different positions of the text\n",
    " - In the example, the model uses information from all the words to predict if the movie review was good or not.\n",
    "\n",
    "9. Sequence to sequence models\n",
    " - RNNs model sequence data and can have different lengths of inputs and outputs. \n",
    "    1. Many inputs to One output is commonly used for classification tasks, where the final output is a probability distribution. \n",
    "        - This is used on sentiment analysis and multi-class classification applications.\n",
    "    2. Many inputs to Many outputs: Text Generation\n",
    "        - start the same as in the classification case, but for the outputs, it uses the previous prediction as input to the next prediction.\n",
    "    3. Many inputs to Many outputs: Neural Machine Translation\n",
    "        - is separated in two blocks: encoder and decoder. The encoder learns the characteristics of the input language, while the decoder learns for the output language. The encoder has no prediction (no arrows going up), and the decoder doesn't receive inputs (no arrows from below).\n",
    "    4. Many inputs to Many outputs: language models \n",
    "        - starts with an artificial zero input, and then for every input word i the model tries to predict the next word i plus one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the number of parameter of RNN and ANN\n",
    "In this exercise, you will compare the number of parameters of an artificial neural network (ANN) with the recurrent neural network (RNN) architectures. Here, the vocabulary size is equal to 10,000 for both models.\n",
    "\n",
    "The models have been defined for you with similar architectures of only one layer with 256 units (Dense or RNN) plus the output layer. They are stored on variables ann_model and rnn_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [1]:\n",
    "ann_model.summary()\n",
    "Model: \"ann_model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_1 (Dense)              (None, None, 256)         2560256   \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, None, 1)           257       \n",
    "=================================================================\n",
    "Total params: 2,560,513\n",
    "Trainable params: 2,560,513\n",
    "Non-trainable params: 0\n",
    "################################################\n",
    "    \n",
    "In [2]:\n",
    "rnn_model.summary()\n",
    "Model: \"rnn_model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "simple_rnn_1 (SimpleRNN)     (None, 256)               66048     \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 1)                 257       \n",
    "=================================================================\n",
    "Total params: 66,305\n",
    "Trainable params: 66,305\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "\n",
    "# The RNN model has fewer parameters than the ANN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "In the video exercise, you were exposed to the various applications of sequence to sequence models. In this exercise you will see how to use a pre-trained model for sentiment analysis.\n",
    "\n",
    "The model is pre-loaded in the environment on variable model. Also, the tokenized test set variables X_test and y_test and the pre-processed original text data sentences from IMDb are also available.You will learn how to pre-process the text data and how to create and train the model using Keras later in the course.\n",
    "\n",
    "You will use the pre-trained model to obtain predictions of sentiment. The model returns a number between zero and one representing the probability of the sentence to have a positive sentiment. So, you will create a decision rule to set the prediction to positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first sentence on `X_test`\n",
    "print(X_test[0])\n",
    "\n",
    "# Get the prediction for all the sentences\n",
    "# only takes 1 argument here\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Transform the predition into positive (> 0.5) or negative (<= 0.5)\n",
    "pred_sentiment = [\"positive\" if x>0.5 else \"negative\" for x in pred]\n",
    "\n",
    "# Create a data frame with sentences, predictions and true values\n",
    "result = pd.DataFrame({'sentence': sentences, 'y_pred': pred_sentiment, 'y_true': y_test})\n",
    "\n",
    "# Print the first lines of the data frame\n",
    "print(result.head())\n",
    "\n",
    "'''\n",
    "                                            sentence    y_pred    y_true\n",
    "0  the it of yet br stress and must in at town wh...  positive  negative\n",
    "1  the what have just be ever have 2 at is over d...  negative  positive\n",
    "2  the was me of and in character and performance...  negative  positive\n",
    "3  the as on mean unlike and movie pictures is pa...  negative  negative\n",
    "4  the genuine was capture now and and and new to...  negative  negative\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that some of the predictions were correct and some were not. The model used was very simple and its accuracy was not very high. You will learn later some tuning approaches to the sentiment classification model. Also, the process of pre-processing the text data and creating, training and testing models in Keras will be detailed later in the course. Finally, you created a decision rule to determine if the sentiment would be classified as positive or negative. In many applications, the value of 0.5 is used as decision boundary, but other values can also be used depending on what metric you want to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction to language models\n",
    " - In this lesson, you will learn in more detail how to create a language model from raw text data.\n",
    "\n",
    "2. Sentence probability\n",
    " - Language models represent the probability of a sentence. For example, what is the probability of the sentence I love this movie? What is the probability of each word in this sentence to appear in this particular order? The way this probability is computed changes from one model to another. Unigram models use the probability of each word inside the document, and assume the probabilities are independent. \n",
    " - N-gram models use the probability of each word conditional to the previous N minus one words. \n",
    " - When N equals to 2 it's called bigram, and when it is equal to 3 it's called trigram.\n",
    " - Skipgram model does the opposite, computes the probability of the context words, or neighboring words, given the center word.\n",
    " - Neural networks models with a softmax function in the last layer of the model, output layer, with units equal to the size of the vocabulary are also language models.\n",
    "\n",
    "4. Link to RNNs\n",
    " - We are focusing on Recurrent Neural Networks. So how exactly are language models related to them? Well, everywhere! Recurrent Neural Network models are themselves language models when trained on text data, because they give the probability of the next token given the previous k tokens.\n",
    " - Also, an embedding layer can be used to create vector representations of the tokens as the first layer.\n",
    "\n",
    "6. Building vocabulary dictionaries\n",
    " - When creating RNN models, we need to transform the text data into a sequence of numbers, which are the indexes of the tokens in the array of unique tokens, the vocabulary. To do that, we first need to create an array containing each unique word of the corpus. We can use the combination list-set to create a list of unique words. And we can get all words in a text by splitting the text using space as the separator. Other languages such as Chinese need additional steps to get words since there is no space between the characters. We can now create dictionaries that map words to their index on the vocabulary and vice versa using dictionary comprehension. By enumerating a list, we obtain the numeric indexes and the items as tuples, and we can use them to create key-value dictionaries. The first dictionary uses the words as keys and the indexes as values, it can transform the text into numerical values. The later one is used to go back from numbers to words, since it has indexes as keys and words as values.\n",
    "\n",
    "7. Preprocessing input\n",
    " - With the created dictionaries, we can prepare pairs of X and y to be used on a supervised machine learning model. For that, we can loop into the sequences of numerical indexes in blocks of fixed-length size. We use the initial words as x and the final word as y, and shift the text step words forward. If we use a step equal to 2, it means that the X sentences will be shifted by 2 words at a time.\n",
    "\n",
    "8. Transforming new texts\n",
    " - When preparing new data, we can use the dictionary to get the correct indexes for each word. Using the example on the slide, create a list that will contain the transformed text. Loop for every sentence of the new text create a temporary list that will contain the current sentence. iterate over all words of the sentence by splitting the sentence on it's white spaces. get the index using the dictionary append the index to the sentence list then, append the sentence of indexes on the first list you created, new text split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building vocabulary dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique words\n",
    "unique_words = list(set(text.split(' ')))\n",
    "\n",
    "# create dictionary: word is key, index is value\n",
    "word_to_index = {k:v for (v,k) in enumerate(unique_words)}\n",
    "\n",
    "# create dictionary: index is key, word is value\n",
    "index_to_word = {k:v for (k,v) in enumerate(unique_words)}\n",
    "\n",
    "#####################\n",
    "# preprocessing input\n",
    "#####################\n",
    "# initialize variables X and y\n",
    "X = []\n",
    "y = []\n",
    "# loop over the text: length 'sentence_size' per time with step equal to 'step'\n",
    "# if step=2, then sentences will be shifted 2 words at a time\n",
    "for i in range(0, len(text) - sentence_size, step):\n",
    "    X.append(text[i:i + sentence_size])\n",
    "    y.append(text[i + sentence_size])\n",
    "\n",
    "'''\n",
    "example (numbers are numerical indexes of vocabulary):\n",
    "sentence is: 'i loved this movie' > (['i','loved','this'],'movie')\n",
    "X[0],y[0] = ([10, 444, 11], 17)\n",
    "'''\n",
    "\n",
    "#####################\n",
    "# transforming new texts\n",
    "#####################\n",
    "# create list to keep the sentences of indexes\n",
    "new_text_split = []\n",
    "# loop and get the indexes from dictionary\n",
    "for sentence in new_text:\n",
    "    # temporary list to contain the current sentence\n",
    "    sent_split = []\n",
    "    # split the sentences into words\n",
    "    for wd in sentence.split(' '):\n",
    "        ix = wd_to_index[wd]\n",
    "        sent_split.append(ix)\n",
    "    # append the sentence of indexes on the first list\n",
    "    new_text_split.append(sent_split)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting used to text data\n",
    "In this exercise, you will play with text data by analyzing quotes from Sheldon Cooper in The Big Bang Theory TV show. This will give you a chance to analyze sentences to obtain insights on what it's like to deal with real-world text data.\n",
    "\n",
    "You will use dictionary comprehensions to create dictionaries that map words to indexes and vice versa. The use of dictionaries instead of, for example, a pandas.DataFrame is because they are more intuitive and don't add unnecessary extra complexity.\n",
    "\n",
    "The data is available in sheldon_quotes with the first two sentences already printed for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the list of sentences into a list of words\n",
    "all_words = ' '.join(sheldon_quotes).split(' ')\n",
    "\n",
    "# Get number of unique words\n",
    "unique_words = list(set(all_words))\n",
    "\n",
    "# Dictionary of indexes as keys and words as values\n",
    "index_to_word = {i:wd for i, wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(index_to_word)\n",
    "\n",
    "# Dictionary of words as keys and indexes as values\n",
    "word_to_index = {wd:i for i, wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(word_to_index)\n",
    "'''\n",
    "{0: '(3', 1: 'Ah,', 2: \"Amy's\", 3: 'And', 4: 'Explorer', 5: 'Firefox.', 6: 'For', 7: 'Galileo,', 8: 'Goblin', 9: 'Green', 10: 'Hubble', 11: 'I', 12: \"I'm\", 13: 'Internet', 14: 'Ladybugs', 15: 'Oh', 16: 'Paul', 17: 'Penny', 18: 'Penny!', 19: 'Pope', 20: 'Scissors', 21: 'She', 22: 'Spider-Man,', 23: 'Spock', 24: 'Spock,', 25: 'Thankfully', 26: 'The', 27: 'Two', 28: 'V', 29: 'Well,', 30: 'What', 31: 'Wheaton!', 32: 'Wil', 33: \"You're\", 34: 'a', 35: 'afraid', 36: 'all', 37: 'always', 38: 'am', 39: 'and', 40: 'appeals', 41: 'are', 42: 'art', 43: 'as', 44: 'at', 45: 'aware', 46: 'based', 47: 'be', 48: 'became', 49: 'because', 50: 'been', 51: 'birthday', 52: 'bitch.', 53: 'black', 54: 'blood', 55: 'bottle.', 56: 'bottom', 57: 'brain', 58: 'breaker.', 59: 'bus', 60: 'but', 61: 'calls', 62: 'can', 63: 'care', 64: 'catatonic.', 65: 'center', 66: 'chance', 67: 'circuit', 68: 'computer', 69: 'could', 70: 'covers', 71: 'crushes', 72: 'cry', 73: 'cuts', 74: 'days', 75: 'decapitates', 76: 'deity.', 77: 'discovering', 78: 'disproves', 79: 'do', 80: 'does', 81: \"don't\", 82: 'eat', 83: 'eats', 84: 'every', 85: 'example,', 86: 'flashlight', 87: 'for', 88: 'free', 89: 'genitals,', 90: 'genitals.', 91: 'get', 92: 'ghost', 93: 'girlfriend', 94: 'gravity,', 95: 'had', 96: 'hand.', 97: 'has,', 98: 'have', 99: 'have?', 100: 'having', 101: 'heartless', 102: 'here', 103: 'hole', 104: 'humans', 105: 'if', 106: 'impairment;', 107: 'in', 108: 'insane,', 109: 'insects', 110: 'involves', 111: 'is', 112: \"isn't\", 113: 'it', 114: 'it.', 115: 'just', 116: 'kept', 117: 'knocks)', 118: 'later,', 119: 'little', 120: 'living', 121: 'lizard', 122: 'lizard,', 123: 'loud', 124: 'makes', 125: 'man', 126: 'masturbating', 127: 'me', 128: 'memory', 129: 'messy,', 130: 'money.', 131: 'moon-pie', 132: 'mother', 133: 'moved', 134: 'much', 135: 'must', 136: 'my', 137: 'next', 138: 'not', 139: 'nummy-nummy', 140: 'of', 141: 'on', 142: 'one.', 143: 'other', 144: 'others', 145: 'paper', 146: 'paper,', 147: 'people', 148: 'please', 149: 'poisons', 150: 'present', 151: 'prize', 152: 'relationship', 153: 'render', 154: 'reproduce', 155: 'right', 156: 'rock', 157: 'rock,', 158: 'rushed', 159: 'sad.', 160: 'say', 161: 'scissors', 162: 'scissors,', 163: 'scissors.', 164: 'searching', 165: 'sexual', 166: 'she', 167: 'smashes', 168: 'so', 169: 'sooner', 170: 'stopping', 171: 'stupid,', 172: 'taken', 173: 'telescope', 174: 'tested.', 175: 'that', 176: 'the', 177: 'things', 178: 'think', 179: 'thou', 180: 'three', 181: 'to', 182: 'today', 183: 'town.', 184: 'tried', 185: 'unnecessary', 186: 'unsanitary', 187: 'up.', 188: 'used', 189: 'usually', 190: 'vaporizes', 191: 'vodka', 192: 'way', 193: 'we', 194: 'well,', 195: 'which', 196: 'white', 197: 'will', 198: 'with', 199: 'women,', 200: 'would', 201: 'years,', 202: 'you', 203: 'your'}\n",
    "{'(3': 0, 'Ah,': 1, \"Amy's\": 2, 'And': 3, 'Explorer': 4, 'Firefox.': 5, 'For': 6, 'Galileo,': 7, 'Goblin': 8, 'Green': 9, 'Hubble': 10, 'I': 11, \"I'm\": 12, 'Internet': 13, 'Ladybugs': 14, 'Oh': 15, 'Paul': 16, 'Penny': 17, 'Penny!': 18, 'Pope': 19, 'Scissors': 20, 'She': 21, 'Spider-Man,': 22, 'Spock': 23, 'Spock,': 24, 'Thankfully': 25, 'The': 26, 'Two': 27, 'V': 28, 'Well,': 29, 'What': 30, 'Wheaton!': 31, 'Wil': 32, \"You're\": 33, 'a': 34, 'afraid': 35, 'all': 36, 'always': 37, 'am': 38, 'and': 39, 'appeals': 40, 'are': 41, 'art': 42, 'as': 43, 'at': 44, 'aware': 45, 'based': 46, 'be': 47, 'became': 48, 'because': 49, 'been': 50, 'birthday': 51, 'bitch.': 52, 'black': 53, 'blood': 54, 'bottle.': 55, 'bottom': 56, 'brain': 57, 'breaker.': 58, 'bus': 59, 'but': 60, 'calls': 61, 'can': 62, 'care': 63, 'catatonic.': 64, 'center': 65, 'chance': 66, 'circuit': 67, 'computer': 68, 'could': 69, 'covers': 70, 'crushes': 71, 'cry': 72, 'cuts': 73, 'days': 74, 'decapitates': 75, 'deity.': 76, 'discovering': 77, 'disproves': 78, 'do': 79, 'does': 80, \"don't\": 81, 'eat': 82, 'eats': 83, 'every': 84, 'example,': 85, 'flashlight': 86, 'for': 87, 'free': 88, 'genitals,': 89, 'genitals.': 90, 'get': 91, 'ghost': 92, 'girlfriend': 93, 'gravity,': 94, 'had': 95, 'hand.': 96, 'has,': 97, 'have': 98, 'have?': 99, 'having': 100, 'heartless': 101, 'here': 102, 'hole': 103, 'humans': 104, 'if': 105, 'impairment;': 106, 'in': 107, 'insane,': 108, 'insects': 109, 'involves': 110, 'is': 111, \"isn't\": 112, 'it': 113, 'it.': 114, 'just': 115, 'kept': 116, 'knocks)': 117, 'later,': 118, 'little': 119, 'living': 120, 'lizard': 121, 'lizard,': 122, 'loud': 123, 'makes': 124, 'man': 125, 'masturbating': 126, 'me': 127, 'memory': 128, 'messy,': 129, 'money.': 130, 'moon-pie': 131, 'mother': 132, 'moved': 133, 'much': 134, 'must': 135, 'my': 136, 'next': 137, 'not': 138, 'nummy-nummy': 139, 'of': 140, 'on': 141, 'one.': 142, 'other': 143, 'others': 144, 'paper': 145, 'paper,': 146, 'people': 147, 'please': 148, 'poisons': 149, 'present': 150, 'prize': 151, 'relationship': 152, 'render': 153, 'reproduce': 154, 'right': 155, 'rock': 156, 'rock,': 157, 'rushed': 158, 'sad.': 159, 'say': 160, 'scissors': 161, 'scissors,': 162, 'scissors.': 163, 'searching': 164, 'sexual': 165, 'she': 166, 'smashes': 167, 'so': 168, 'sooner': 169, 'stopping': 170, 'stupid,': 171, 'taken': 172, 'telescope': 173, 'tested.': 174, 'that': 175, 'the': 176, 'things': 177, 'think': 178, 'thou': 179, 'three': 180, 'to': 181, 'today': 182, 'town.': 183, 'tried': 184, 'unnecessary': 185, 'unsanitary': 186, 'up.': 187, 'used': 188, 'usually': 189, 'vaporizes': 190, 'vodka': 191, 'way': 192, 'we': 193, 'well,': 194, 'which': 195, 'white': 196, 'will': 197, 'with': 198, 'women,': 199, 'would': 200, 'years,': 201, 'you': 202, 'your': 203}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing text data for model input\n",
    "Previously, you learned how to create dictionaries of indexes to words and vice versa. In this exercise, you will split the text by characters and continue to prepare the data for supervised learning.\n",
    "\n",
    "Splitting the texts into characters may seem strange, but it is often done for text generation. Also, the process to prepare the data is the same, the only change is how to split the texts.\n",
    "\n",
    "You will create the training data containing a list of fixed-length texts and their labels, which are the corresponding next characters.\n",
    "\n",
    "You will continue to use the dataset containing quotes from Sheldon (The Big Bang Theory), available in the sheldon_quotes variable.\n",
    "\n",
    "The print_examples() function print the pairs so you can see how the data was transformed. Use help() for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to keep the sentences and the next character\n",
    "sentences = []   # ~ Training data\n",
    "next_chars = []  # ~ Training labels\n",
    "\n",
    "# Define hyperparameters\n",
    "step = 2          # ~ Step to take when reading the texts in characters\n",
    "chars_window = 10 # ~ Number of characters to use to predict the next one  \n",
    "\n",
    "# Loop over the text: length `chars_window` per time with step equal to `step`\n",
    "for i in range(0, len(sheldon_quotes) - chars_window, step):\n",
    "    sentences.append(sheldon_quotes[i:i + chars_window])\n",
    "    next_chars.append(sheldon_quotes[i + chars_window])\n",
    "\n",
    "# Print 10 pairs using function\n",
    "print_examples(sentences, next_chars, 10)\n",
    "'''\n",
    "Sentence\tNext char\n",
    "You're afr\ta\n",
    "u're afrai\td\n",
    "re afraid \to\n",
    " afraid of\t \n",
    "fraid of i\tn\n",
    "aid of ins\te\n",
    "d of insec\tt\n",
    "of insects\t \n",
    " insects a\tn\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this you are ready to use the sentences and next character to train a supervised learning model! \n",
    "\n",
    "Don't mind that the printed sentences look strange, since you used characters instead of words and defined a sentence with a fixed length, the texts can be broken in the middle of a word. \n",
    "\n",
    "Note that the process of creating the sentences and next chars is the same when using words instead of characters, the only change being the values present on the lists (words instead of characters). \n",
    "\n",
    "Now, before going straight to training machine learning models, let's see what to do when you have a new text data not pre-processed yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T03:19:59.097025Z",
     "start_time": "2021-09-27T03:19:59.085319Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In this exercise, you will transform a new text into sequences of \n",
    "numerical indexes on the dictionaries created before.\n",
    "\n",
    "This is useful when you already have a trained model and want to \n",
    "apply it on a new dataset. The preprocessing steps done on the \n",
    "training data should also be applied to the new text, \n",
    "so the model can make predictions/classifications.\n",
    "\n",
    "Here, you will also use a special token \"<UKN/>\" to represent words\n",
    "that are not in the vocabulary. Typically, these special tokens \n",
    "are the first indexes of the dictionaries, the position 0.\n",
    "\n",
    "The variables word_to_index, index_to_word and vocabulary are \n",
    "already loaded in the environment. Also, the variable with the \n",
    "new text is also loaded as new_text. The new text has been printed \n",
    "for you to have a look.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the sentences and get indexes\n",
    "new_text_split = []\n",
    "for sentence in new_text:\n",
    "    sent_split = []\n",
    "    for wd in sentence.split(' '):\n",
    "        index = word_to_index.get(wd, 0)\n",
    "        sent_split.append(index)\n",
    "    new_text_split.append(sent_split)\n",
    "\n",
    "# Print the first sentence's indexes\n",
    "print(new_text_split[0])\n",
    "\n",
    "# Print the sentence converted using the dictionary\n",
    "print(' '.join([index_to_word[index] for index in new_text_split[0]]))\n",
    "\n",
    "'''\n",
    "[276, 15070, 10160, 14750, 14590, 5715, 13813, 12418, 22564, 12797, 15443, 13813, 0, 5368, 14578, 13813, 16947, 12507, 23031, 12859, 5975, 16795, 13813, 5368, 21189, 22564, 0, 5910]\n",
    "A man either lives life as it happens to him meets it <UKN/> and licks it or he turns his back on it and starts to <UKN/> away\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can see that some of the words were not found on the dictionary \n",
    "and have index = 0. By using the token '<UKN/>' in the training phase,\n",
    "you can easily use the model on unseen data without getting errors.\n",
    "This is also done when limiting the size of the vocabulary, say to \n",
    "5,000 most frequent words, and setting the others as '<UKN/>'.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to RNN inside Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction to RNN inside Keras\n",
    " - In this lesson, we implement the RNN models using keras. Previously, you were introduced to the architecture of language models. Now we will use keras to create and train RNN models.\n",
    "\n",
    "2. What is keras?\n",
    " - Keras is a high-level API with deep learning frameworks as background. \n",
    " - It is possible to configure keras with Tensorflow, CNTK or Theano.\n",
    " - To install keras, we can simply use the Python package manager pip. After installation, we can use its modules to execute fast experimentation and research. Next we will introduce the main modules of keras that will be useful for the language models\n",
    "\n",
    "3. keras.models\n",
    " - keras models contain two classes of models. The Sequential class has a structure where each layer is implemented one after the other, meaning that the output of one layer is the input of the next one. The Model class is a generic definition of a model that is more flexible and allows multiple inputs and outputs.\n",
    "\n",
    "4. keras.layers\n",
    " - keras layers contains the different types of layers including the \n",
    "     - LSTM cells\n",
    "     - GRU cells\n",
    "     - Dense\n",
    "     - Dropout\n",
    "     - Embedding\n",
    "     - Bidirectional\n",
    "\n",
    "5. keras.preprocessing\n",
    " - keras preprocessing contains useful functions for pre-processing the data such as the pad_sequences method that transforms text data into fixed-length vectors. In the example, we padded the texts to equal length of 3.\n",
    " - ie. keras.preprocessing.sequence.pad_sequences(texts, maxlen=3)\n",
    "\n",
    "6. keras.datasets\n",
    " - The datasets module contains useful datasets. \n",
    " - imdb movie reviews that is used for sentiment analysis\n",
    " - reuters newswire dataset used for topic classification with 46 classes\n",
    " - other datasets that you can check on the keras website.\n",
    "\n",
    "7. Creating a model\n",
    " - You can build a Sequential model in keras with just a few lines of code. Import the required classes as: from keras dot models import sequential from keras dot layers import dense Then, instantiate the class in the variable called model with: model equals to sequential open and close parenthesis. Add desired layers with the method add as in: model dot add dense 64, activation equals to the string relu, input_dim equals to 100 The parameter input dim declares the shape of the input data, which is mandatory for the first layer in the model. Then add the output layer: model dot add dense 1, activation equal to the string sigmoid Finally, we compile the model by executing the compile method of the class. We pass the string adam to the optimizer parameter, the string mean squared error to loss, and a single-element list containing the string accuracy to the metrics parameter\n",
    "\n",
    "8. Training the model\n",
    " - To train the model, we use the fit method on the training data. For example: model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    " - epochs is the number of iterations over the entire dataset and defaults to one. \n",
    " - batch size is the size of a subset of the data that will be used on each step. When the dataset cannot fit in the memory this is crucial. It defaults to 32.\n",
    "\n",
    "9. Model evaluation and usage\n",
    " - To analyze the model's performance, we can use the method evaluate as model dot evaluate x-test comma y-test This method returns the loss and accuracy values. To use the model on new data, use the method predict as: model dot predict new_data\n",
    "\n",
    "10. Full example: IMDB Sentiment Classification\n",
    " - To create a full example, let's instantiate the Sequential class, add three layers (don't bother with new layers for now, we will explain them in details on chapter 2) and compile. Next, we can use the training set to fit the model. And measure its accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-27T03:53:25.620348Z",
     "start_time": "2021-09-27T03:53:25.464994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "# import required modules\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# instantiate the model class\n",
    "model = Sequential()\n",
    "\n",
    "# add the layers\n",
    "model.add(Dense(64, activation='relu', input_dim=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# training the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# evaluate the model, yields lss score and accuracy\n",
    "model.evalaute(X_test, y_test)\n",
    "\n",
    "# make predictions on new data\n",
    "model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example - IMDB sentiment classification\n",
    "# build and compile the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 128))\n",
    "model.add(LSTM(128, dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# training\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# evaluation\n",
    "score, acc = model.evaluation(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you'll practice using two classes from the keras.models module. You will create one model using the two classes Sequential and Model.\n",
    "\n",
    "The Sequential class is easier since the layers are assumed to be in order, while the Model class is more flexible and allows multiple inputs, multiple outputs and shared layers (shared weights).\n",
    "\n",
    "The Model class needs to explicitly declare the input layer, while in the Sequential class, this is done with the input_shape parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example - create one model with 2 classes - Sequential and Model\n",
    "# Instantiate the class\n",
    "model = Sequential(name=\"sequential_model\")\n",
    "\n",
    "# One LSTM layer (defining the input shape because it is the \n",
    "# initial layer)\n",
    "model.add(LSTM(128, input_shape=(None, 10), name=\"LSTM\"))\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "model.add(Dense(1, activation=\"sigmoid\", name=\"output\"))\n",
    "\n",
    "# The summary shows the layers and the number of parameters \n",
    "# that will be trained\n",
    "model.summary()\n",
    "'''\n",
    "Model: \"sequential_1\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "LSTM (LSTM)                  (None, 128)               71168     \n",
    "_________________________________________________________________\n",
    "output (Dense)               (None, 1)                 129       \n",
    "=================================================================\n",
    "Total params: 71,297\n",
    "Trainable params: 71,297\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "'''\n",
    "# Define the input layer\n",
    "main_input = Input(shape=(None, 10), name=\"input\")\n",
    "\n",
    "# One LSTM layer (input shape is already defined)\n",
    "lstm_layer = LSTM(128, name=\"LSTM\")(main_input)\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "main_output = Dense(1, activation=\"sigmoid\", name=\"output\")(lstm_layer)\n",
    "\n",
    "# Instantiate the class at the end\n",
    "model = Model(inputs=main_input, outputs=main_output, name=\"modelclass_model\")\n",
    "\n",
    "# Same amount of parameters to train as before (71,297)\n",
    "model.summary()\n",
    "'''\n",
    "Model: \"modelclass_model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input (InputLayer)           (None, None, 10)          0         \n",
    "_________________________________________________________________\n",
    "LSTM (LSTM)                  (None, 128)               71168     \n",
    "_________________________________________________________________\n",
    "output (Dense)               (None, 1)                 129       \n",
    "=================================================================\n",
    "Total params: 71,297\n",
    "Trainable params: 71,297\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keras.models.Sequential is very easy to use to add layers in sequence. \n",
    "\n",
    "On the other hand, the keras.models.Model class is very flexible and is usually the choice when scientists need deep customization in their solution. Also, you saw how one layer is connected to another layer in both cases, by adding them in sequence using the method add, or by creating a layer and calling the desired (previous) layer like a function, in the Model class API, every layer is callable on a tensor and always return a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras preprocessing\n",
    "The second most important module of Keras is keras.preprocessing. You will see how to use the most important modules and functions to prepare raw data to the correct input shape. Keras provides functionalities that substitute the dictionary approach you learned before.\n",
    "\n",
    "You will use the module keras.preprocessing.text.Tokenizer to create a dictionary of words using the method .fit_on_texts() and change the texts into numerical ids representing the index of each word on the dictionary using the method .texts_to_sequences().\n",
    "\n",
    "Then, use the function .pad_sequences() from keras.preprocessing.sequence to make all the sequences have the same size (necessary for the model) by adding zeros on the small texts and cutting the big ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant classes/functions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# In [1]:\n",
    "# texts\n",
    "# Out[1]:\n",
    "# array(['So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.',\n",
    "#        'Hello, female children. Allow me to inspire you with a story about a great female scientist. Polish-born, French-educated Madame Curie. Co-discoverer of radioactivity, she was a hero of science, until her hair fell out, her vomit and stool became filled with blood, and she was poisoned to death by her own discovery. With a little hard work, I see no reason why that can’t happen to any of you. Are we done? Can we go?'],\n",
    "#       dtype='<U419')\n",
    "\n",
    "# Build the dictionary of indexes\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Change texts into sequence of indexes\n",
    "texts_numeric = tokenizer.texts_to_sequences(texts)\n",
    "print(\"Number of words in the sample texts: ({0}, {1})\".format(\n",
    "    len(texts_numeric[0]), len(texts_numeric[1])))\n",
    "\n",
    "# Pad the sequences\n",
    "texts_pad = pad_sequences(texts_numeric, 60)\n",
    "print(\"Now the texts have fixed length: 60. Let's see the first one: \\n{0}\".format(\n",
    "    texts_pad[0]))\n",
    "'''\n",
    "Number of words in the sample texts: (54, 78)\n",
    "Now the texts have fixed length: 60. Let's see the first one: \n",
    "[ 0  0  0  0  0  0 24  4  1 25 13 26  5  1 14  3 27  6 28  2  7 29 30 13\n",
    " 15  2  8 16 17  5 18  6  4  9 31  2  8 32  4  9 15 33  9 34 35 14 36 37\n",
    "  2 38 39 40  2  8 16 41 42  5 18  6]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your first RNN model\n",
    "In this exercise you will put in practice the Keras modules to build your first RNN model and use it to classify sentiment on movie reviews.\n",
    "\n",
    "This first model has one recurrent layer with the vanilla RNN cell: SimpleRNN, and the output layer with two possible values: 0 representing negative sentiment and 1 representing positive sentiment.\n",
    "\n",
    "You will use the IMDB dataset contained in keras.datasets. A model was already trained and its weights stored in the file model_weights.h5. You will build the model's architecture and use the pre-loaded variables x_test and y_test to check the its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model to classify sentiment on movie reviews\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=128, input_shape=(None, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Method '.evaluate()' shows the loss and accuracy\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Loss: {0} \\nAccuracy: {1}\".format(loss, acc))\n",
    "\n",
    "# Loss: 0.6991182217597961 \n",
    "# Accuracy: 0.495\n",
    "\n",
    "# note accuracy is very low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing and Exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Vanishing and exploding gradients\n",
    " - You learned how to prepare text documents and use them on a RNN model to classify sentiment on movie reviews. But, the accuracy was not as expected! In this lesson you will be introduced to some pitfalls of vanilla RNN cells, which are the vanishing or exploding gradient problems, and how to deal with them.\n",
    "\n",
    "2. Training RNN models\n",
    " - To understand the vanishing or exploding gradient problems, you first need to understand how the RNN model is trained. In other words, how to perform back propagation. In this picture, you can see the forward propagation and back propagation directions. The important part here is that the they follow two directions: vertical (between input and output) and horizontal (going through time) . Because of this horizontal direction, back propagation is referred as back propagation through time.\n",
    "\n",
    "3. Forward propagation\n",
    " - In the forward propagation phase, we compute a hidden state a that will carry past information by applying the linear combination over the previous step and the current input. The output y is computed only in the last hidden state often by applying a sigmoid or softmax activation function. The loss function can be the cross-entropy function and we use it to have a numeric value of the error. We can see that the past information is carried out during the forward propagation with an example. The second step combines the results from the first step, and receive the second word as input. We can also see that the weight matrix Wa is used on all steps, which means the weights are shared among all the inputs.\n",
    "\n",
    "4. Back propagation through time (BPTT)\n",
    " - In the back propagation phase, we have to compute the derivatives of the loss function with respect to the parameters. To compute the derivative of the loss with respect to the matrix Wa, we need to use the chain rule because y hat depends on a_t which also depends on Wa. But, a_t also depends on a_t minus 1 that depends on Wa. Thus, we need to consider the contribution of every previous step by summing up their derivatives with respect to the matrix Wa. Also, the derivative of at with respect to Wa also need the chain rule of derivatives and can be written as the product of the intermediate states multiplied by the derivative of the first state with respect to the matrix.\n",
    "\n",
    "5. BPTT continuation\n",
    " - Not going into too much detail on the math, when computing the gradients of the loss function with respect to the weight matrix we obtain the matrix Wa power t minus one multiplied by a term. Intuitively, if the values of the matrix are below one, the series will converge to zero, and if its values are above one it will diverge to infinity.\n",
    "\n",
    "6. Solutions to the gradient problems\n",
    " - Researchers found some approaches to avoid these problems. \n",
    " - Dealing with Exploding gradients\n",
    "     - Gradient clipping (limiting the size of the gradients) or scaling\n",
    " - Dealing with Vanishing gradients\n",
    "     - Initializing the matrix W as an orthogonal matrix makes their multiplication always be equal to one \n",
    "     - Using regularization controls the size of the entries\n",
    "     - Using the ReLU activation function (instead of tanh, sigmoid, softmax), the derivative becomes a constant, and thus doesn't increase or decrease exponentially \n",
    "     - use other RNN cells such as GRU and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploding gradient problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the video exercise, you learned about two problems that may arise when working with RNN models: the vanishing and exploding gradient problems.\n",
    "\n",
    "This exercise explores the exploding gradient problem, showing that the derivative of a function can increase exponentially, and how to solve it with a simple technique.\n",
    "\n",
    "The data is already loaded on the environment as X_train, X_test, y_train and y_test.\n",
    "\n",
    "You will use a Stochastic Gradient Descent (SGD) optimizer and Mean Squared Error (MSE) as the loss function.\n",
    "\n",
    "In the first step you will observe the gradient exploding by computing the MSE on the train and test sets. On step 2, you will change the optimizer using the clipvalue parameter to solve the problem.\n",
    "\n",
    "The Stochastic Gradient Descent in Keras is loaded as SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding gradient example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras model with one hidden Dense layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu',\n",
    "                kernel_initializer=he_uniform(seed=42)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9))\n",
    "history = model.fit(X_train, y_train, validation_data=(\n",
    "    X_test, y_test), epochs=100, verbose=0)\n",
    "\n",
    "# See Mean Square Error for train and test data\n",
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the values of MSE\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "\n",
    "# Train: nan, Test: nan # >> Gradients exploded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding gradient problem solved with gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the SGD() clipvalue param = 3.0, then run again\n",
    "\n",
    "# Create a Keras model with one hidden Dense layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile and fit the model\n",
    "# NOTE - clipvalue added to solve gradient exploding problem\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9, clipvalue=3.0))\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n",
    "\n",
    "# See Mean Square Error for train and test data\n",
    "train_mse = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the values of MSE\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "\n",
    "# Train: 73.888, Test: 100.110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing gradient problem\n",
    "The other possible gradient problem is when the gradients vanish, or go to zero. This is a much harder problem to solve because it is not as easy to detect. If the loss function does not improve on every step, is it because the gradients went to zero and thus didn't update the weights? Or is it because the model is not able to learn?\n",
    "\n",
    "This problem occurs more often in RNN models when long memory is required, meaning having long sentences.\n",
    "\n",
    "In this exercise you will observe the problem on the IMDB data, with longer sentences selected. The data is loaded in X and y variables, as well as classes Sequential, SimpleRNN, Dense and matplotlib.pyplot as plt. The model was pre-trained with 100 epochs and its weights are stored on the file model_weights.h5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=600, input_shape=(None, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Plot the accuracy x epoch graph\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe that at some point the accuracy stopped to improve, which can happen because of the vanishing gradient problem. This kind of problem is harder to detect than the exploding gradient problem and will demand deeper analysis by the data scientist. Researchers found a model architecture way to solve this problem, which you will study later in this course. Instead of using SimpleRNN cells, you can use the more complex ones such as Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU and LSTM cells\n",
    "- models architectures that help solve the vanishing gradient problem\n",
    "- Gated Recurrent Unit (GRU) cells\n",
    "- Long Short-Term Memory (LSTM) cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. GRU and LSTM cells\n",
    " - In this lesson you will learn about two different RNN cells that will achieve good results in language modeling and solve the vanishing gradient problem.\n",
    "\n",
    "2. SimpleRNN cell in detail\n",
    " - Let's first have a detailed look of the SimpleRNN cell. On every cell, we compute the new memory state based on the previous memory state t minus one and the current input word Xt. In the computations, we have a weight matrix Wa that is shared between all steps. We will consider the case of classification tasks and thus the output y hat will be computed only in the last step.\n",
    "\n",
    "3. GRU cell - add Update gate to RNN\n",
    " - GRU cells were proposed in 2014, and add one gate to the vanilla RNN cell. Now before updating the memory cell, we first compute a candidate a-tilde that will carry the present information. Then we compute the update gate GU that will determine if the candidate a tilde will be used as memory state or if we keep the past memory state a minus one. If the gate is zero, the network keeps the previous hidden state, and if it is equal to one it uses the new value of a tilde. Other values will be a combination of the previous and the candidate memory state, but during training it tends to get close to zero or one.\n",
    "\n",
    "4. LSTM cell - adds 3 gates - forget gate, update gate, output gate\n",
    " - LSTM was first proposed in 1997, and adds three gates to the vanilla RNN cell. The forget gate g_f determines if the previous state c_t minus one state should be forgotten (meaning to have its value set to zero) or not. The update gate g_u do the same for the candidate hidden state c tilde. The output gate g_o do the same for the new hidden state c_t. The green circles on the picture represent the gates. We can think of them as an open or closed gate, allowing for the left side to pass through or not if the gates value are 0 or 1 respectively.\n",
    "\n",
    "5. No more vanishing gradients\n",
    " - Because GRU and LSTM cells add gates to the equations, the gradients are no longer only dependent on the memory cell state. \n",
    " - The derivative of the loss function with respect to the weights matrix depends on all the gates and on the memory cell, summing each of its parts. \n",
    " - Without going into deeper details on the math, this architecture adds the different gradients (corresponding to the gradients of each gate and the memory state), making the total gradient stop converging to zero or diverging. On every step, if the gradient is exponentially increasing or decreasing, we expect the training phase to adjust the value of the corresponding gate accordingly to stop this vanishing or exploding tendency.\n",
    "\n",
    "6. Usage in keras\n",
    " - Without further discussing the intuition and the theory, let's put the new RNN cells in practice inside keras. First, the layers with the GRU and LSTM cells are available in the keras dot layers dot recurrent, with a shortcut on keras dot layers. To use the GRU and LSTM cells on a keras model, we simple add them as usual. The important parameters are the number of units, meaning the number of memory cells to keep track, and the return sequences parameter that is used when adding more than one layer in sequence, making all the cells to emit an output that will be fed to the next layer as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "# import the layers\n",
    "from keras.layers import GRU, LSTM\n",
    "\n",
    "# add the layers to a model\n",
    "# note return_sequences True for first layer, but False for last LSTM layer\n",
    "model.add(GRU(units=128, return_sequences=True, name='GRU layer'))\n",
    "model.add(LSTM(units=64, return_sequences=False, name='LSTM layer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU cells are better than simpleRNN\n",
    "In this exercise you will re-run the same model as the first chapter of the course to compare the accuracy of the model by simpling changing the SimpleRNN cell to a GRU cell.\n",
    "\n",
    "The model was already trained with 10 epochs, as in the previous model with a SimpleRNN cell. In order to compare the models, a test set (x_test, y_test) is already loaded in the environment, as well as the old model SimpleRNN_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules\n",
    "from keras.layers import GRU, Dense\n",
    "\n",
    "# Print the old and new model summaries\n",
    "SimpleRNN_model.summary()\n",
    "gru_model.summary()\n",
    "\n",
    "# Evaluate the models' performance (ignore the loss value)\n",
    "_, acc_simpleRNN = SimpleRNN_model.evaluate(X_test, y_test, verbose=0)\n",
    "_, acc_GRU = gru_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the results\n",
    "print(\"SimpleRNN model's accuracy:\\t{0}\".format(acc_simpleRNN))\n",
    "print(\"GRU model's accuracy:\\t{0}\".format(acc_GRU))\n",
    "'''\n",
    "Model: \"simple_rnn_model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "simple_rnn_1 (SimpleRNN)     (None, 128)               16640     \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 1)                 129       \n",
    "=================================================================\n",
    "Total params: 16,769\n",
    "Trainable params: 16,769\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Model: \"gru_model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "gru_1 (GRU)                  (None, 128)               49920     \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 1)                 129       \n",
    "=================================================================\n",
    "Total params: 50,049\n",
    "Trainable params: 50,049\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "SimpleRNN model's accuracy: 0.495\n",
    "GRU model's accuracy: 0.58\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking RNN layers\n",
    "Deep RNN models can have tens to hundreds of layers in order to achieve state-of-the-art results.\n",
    "\n",
    "In this exercise, you will get a glimpse of how to create deep RNN models by stacking layers of LSTM cells one after the other.\n",
    "\n",
    "To do this, you will set the return_sequences argument to True on the firsts two LSTM layers and to False on the last LSTM layer.\n",
    "\n",
    "To create models with even more layers, you can keep adding them one after the other or create a function that uses the .add() method inside a loop to add many layers with few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LSTM layer\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=(None, 1), return_sequences=True))\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('lstm_stack_model_weights.h5')\n",
    "\n",
    "print(\"Loss: %0.04f\\nAccuracy: %0.04f\" % tuple(model.evaluate(X_test, y_test, verbose=0)))\n",
    "# Loss: 0.6789\n",
    "# Accuracy: 0.5590"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer - for transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Embedding layer\n",
    " - You will learn now about vectorization of a language model using the embedding layer in keras, and how it can be used for transfer learning.\n",
    "\n",
    "2. Why embeddings\n",
    " - Advantages\n",
    "     - Reduce the dimension\n",
    "         - The first reason to use embeddings is because the one-hot encoding of the tokens in a scenario with a very big vocabulary (maybe 100 thousands words) demands a lot of memory. An embedding layer with dimension, say, 300 is more viable. \n",
    "         - one_hot = np.array((N, 100000))\n",
    "         - embedd = np.array((N, 300))\n",
    "     - Dense representation\n",
    "         - Also, embeddings are a dense representations of the words, and the implementations gives surprisingly nice understanding of the tokens. Like the famous king - man + woman = queen. \n",
    "     - Transfer learning\n",
    "\n",
    " - Disadvantages\n",
    "     - it demands training lots of parameters to learn this representation\n",
    "     - can make training slower\n",
    "\n",
    "3. How to use in keras\n",
    " - To use the embedding layer in keras, we first import it from keras dot layers module. The embedding layer should be the first layer of the model. The relevant parameters include: input dim, which is the size of the vocabulary output dim, which is the dimension of the embedding space trainable, that defines if this layer should have its weights updated or not during the training phase embedding initializer, that can be used to perform transfer learning by using pre-trained weights for the words in your vocabulary. Often, when using transfer learning we set trainable to False, but it is not mandatory. The final parameter is the input length, which determines the size of the sequences (it assumes that you padded the input sentences beforehand)\n",
    "\n",
    "4. Transfer learning\n",
    " - There are many pre-trained vectors that were trained on big datasets such as the Wikipedia, news articles, etc. To train a model on those big sets demand a lot of computer power, but loading the weights does not! Recent advances in NLP and language models research is based on open sourcing pre-trained weights on big datasets using popular models such as glove, word to vec and bert, among others. In keras, we need the constant initializer to define the pre-trained matrix of the Embedding layer.\n",
    "\n",
    "5. Using GloVE pre-trained vectors\n",
    " - Glove files contain rows separated by spaces, where the first column is the word and the others are the weights values for each dimension of the embedding space. To read the values, then, we loop over the rows of the file, split the line by spaces, get the word as the first item of the list and the rest of the list are the weights. We use dictionaries to easily store for each word an np array with the values. We also cast the values to have float32 type because it is the type used to create the vectors.\n",
    "\n",
    "6. Using the GloVE on a specific task\n",
    " - To use the GloVE vectors in a specific task, we can simply select the words present on the vocabulary list, ignoring the other words to save memory. We need the task-specific vocabulary dictionary with words as keys and indexes as values, the glove dict created in the previous slide and the dimension of the embedding space as inputs. We define a matrix with shape equal to the number of words plus one and the embedding space dim. We add one because the index zero is reserved for the padding token. We iterate over the vocabulary words, if the word is found in the glove vectors, then we update this row of the matrix with the values from glove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer in keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T03:45:42.820269Z",
     "start_time": "2021-09-28T03:45:42.772510Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "model = Sequential()\n",
    "\n",
    "# use as the first layer\n",
    "model.add(Embedding(input_dim=100000,\n",
    "                    out_dim=300, # embedding space dimension\n",
    "                    trainable=True, # update weights during training or not\n",
    "                    embeddings_initializer=None # use transfer learning for words in vocabulary, but often False\n",
    "                    input_length=120 # size of sequences\n",
    "                   ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "Transfer learning for language models\n",
    "- GloVE\n",
    "- word2vec\n",
    "- BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in keras, need to import Constant for embedding layer\n",
    "from keras.initializers import Constant\n",
    "model.add(Embedding(input_dim=vocabulary_size,\n",
    "                    out_dim=embedding_dim,\n",
    "                    embeddings_initializer=Constant(pre_trained_vectors)\n",
    "                   ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GloVE pre-trained vectors\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "- rows separated by spaces\n",
    "- 1st column is the word, the others are the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hte GloVE vectors\n",
    "def get_glove_vectors(filename='glove.6B.300d.txt'):\n",
    "    # get all word vectors from pre-trained model\n",
    "    glove_vector_dict = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = values[1:]\n",
    "            glove_vector_dict[word]=np.asarray(coefs, dtype='float32')\n",
    "        \n",
    "    # is this right to return?\n",
    "    return embeddings_index\n",
    "\n",
    "# using the GloVE on a specific task\n",
    "# filter GloVE vectors to specific task, word:index key:value\n",
    "def filter_glove(vocabulary_dict, glove_dict, wordvec_dim=300):\n",
    "    # create a matrix to store the vectors\n",
    "    embedding_matrix = np.zeros((len(vocabular_dict)+1, wordvec_dim))\n",
    "    for word, i in vocabulary_dict.items():\n",
    "        embedding_vector = glove_dict.get(word)\n",
    "        # if the word is found then we update\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in the glove_dict will be all-zeros\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - Number of parameters comparison\n",
    "You saw that the one-hot representation is not a good representation of words because it is very sparse. Using the Embedding layer creates a dense representation of the vectors, but also demands a lot of parameters to be learned.\n",
    "\n",
    "In this exercise you will compare the number of parameters of two models using embeddings and one-hot encoding to see the difference.\n",
    "\n",
    "The model model_onehot is already loaded in the environment, as well as the Sequential, Dense and GRU from keras. Finally, the parameters vocabulary_size=80000 and sentence_len=200 are also loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the embedding layer\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# Create a model with embeddings\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=vocabulary_size+1, output_dim=wordvec_dim, input_length=sentence_len, trainable=True))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the summaries of the one-hot model\n",
    "model_onehot.summary()\n",
    "\n",
    "# Print the summaries of the model with embeddings\n",
    "model.summary()\n",
    "\n",
    "'''\n",
    "Model: \"model_onehot\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "gru_1 (GRU)                  (None, 128)               49920     \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 1)                 129       \n",
    "=================================================================\n",
    "Total params: 50,049\n",
    "Trainable params: 50,049\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Model: \"emb_model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_1 (Embedding)      (None, 200, 300)          24000600  \n",
    "_________________________________________________________________\n",
    "gru_2 (GRU)                  (None, 128)               164736    \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 1)                 129       \n",
    "=================================================================\n",
    "Total params: 24,165,465\n",
    "Trainable params: 24,165,465\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the immense difference in the number of parameters when using the embedding layer! Don't worry, in the next exercise you will learn how make transfer learning to avoid having to train this layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "You saw that when training an embedding layer, you need to learn a lot of parameters.\n",
    "\n",
    "In this exercise, you will see that when using transfer learning it is possible to use the pre-trained weights and don't update them, meaning that all the parameters of the embedding layer will be fixed, and the model will only need to learn the parameters from the other layers.\n",
    "\n",
    "The function load_glove is already loaded on the environment and retrieves the glove matrix as a numpy.ndarray vector. It uses the function covered on the lesson's slides to retrieve the glove vectors with 200 embedding dimensions for the vocabulary present in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the glove pre-trained vectors\n",
    "glove_matrix = load_glove('glove_200d.zip')\n",
    "\n",
    "# Create a model with embeddings\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=vocabulary_size + 1, output_dim=wordvec_dim, \n",
    "                    embeddings_initializer=Constant(glove_matrix), \n",
    "                    input_length=sentence_len, trainable=False))\n",
    "model.add(GRU(128))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the summaries of the model with embeddings\n",
    "model.summary()\n",
    "'''\n",
    "Model: \"emb_model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "embedding_1 (Embedding)      (None, 200, 200)          2000400   \n",
    "_________________________________________________________________\n",
    "gru_1 (GRU)                  (None, 128)               126336    \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 1)                 129       \n",
    "=================================================================\n",
    "Total params: 2,126,865\n",
    "Trainable params: 126,465\n",
    "Non-trainable params: 2,000,400\n",
    "_________________________________________________________________\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total parameters is very big, but the number of parameteres that will be trained is much smaller. The trained vectors already has values for the words, but is equal to a vector of zeros for new words not present in the pre-trained vectors. This can lead to problems if the task at hand is very specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings improves performance\n",
    "Does the embedding layer improves the accuracy of the model? Let's check it out in the same IMDB data.\n",
    "\n",
    "The model was already trained with 10 epochs, as in the previous model with simpleRNN cell. In order to compare the models, a test set (X_test, y_test) is available in the environment, as well as the old model simpleRNN_model. The old model's accuracy is loaded in the variable acc_SimpleRNN.\n",
    "\n",
    "All required modules and functions as loaded in the environment: Sequential() from keras.models, Embedding and Dense from keras.layers and SimpleRNN from keras.layers.recurrent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with embedding\n",
    "model = Sequential(name=\"emb_model\")\n",
    "model.add(Embedding(input_dim=max_vocabulary,\n",
    "                    output_dim=wordvec_dim, input_length=max_len))\n",
    "model.add(SimpleRNN(units=128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('embedding_model_weights.h5')\n",
    "\n",
    "# Evaluate the models' performance (ignore the loss value)\n",
    "_, acc_embeddings = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the results\n",
    "print(\"SimpleRNN model's accuracy:\\t{0}\\nEmbeddings model's accuracy:\\t{1}\".format(\n",
    "    acc_simpleRNN, acc_embeddings))\n",
    "\n",
    "# SimpleRNN model's accuracy: 0.495\n",
    "# Embeddings model's accuracy: 0.733\n",
    "\n",
    "# the embedding layer greatly improves the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving RNN model and overfitting - Sentiment classification revisited\n",
    "Ways to improve the SimpleRNN model\n",
    "- add the embedding layer\n",
    "- increase the number of layers\n",
    "- tune the parameters\n",
    "- increase vocabulary size\n",
    "- accept longer sentences with more memory cells\n",
    "\n",
    "Avoid overfitting - RNN models can overfit\n",
    "- test different batch sizes\n",
    "- add dropout layers\n",
    "- add dropout and recurrent_dropout parameters on RNN layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid overfitting\n",
    "\n",
    "# add dropout layer\n",
    "# removes 20% of input to add noise\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "# add dropout and recurrent_dropout parameters\n",
    "# removes 10% of input and memory cells respectively\n",
    "model.add(LSTM(128, dropout=0.1, recurrent_dropout=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Layer and MaxPooling layer\n",
    "- convolution layer do feature selection on the embedding vector\n",
    "- achieves state-of-the-art results in many NLP problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(vocabulary_size, wordvec_dim, ...))\n",
    "model.add(Conv1D(num_filters=32, kernel_size=3, padding='same'))\n",
    "model.add(MaxPooling1D(pool_size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# add embedding layer\n",
    "model.add(Embedding(vocabulary_size, wordvec_dim, trainable=True,\n",
    "                    embeddings_intializer=Constant(glove_matrix),\n",
    "                    input_length=max_text_len, name=\"Embedding\"))\n",
    "model.add(Dense(wordvec_dim, activation='relu', name=\"Dense1\"))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(LSTM(64, return_sequences=True, dropout=0.15, name=\"LSTM\"))\n",
    "model.add(GRU(64, return_sequences=False, dropout=0.15, name=\"GRU\"))\n",
    "model.add(Dense(64, name=\"Dense2\"))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(32, name=\"Dense3\"))\n",
    "model.add(Dense(1, activation='sigmoid', name=\"Output\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better sentiment classification\n",
    "In this exercise, you go back to the sentiment classification problem seen in Chapter 1.\n",
    "\n",
    "You are going to add more complexity to the model and improve its accuracy. You will use an Embedding layer to train word vectors on the training set and two LSTM layers to keep track of longer texts. Also, you will add an extra Dense layer before the output.\n",
    "\n",
    "This is no longer a simple model, and the training can take some time. For this reason, a pre-trained model is available by loading its weights with the method .load_weights() from the keras.models.Sequential class. The model was trained with 10 epochs and its weights are available on the file model_weights.h5.\n",
    "\n",
    "The following modules are loaded on the environment: Sequential, Embedding, LSTM, Dropout, Dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, wordvec_dim,\n",
    "                    trainable=True, input_length=max_text_len))\n",
    "model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.15))\n",
    "model.add(Dense(16))\n",
    "model.add(Dropout(rate=0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Load pre-trained weights\n",
    "model.load_weights('model_weights.h5')\n",
    "\n",
    "# Print the obtained loss and accuracy\n",
    "print(\"Loss: {0}\\nAccuracy: {1}\".format(\n",
    "    *model.evaluate(X_test, y_test, verbose=0)))\n",
    "'''\n",
    "Loss: 1.0716214485168456\n",
    "Accuracy: 0.822\n",
    "\n",
    "just increased the accuracy of your sentiment classification \n",
    "task from poorly 50% to more than 80%\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the CNN layer\n",
    "In this exercise, you will use a pre-trained model that makes use of the Conv1D and MaxPooling1D layers from the keras.layers.convolutional module, and achieves even better accuracy on the classification task.\n",
    "\n",
    "This architecture achieved good results in language modeling tasks such as classification, and is added here as an extra exercise to see it in action and have some intuitions.\n",
    "\n",
    "Because this layer is not in the scope of the course, you will focus on how to use the layers together with the RNN layers you already learned.\n",
    "\n",
    "Please follow the instructions to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "model_cnn.summary()\n",
    "\n",
    "# Load pre-trained weights\n",
    "model_cnn.load_weights('model_weights.h5')\n",
    "\n",
    "# Evaluate the model to get the loss and accuracy values\n",
    "loss, acc = model_cnn.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print the loss and accuracy obtained\n",
    "print(\"Loss: {0}\\nAccuracy: {1}\".format(loss, acc))\n",
    "\n",
    "'''\n",
    "Model: \"cnn_model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "Embedding (Embedding)        (None, 800, 100)          2000100   \n",
    "_________________________________________________________________\n",
    "dropout_1 (Dropout)          (None, 800, 100)          0         \n",
    "_________________________________________________________________\n",
    "Conv (Conv1D)                (None, 797, 16)           6416      \n",
    "_________________________________________________________________\n",
    "MaxPool (MaxPooling1D)       (None, 398, 16)           0         \n",
    "_________________________________________________________________\n",
    "dropout_2 (Dropout)          (None, 398, 16)           0         \n",
    "_________________________________________________________________\n",
    "LSTM (LSTM)                  (None, 64)                20736     \n",
    "_________________________________________________________________\n",
    "dropout_3 (Dropout)          (None, 64)                0         \n",
    "_________________________________________________________________\n",
    "Dense2 (Dense)               (None, 16)                1040      \n",
    "_________________________________________________________________\n",
    "Output (Dense)               (None, 1)                 17        \n",
    "=================================================================\n",
    "Total params: 2,028,309\n",
    "Trainable params: 2,028,309\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Loss: 0.4343099966049194\n",
    "Accuracy: 0.836\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you achieved very high accuracy on the sentiment classification task! Remark that on the training data the model achieved more than 98% accuracy, and because the accuracy was not in the same level on the test data, you can guess that it had some level of overfitting. It may be because the dataset was not big enough to train the model and some patterns present on the test data weren't present on the train set. Finally, the model can be further extended to have additional layers to achieve even better results, but will also demand more data and computer power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
