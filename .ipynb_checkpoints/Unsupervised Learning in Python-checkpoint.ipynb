{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the world of unsupervised learning, called as such because you are not guiding, or supervising, the pattern discovery by some prediction task, but instead uncovering hidden structure from unlabeled data. Unsupervised learning encompasses a variety of techniques in machine learning, from clustering to dimension reduction to matrix factorization. In this course, you'll learn the fundamentals of unsupervised learning and implement the essential algorithms using scikit-learn and scipy. You will learn how to cluster, transform, visualize, and extract insights from unlabeled datasets, and end the course by building a recommender system to recommend popular musical artists.\n",
    "\n",
    "Outline:\n",
    "1. Clustering for dataset exploration\n",
    "2. Visualization with hierarchical clustering and t-SNE\n",
    "3. Decorrelating your data and dimension reduction - PCA\n",
    "4. Discovering interpretable features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clustering for dataset exploration\n",
    "Discover underlying groups (or \"clusters\") in a dataset.\n",
    "\n",
    "k-means clustering\n",
    "- finds clusters of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means clustering in sklearn\n",
    "print(samples)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(samples)\n",
    "KMeans(algorithm='auto', ...)\n",
    "labels = model.predict(samples)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Cluster labels for new samples\n",
    "- new samples can be assigned to existing clusters\n",
    "- k-means remembers the mean of each cluster (the \"centroids\")\n",
    "- find the nearest centroid to each new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster labels for new samples\n",
    "print(new_samples)\n",
    "\n",
    "new_labels = model.predict(new_samples)\n",
    "print(new_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Scatter plots to visualize\n",
    "Use Iris dataset\n",
    "- scatter plot of sepal length vs petal length\n",
    "- each point represents an iris sample\n",
    "- color points by cluster labels\n",
    "- Use PyPlot (matplotlib.pyplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot\n",
    "import matplotlib.pyplot as plt\n",
    "# sepal length is at 0\n",
    "xs = samples[:,0]\n",
    "# petal length is at 2\n",
    "ys = samples[:,2]\n",
    "plt.scatter(xs, ys, c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 How many clusters?\n",
    "You are given an array 'points' of size 300x2, where each row gives the (x, y) co-ordinates of a point on a map. Make a scatter plot of these points, and use the scatter plot to guess how many clusters there are.\n",
    "\n",
    "matplotlib.pyplot has already been imported as plt. In the IPython Shell:\n",
    "\n",
    "Create an array called xs that contains the values of points[:,0] - that is, column 0 of points.\n",
    "Create an array called ys that contains the values of points[:,1] - that is, column 1 of points.\n",
    "Make a scatter plot by passing xs and ys to the plt.scatter() function.\n",
    "Call the plt.show() function to show your plot.\n",
    "How many clusters do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [1]: xs=points[:,0]\n",
    "\n",
    "In [2]: ys=points[:,1]\n",
    "\n",
    "In [3]: plt.scatter(xs,ys)\n",
    "Out[3]: <matplotlib.collections.PathCollection at 0x7f41fd3c0d30>\n",
    "\n",
    "In [4]: plt.show()\n",
    "\n",
    "# looks like 3 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Clustering 2D points\n",
    "From the scatter plot of the previous exercise, you saw that the points seem to separate into 3 clusters. You'll now create a KMeans model to find 3 clusters, and fit it to the data points from the previous exercise. After the model has been fit, you'll obtain the cluster labels for some new points using the .predict() method.\n",
    "\n",
    "You are given the array points from the previous exercise, and also an array new_points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a KMeans instance with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit model to points\n",
    "model.fit(points)\n",
    "\n",
    "# Determine the cluster labels of new_points: labels\n",
    "labels = model.predict(new_points)\n",
    "\n",
    "# Print cluster labels of new_points\n",
    "print(labels)\n",
    "\n",
    "# [2 1 0 2 1 2 1 1 1 0 2 1 1 0 0 1 0 0 1 1 0 1 2 1 2 0 1 0 0 2 2 1 1 1 0 2 1\n",
    "#  1 2 1 0 2 2 0 2 1 0 0 1 1 1 1 0 0 2 2 0 0 0 2 2 1 1 1 2 1 0 1 2 0 2 2 2 1\n",
    "#  2 0 0 2 1 0 2 0 2 1 0 1 0 2 1 1 1 2 1 1 2 0 0 0 0 2 1 2 0 0 2 2 1 2 0 0 2\n",
    "#  0 0 0 1 1 1 1 0 0 1 2 1 0 1 2 0 1 0 0 1 0 1 0 2 1 2 2 1 0 2 1 2 2 0 1 1 2\n",
    "#  0 2 0 1 2 0 0 2 0 1 1 0 1 0 0 1 1 2 1 1 0 2 0 2 2 1 2 1 1 2 2 0 2 2 2 0 1\n",
    "#  1 2 0 2 0 0 1 1 1 2 1 1 1 0 0 2 1 2 2 2 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0\n",
    "#  1 1 2 0 2 2 0 2 0 2 0 1 1 0 1 1 1 0 2 2 0 1 1 0 1 0 0 1 0 0 2 0 2 2 2 1 0\n",
    "#  0 0 2 1 2 0 2 0 0 1 2 2 2 0 1 1 1 2 1 0 0 1 2 2 0 2 2 0 2 1 2 0 0 0 0 1 0\n",
    "#  0 1 1 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Inspect your clustering with scatter plot\n",
    "Let's now inspect the clustering you performed in the previous exercise!\n",
    "\n",
    "A solution to the previous exercise has already run, so 'new_points' is an array of points and 'labels' is the array of their cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign the columns of new_points: xs and ys\n",
    "xs = new_points[:,0]\n",
    "ys = new_points[:,1]\n",
    "\n",
    "# Make a scatter plot of xs and ys, using labels to define the colors\n",
    "plt.scatter(xs,ys,c=labels,alpha=0.5)\n",
    "\n",
    "# Assign the cluster centers: centroids\n",
    "# Compute the coordinates of the centroids \n",
    "centroids = model.cluster_centers_\n",
    "\n",
    "# Assign the columns of centroids: centroids_x, centroids_y\n",
    "centroids_x = centroids[:,0]\n",
    "centroids_y = centroids[:,1]\n",
    "\n",
    "# Make a scatter plot of centroids_x and centroids_y\n",
    "# D marker = diamonds, size of markers 50\n",
    "plt.scatter(centroids_x, centroids_y,marker='D',s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Evaluating a clustering\n",
    "- How can you check the quality of the clustering?\n",
    "    - Can check correspondence with ie. iris species\n",
    "- If no species to check against,..\n",
    "- Measure quality of clustering\n",
    "- Informs choice of how many clusters to look for\n",
    "\n",
    "Iris: clusters vs specieis\n",
    "- k-means found 3 clusters amongst the iris samples\n",
    "- Do the clusters correspond to the species\n",
    "\n",
    "Cross tabulation with pandas\n",
    "- clusters vs species is a \"cross-tabulation\"\n",
    "- Use pandas library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a crosstab\n",
    "\n",
    "# align labels and specieis\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'labels':labels, 'species':species})\n",
    "print(df)\n",
    "\n",
    "# crosstab of labels and species\n",
    "ct = pd.crosstab(df['labels'], df['species'])\n",
    "print(ct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Meauring clustering quality\n",
    "- Using only samples and their cluster labels\n",
    "- a good clustering has tight clusters\n",
    "\n",
    "Inertia measures clustering quality\n",
    "- measures how spread out the clusters are (lower is better)\n",
    "- distance from each sample to centroid of its cluster\n",
    "- after fit(), avaoilable as attribute inertia_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inertia measure for clustering quality\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(samples)\n",
    "print(model.inertia_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 How many clusters to choose\n",
    "- a good clustering has tight clusters (so low inertia)\n",
    "- but not too many clusters\n",
    "- choose an \"elbow\" in the inertia plot\n",
    "    - elbow = where inertia begins to decrease more slowly\n",
    "    - ie. for iris datset, 3 is a good choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 How many clusters of grain?\n",
    "In the video, you learned how to choose a good number of clusters for a dataset using the k-means inertia graph. You are given an array samples containing the measurements (such as area, perimeter, length, and several others) of samples of grain. What's a good number of clusters in this case?\n",
    "\n",
    "KMeans and PyPlot (plt) have already been imported for you.\n",
    "\n",
    "This dataset was sourced from the UCI Machine Learning Repository.\n",
    "https://archive.ics.uci.edu/ml/datasets/seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, 6)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(samples)\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "# Plot ks vs inertias\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()\n",
    "\n",
    "# The inertia decreases very slowly from 3 clusters to 4, \n",
    "# so it looks like 3 clusters would be a good choice for this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Evaluating the grain clustering\n",
    "In the previous exercise, you observed from the inertia plot that 3 is a good number of clusters for the grain data. In fact, the grain samples come from a mix of 3 different grain varieties: \"Kama\", \"Rosa\" and \"Canadian\". \n",
    "\n",
    "In this exercise, cluster the grain samples into three clusters, and compare the clusters to the grain varieties using a cross-tabulation.\n",
    "\n",
    "You have the array samples of grain samples, and a list varieties giving the grain variety for each sample. Pandas (pd) and KMeans have already been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create clusters and compare in cross-tab\n",
    "\n",
    "# Create a KMeans model with 3 clusters: model\n",
    "model = KMeans(n_clusters=3)\n",
    "\n",
    "# Use fit_predict to fit model and obtain cluster labels: labels\n",
    "labels = model.fit_predict(samples)\n",
    "\n",
    "# Create a DataFrame with labels and varieties as columns: df\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'],df['varieties'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11 Transforming features for better clusterings\n",
    "- Piedmont wines dataset\n",
    "- 178 samples from 3 distinct varieties of red wine\n",
    "- features measure chemical composition (ie. alcohol content), color intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering the wines\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=3)\n",
    "labels = model.fit_predict(samples)\n",
    "# clusters vs varieties cross-tab\n",
    "df = pd.DataFrame({'labels':labels,\n",
    "                  'varieties':varieties})\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "print(ct)\n",
    "\n",
    "# Feature variances very different, so crosstab shoes clustering not good\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.11.a StandardScaler\n",
    "- In kmeans, feature variance = feature influence\n",
    "- the features need to be transformed to have equal variance\n",
    "- StandardScaler transforms each feature to have mean 0 and variance 1\n",
    "- Features are \"standardized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(samples)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "samples_scaled = scaler.transform(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 steps with pipeline: StandardScaler, then KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "scaler = StandardScaler()\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans)\n",
    "# fit scaler and kmeans\n",
    "pipeline.fit(samples)\n",
    "# get cluster labels\n",
    "labels = pipeline.predict(samples)\n",
    "\n",
    "# Feature standardization improves clustering\n",
    "df = pd.DataFrame({'labels':labels,\n",
    "                  'varieties':varieties})\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "print(ct)\n",
    "\n",
    "# big improvement in clustering after standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.11.b sklearn preprocessing steps - few options\n",
    "- StandardScaler is a \"preprocessing\" step\n",
    "- MaxAbsScaler\n",
    "- Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.11.c Practice: Scaling fish data for clustering\n",
    "You are given an array samples giving measurements of fish. Each row represents an individual fish. The measurements, such as weight in grams, length in centimeters, and the percentage ratio of height to length, have very different scales. In order to cluster this data effectively, you'll need to standardize these features first. In this exercise, you'll build a pipeline to standardize and cluster the data.\n",
    "\n",
    "These fish measurement data were sourced from the Journal of Statistics Education.\n",
    "http://ww2.amstat.org/publications/jse/jse_data_archive.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, kmeans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.11.d Practice: Clustering the fish data\n",
    "You'll now use your standardization and clustering pipeline from the previous exercise to cluster the fish by their measurements, and then create a cross-tabulation to compare the cluster labels with the fish species.\n",
    "\n",
    "As before, samples is the 2D array of fish measurements. Your pipeline is available as pipeline, and the species of every fish sample is given by the list species.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Fit the pipeline to samples\n",
    "pipeline.fit(samples)\n",
    "\n",
    "# Calculate the cluster labels: labels\n",
    "labels = pipeline.predict(samples)\n",
    "\n",
    "# Create a DataFrame with labels and species as columns: df\n",
    "df = pd.DataFrame({'labels':labels,'species':species})\n",
    "\n",
    "# Create crosstab: ct\n",
    "ct = pd.crosstab(df['labels'], df['species'])\n",
    "\n",
    "# Display ct\n",
    "print(ct)\n",
    "\n",
    "# species  Bream  Pike  Roach  Smelt\n",
    "# labels                            \n",
    "# 0           33     0      1      0\n",
    "# 1            1     0     19      1\n",
    "# 2            0    17      0      0\n",
    "# 3            0     0      0     13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.12 Clustering stocks using Normalizer and KMeans\n",
    "In this exercise, you'll cluster companies using their daily stock price movements (i.e. the dollar difference between the closing and opening prices for each trading day). You are given a NumPy array movements of daily price movements from 2010 to 2015 (obtained from Yahoo! Finance), where each row corresponds to a company, and each column corresponds to a trading day.\n",
    "\n",
    "Some stocks are more expensive than others. To account for this, include a Normalizer at the beginning of your pipeline. The Normalizer will separately transform each company's stock price to a relative scale before the clustering begins.\n",
    "\n",
    "Note that Normalizer() is different to StandardScaler(), which you used in the previous exercise. \n",
    "- While StandardScaler() standardizes features (such as the features of the fish data from the previous exercise) by removing the mean and scaling to unit variance, \n",
    "- Normalizer() rescales each sample - here, each company's stock price - independently of the other.\n",
    "\n",
    "KMeans and make_pipeline have already been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipeline\n",
    "\n",
    "# Import Normalizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Create a normalizer: normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Create a KMeans model with 10 clusters: kmeans\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "\n",
    "# Make a pipeline chaining normalizer and kmeans: pipeline\n",
    "pipeline = make_pipeline(normalizer, kmeans)\n",
    "\n",
    "# Fit pipeline to the daily price movements\n",
    "pipeline.fit(movements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1.12.a Which stocks move together?\n",
    "In the previous exercise, you clustered companies by their daily stock price movements. So which company have stock prices that tend to change in the same way? You'll now inspect the cluster labels from your clustering to find out.\n",
    "\n",
    "Your solution to the previous exercise has already been run. Recall that you constructed a Pipeline pipeline containing a KMeans model and fit it to the NumPy array movements of daily stock movements. In addition, a list companies of the company names is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Predict the cluster labels: labels\n",
    "labels = pipeline.predict(movements)\n",
    "\n",
    "# Create a DataFrame aligning labels and companies: df\n",
    "df = pd.DataFrame({'labels': labels, 'companies': companies})\n",
    "\n",
    "# Display df sorted by cluster label\n",
    "print(df.sort_values('labels'))\n",
    "\n",
    "                             companies  labels\n",
    "45                                Sony       0\n",
    "34                          Mitsubishi       0\n",
    "48                              Toyota       0\n",
    "7                                Canon       0\n",
    "21                               Honda       0\n",
    "32                                  3M       1\n",
    "35                            Navistar       1\n",
    "44                        Schlumberger       1\n",
    "13                   DuPont de Nemours       1\n",
    "12                             Chevron       1\n",
    "0                                Apple       1\n",
    "47                            Symantec       1\n",
    "8                          Caterpillar       1\n",
    "50  Taiwan Semiconductor Manufacturing       1\n",
    "51                   Texas instruments       1\n",
    "53                       Valero Energy       1\n",
    "57                               Exxon       1\n",
    "10                      ConocoPhillips       1\n",
    "23                                 IBM       1\n",
    "6             British American Tobacco       2\n",
    "41                       Philip Morris       2\n",
    "19                     GlaxoSmithKline       2\n",
    "36                    Northrop Grumman       3\n",
    "29                     Lookheed Martin       3\n",
    "4                               Boeing       3\n",
    "15                                Ford       4\n",
    "5                      Bank of America       4\n",
    "26                      JPMorgan Chase       4\n",
    "1                                  AIG       4\n",
    "58                               Xerox       4\n",
    "16                   General Electrics       4\n",
    "55                         Wells Fargo       4\n",
    "3                     American express       4\n",
    "18                       Goldman Sachs       4\n",
    "54                            Walgreen       5\n",
    "39                              Pfizer       5\n",
    "56                            Wal-Mart       5\n",
    "27                      Kimberly-Clark       5\n",
    "25                   Johnson & Johnson       5\n",
    "40                      Procter Gamble       5\n",
    "2                               Amazon       6\n",
    "59                               Yahoo       6\n",
    "42                   Royal Dutch Shell       7\n",
    "30                          MasterCard       7\n",
    "31                           McDonalds       7\n",
    "20                          Home Depot       7\n",
    "52                            Unilever       7\n",
    "49                               Total       7\n",
    "37                            Novartis       7\n",
    "46                      Sanofi-Aventis       7\n",
    "43                                 SAP       7\n",
    "22                                  HP       8\n",
    "17                     Google/Alphabet       8\n",
    "33                           Microsoft       8\n",
    "11                               Cisco       8\n",
    "14                                Dell       8\n",
    "24                               Intel       8\n",
    "9                    Colgate-Palmolive       9\n",
    "28                           Coca Cola       9\n",
    "38                               Pepsi       9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualization with hierarchical clustering and t-SNE\n",
    "2 unsupervised learning techniques for data visualization\n",
    "1. hierarchical clustering\n",
    "2. t-SNE. \n",
    "\n",
    "Hierarchical clustering merges the data samples into ever-coarser clusters, yielding a tree visualization of the resulting cluster hierarchy. \n",
    "\n",
    "t-SNE maps the data samples into 2d space so that the proximity of the samples to one another can be visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decorrelating your data and dimension reduction - PCA\n",
    "Dimension reduction summarizes a dataset using its common occuring patterns.\n",
    "\n",
    "Learn about the most fundamental of dimension reduction techniques, \"Principal Component Analysis\" (\"PCA\"). PCA is often used before supervised learning to improve model performance and generalization. It can also be useful for unsupervised learning. For example, you'll employ a variant of PCA will allow you to cluster Wikipedia articles by their content!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Discovering interpretable features\n",
    "Dimension reduction technique called \"Non-negative matrix factorization\" (\"NMF\") that expresses samples as combinations of interpretable parts. For example, it expresses documents as combinations of topics, and images in terms of commonly occurring visual patterns. You'll also learn to use NMF to build recommender systems that can find you similar articles to read, or musical artists that match your listening history!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
