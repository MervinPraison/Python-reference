{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classifiers with scikit-learn\n",
    "- logistic regression\n",
    "- SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Applying logistic regression and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review KNN classification\n",
    "- using Large Movie Review Dataset\n",
    "# The variables X_train, X_test, y_train, and y_test are already \n",
    "# loaded into the environment. The X variables contain features \n",
    "# based on the words in the movie reviews, and the y variables \n",
    "# contain labels for whether the review sentiment is positive (+1) \n",
    "# or negative (-1).\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create and fit the model with default hyperparameters\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test features, print the results\n",
    "pred = knn.predict(X_test)[0]\n",
    "print(\"Prediction for test example 0:\", pred)\n",
    "# Prediction for test example 0: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing models\n",
    "# Compare k nearest neighbors classifiers with k=1 and k=5 on the\n",
    "# handwritten digits data set, which is already loaded into the \n",
    "# variables X_train, y_train, X_test, and y_test. \n",
    "# You can set k with the n_neighbors parameter when creating the \n",
    "# KNeighborsClassifier object, which is also already imported into \n",
    "# the environment.\n",
    "\n",
    "# Which model has a higher test accuracy?\n",
    "In [2]: knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "In [3]: knn.fit(X_train, y_train)\n",
    "Out[3]: \n",
    "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
    "           weights='uniform')\n",
    "\n",
    "In [4]: knn.score(X_test, y_test)\n",
    "Out[4]: 0.9888888888888889\n",
    "\n",
    "In [5]: knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "In [6]: knn.fit(X_train, y_train)\n",
    "Out[6]: \n",
    "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
    "           weights='uniform')\n",
    "\n",
    "In [7]: knn.score(X_test, y_test)\n",
    "Out[7]: 0.9933333333333333\n",
    "\n",
    "# n_neighbors = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.predict(X_test)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogReg example 2\n",
    "import sklearn.datasets\n",
    "wine = sklearn.datasets.load_wine()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.ft(wine.data, wine.target)\n",
    "lr.score(wine.data, wine.target)\n",
    "# 0.972\n",
    "\n",
    "# confidence intervals\n",
    "lr.predict_proba(wine.data[:1])\n",
    "array([[ 9.951e-01, 4.357e-03, 5.339e-04]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC\n",
    "import sklearn.datasets\n",
    "wine = sklearn.datasets.load_wine()\n",
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC()\n",
    "svm.ft(wine.data, wine.target)\n",
    "svm.score(wine.data, wine.target)\n",
    "# 0.893"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC - uses nonlinear SVM by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "import sklearn.datasets\n",
    "wine = sklearn.datasets.load_wine()\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.ft(wine.data, wine.target)\n",
    "svm.score(wine.data, wine.target)\n",
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples: logistic regression and SVM\n",
    "#For each classifier, print out the training and validation accuracy.\n",
    "\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target)\n",
    "\n",
    "# Apply logistic regression and print scores\n",
    "lr = LogisticRegression()\n",
    "lr.fit(Xtrain, ytrain)\n",
    "print(lr.score(Xtrain,ytrain))\n",
    "print(lr.score(Xtest,ytest))\n",
    "\n",
    "# Apply SVM and print scores\n",
    "svm = SVC()\n",
    "svm.fit(Xtrain, ytrain)\n",
    "print(svm.score(Xtrain,ytrain))\n",
    "print(svm.score(Xtest,ytest))\n",
    "\n",
    "# 0.9977728285077951\n",
    "# 0.9444444444444444\n",
    "# 1.0\n",
    "# 0.26666666666666666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis for movie reviews\n",
    "# In this exercise you'll explore the probabilities outputted by \n",
    "# logistic regression on a subset of the Large Movie Review Dataset. \n",
    "# The variables X and y are already loaded into the environment. \n",
    "# X contains features based on the number of times words appear in \n",
    "# the movie reviews, and y contains labels for whether the review \n",
    "# sentiment is positive (+1) or negative (-1).\n",
    "\n",
    "# Instantiate logistic regression and train\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Predict sentiment for a glowing review\n",
    "review1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\n",
    "review1_features = get_features(review1)\n",
    "print(\"Review:\", review1)\n",
    "print(\"Probability of positive review:\", \n",
    "      lr.predict_proba(review1_features)[0,1])\n",
    "\n",
    "# Predict sentiment for a poor review\n",
    "review2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\n",
    "review2_features = get_features(review2)\n",
    "print(\"Review:\", review2)\n",
    "print(\"Probability of positive review:\", \n",
    "      lr.predict_proba(review2_features)[0,1])\n",
    "\n",
    "# Review: LOVED IT! This movie was amazing. Top 10 this year.\n",
    "# Probability of positive review: 0.8079007873616059\n",
    "# Review: Total junk! I'll never watch a film by that director again, no matter how good the reviews.\n",
    "# Probability of positive review: 0.5855117402793947\n",
    "\n",
    "# note: \"good\" in the second review throws it off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear classifiers\n",
    "- decision boundaries: linear, nonlinear\n",
    "- linearly separable data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing decision boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision boundaries of various classifier types. \n",
    "# A subset of scikit-learn's built-in wine dataset is already \n",
    "# loaded into X along with binary labels in y.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [LogisticRegression(),\n",
    "LinearSVC(),\n",
    "SVC(),\n",
    "KNeighborsClassifier()]\n",
    "\n",
    "# Fit the classifiers\n",
    "for c in classifiers:\n",
    "    c.fit(X, y)\n",
    "\n",
    "# Plot the classifiers\n",
    "plot_4_classifiers(X, y, classifiers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifiers: the coefficients\n",
    "- prediction equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot products\n",
    "x = np.arange(3)\n",
    "x\n",
    "# out: array([0,1,2])\n",
    "    \n",
    "y = np.arange(3,6)\n",
    "y\n",
    "# out: array([3,4,5])\n",
    "    \n",
    "x*y\n",
    "# out: array([0,4,10])\n",
    "\n",
    "# dot product\n",
    "np.sum(x*y)\n",
    "# out: 14\n",
    "# convenient notation: dot product using @\n",
    "x@y\n",
    "# out: 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear classifier prediction\n",
    "- raw model output = coefficients * features + intercept\n",
    "- Linear classifier prediction:\n",
    "    - compute raw model output\n",
    "    - check the sign (which side of decision boundary)\n",
    "        - if positive, predict one class\n",
    "        - if negative, predict the other class\n",
    "- This is the same for logistic regression and linear SVM\n",
    "    - 'fit' is different but 'predict' is the same\n",
    "    - difference in 'fit' in loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Logistic Regression makes predictions\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "lr.predict(X)[10]\n",
    "# out: 0\n",
    "lr.predict(X)[20]\n",
    "# out: 1\n",
    "\n",
    "# get coefficients\n",
    "lr.coef_ & X[10] + lr.intercept_ # raw model output for example 10\n",
    "array([-33.78572166])\n",
    "# it's negative so predict other class\n",
    "# for example 20, positive, so predict one class\n",
    "lr.coef_ & X[20] + lr.intercept_ # raw model output for example 20\n",
    "array([0.08050621])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the model coefficients\n",
    "# coefficients determine slope of boundary\n",
    "# intercept shifts the boundary\n",
    "\n",
    "# Observe the effects of changing the coefficients of a linear\n",
    "# classifer. A 2D dataset is already loaded into the environment\n",
    "# as X and y, along with a linear classifier object model.\n",
    "\n",
    "# Set the coefficients\n",
    "# changed coefficients from 0,1 to -1,1\n",
    "model.coef_ = np.array([[0,1]])\n",
    "# changed intercept from 0 to -3\n",
    "model.intercept_ = np.array([0])\n",
    "\n",
    "# Plot the data and decision boundary\n",
    "plot_classifier(X,y,model)\n",
    "\n",
    "# Print the number of errors\n",
    "num_err = np.sum(y != model.predict(X))\n",
    "print(\"Number of errors:\", num_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a loss function?\n",
    "- sklearn's LinearRegression minimizes a loss\n",
    "- minimization is with respect to coefficients or parameters of model\n",
    "- note: model.score() isn't necessarily the loss function\n",
    "- The loss is used to fit the model on the data, while the score is used to see how we're doing\n",
    "\n",
    "Classification errors: the 0-1 loss\n",
    "- The squared error/loss isn't appropriate for Classification problems since y are categories (not numbers)\n",
    "- a natural loss for classification problem is the number of errors\n",
    "- This is the '0-1 loss': 0 for correct prediction, 1 for incorrect prediction\n",
    "- By summing the results, you get the total number of incorrect\n",
    "- But this loss is hard to minimize, so logreg and SVM don't use this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing a loss function\n",
    "- using scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "minimize(np.square, 0).x\n",
    "#out: array([0.])\n",
    "\n",
    "minimize(np.square, 2).x\n",
    "#out: array([-1.88846401e-08])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model on the Boston housing price data set, which is \n",
    "# already loaded into the variables X and y. For simplicity, we \n",
    "# won't include an intercept in our regression model.\n",
    "\n",
    "# The squared error, summed over training examples\n",
    "# Several ways to get the number of training examples, \n",
    "# such as y.size, len(y), or len(X)\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        # Get the true and predicted target values for example 'i'\n",
    "        y_i_true = y[i]\n",
    "        y_i_pred = w@X[i]\n",
    "        s = s + (y_i_pred - y_i_true)**2\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x\n",
    "print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LinearRegression coefficients\n",
    "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
    "print(lr.coef_)\n",
    "\n",
    "# [-9.16299112e-02  4.86754828e-02 -3.77698794e-03  2.85635998e+00\n",
    "#  -2.88057050e+00  5.92521269e+00 -7.22470732e-03 -9.67992974e-01\n",
    "#   1.70448714e-01 -9.38971600e-03 -3.92421893e-01  1.49830571e-02\n",
    "#  -4.16973012e-01]\n",
    "# [-9.16297843e-02  4.86751203e-02 -3.77930006e-03  2.85636751e+00\n",
    "#  -2.88077933e+00  5.92521432e+00 -7.22447929e-03 -9.67995240e-01\n",
    "#   1.70443393e-01 -9.38925373e-03 -3.92425680e-01  1.49832102e-02\n",
    "#  -4.16972624e-01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function diagrams\n",
    "- raw model output\n",
    "- 0-1 loss diagram\n",
    "- linear regression loss diagram - using least squared\n",
    "    - the raw model output is the prediction here\n",
    "    - the squared error doesn't make sense here\n",
    "- logistic loss diagram\n",
    "    - think of it as \"smooth version of 0-1 loss\"\n",
    "- Hinge loss diagram for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the logistic and hinge losses\n",
    "- log_loss\n",
    "- hinge_loss\n",
    "\n",
    "In this exercise you'll create a plot of the logistic and hinge losses using their mathematical expressions, which are provided to you. The loss function diagram from the video is shown on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical functions for logistic and hinge losses\n",
    "# Feel free to ignore if you're not interested\n",
    "def log_loss(raw_model_output):\n",
    "   return np.log(1+np.exp(-raw_model_output))\n",
    "def hinge_loss(raw_model_output):\n",
    "   return np.maximum(0,1-raw_model_output)\n",
    "\n",
    "# Create a grid of values and plot\n",
    "grid = np.linspace(-2,2,1000)\n",
    "plt.plot(grid, log_loss(grid), label='logistic')\n",
    "plt.plot(grid, hinge_loss(grid), label='hinge')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing logistic regression\n",
    "This is very similar to the earlier exercise where you implemented linear regression \"from scratch\" using scipy.optimize.minimize. However, this time we'll minimize the logistic loss and compare with scikit-learn's LogisticRegression (we've set C to a large value to disable regularization; more on this in Chapter 3!). \n",
    "\n",
    "The log_loss function from the previous exercise is already defined in your environment, and the sklearn breast cancer prediction dataset (first 10 features, standardized) is loaded into the variables X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression is just minimizing the loss function \n",
    "# we've been looking at. \n",
    "\n",
    "# The logistic loss, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        raw_model_output = w@X[i]\n",
    "        s = s + log_loss(raw_model_output * y[i])\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x\n",
    "print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LogisticRegression\n",
    "lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)\n",
    "print(lr.coef_)\n",
    "\n",
    "# [ 1.03592182 -1.65378492  4.08331342 -9.40923002 -1.06786489  0.07892114\n",
    "#  -0.85110344 -2.44103305 -0.45285671  0.43353448]\n",
    "# [[ 1.03731085 -1.65339037  4.08143924 -9.40788356 -1.06757746  0.07895582\n",
    "#   -0.85072003 -2.44079089 -0.45271     0.43334997]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic regression and regularization\n",
    "- Regularization combats overfitting by making the coefficients smaller\n",
    "\n",
    "In sklearn, hyperparameter C is the inverse of the regularization strength\n",
    "- larger c means less regularization\n",
    "- smaller c means more regularization (make coefficients smaller)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does regularization affect TRAINING accuracy?\n",
    "\n",
    "# weak regularization\n",
    "lr_weak_reg = LogisticRegression(C=100)\n",
    "# strong regularization\n",
    "lr_strong_reg = LogisticRegression(C=0.01)\n",
    "\n",
    "# fit both models\n",
    "lr_weak_reg.fit(X_train, y_train)\n",
    "lr_strong_reg.fit(X_train, y_train)\n",
    "\n",
    "# compute training accuracy\n",
    "# Model with weaker regularization gets higher training accuracy\n",
    "lr_weak_reg.score(X_train, y_train)\n",
    "# 1.0\n",
    "lr_strong_reg.score(X_train, y_train)\n",
    "# 0.92\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularized loss = original loss + large coefficient penalty\n",
    "- more regularization: lower training accuracy\n",
    "- Regularization reduces training accuracy due to adding a penalty to the original loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does regularization affect TEST accuracy?\n",
    "\n",
    "# compute training accuracy\n",
    "# Model with weaker regularization gets higher training accuracy\n",
    "lr_weak_reg.score(X_test, y_test)\n",
    "# 0.86\n",
    "lr_strong_reg.score(X_test, y_test)\n",
    "# 0.88\n",
    "\n",
    "# Regularization improves test accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More regularization (almost always) higher TEST accuracy\n",
    "- it's like a compromise on the coefficient\n",
    "- regularization makes you \"fit less\", so overfit less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 vs L2 regularization\n",
    "Linear regularization - Ridge and Lasso\n",
    "- Lasso = linear regression with L1 regularization\n",
    "- Ridge = linear regression with L2 regularization\n",
    "- For other models like logistic regression we just say L1, L2, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_L1 = LogisticRegression(penalty='l1') # L1 regularization\n",
    "lr_L2 = LogisticRegression() # penalty='l2' by default (L2 regularization)\n",
    "lr_L1.fit(X_train, y_train)\n",
    "lr_L2.fit(X_train, y_train)\n",
    "# plot coefficients for both models\n",
    "plt.plot(lr_L1.coef_.flatten())\n",
    "plt.plot(lr_L2.coef_.flatten())\n",
    "\n",
    "# L1 regularization set many features to 0, so did feature selection\n",
    "# L2 regularizaiton shrinks coefficients smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression\n",
    "In Chapter 1 you used logistic regression on the handwritten digits data set. Here, we'll explore the effect of L2 regularization. The handwritten digits dataset is already loaded, split, and stored in the variables X_train, y_train, X_valid, and y_valid. The variables train_errs and valid_errs are already initialized as empty lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over values of C\n",
    "for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    # Create LogisticRegression object and fit\n",
    "    lr = LogisticRegression(C=C_value)\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate error rates and append to lists\n",
    "    train_errs.append( 1.0 - lr.score(X_train,y_train) )\n",
    "    valid_errs.append( 1.0 - lr.score(X_valid,y_valid) )\n",
    "    \n",
    "# Plot results of the training and testing error as a \n",
    "# function of the regularization parameter, C\n",
    "plt.semilogx(C_values, train_errs, C_values, valid_errs)\n",
    "plt.legend((\"train\", \"validation\"))\n",
    "plt.show()\n",
    "\n",
    "# Looking at the plot, what's the best value of C?\n",
    "# 10e-1 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression and feature selection\n",
    "In this exercise we'll perform feature selection on the movie review sentiment data set using L1 regularization. \n",
    "\n",
    "The features and targets are already loaded for you in X_train and y_train. We'll search for the best value of C using scikit-learn's GridSearchCV, which was covered in the prerequisite course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify L1 regularization\n",
    "lr = LogisticRegression(penalty='l1')\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "\n",
    "# Find the number of nonzero coefficients (selected features)\n",
    "best_lr = searcher.best_estimator_\n",
    "coefs = best_lr.coef_\n",
    "print(\"Total number of features:\", coefs.size)\n",
    "print(\"Number of selected features:\", np.count_nonzero(coefs))\n",
    "\n",
    "# Best CV params {'C': 1}\n",
    "# Total number of features: 2500\n",
    "# Number of selected features: 1219"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the most positive and negative words\n",
    "In this exercise we'll try to interpret the coefficients of a logistic regression fit on the movie review sentiment data set. The model object is already instantiated and fit for you in the variable lr.\n",
    "\n",
    "In addition, the words corresponding to the different features are loaded into the variable vocab. For example, since vocab[100] is \"think\", that means feature 100 corresponds to the number of times the word \"think\" appeared in that movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the sorted cofficients\n",
    "inds_ascending = np.argsort(lr.coef_.flatten()) \n",
    "inds_descending = inds_ascending[::-1]\n",
    "\n",
    "# Print the most positive words\n",
    "print(\"Most positive words: \", end=\"\")\n",
    "for i in range(5):\n",
    "    print(vocab[inds_descending[i]], end=\", \")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print most negative words\n",
    "print(\"Most negative words: \", end=\"\")\n",
    "for i in range(5):\n",
    "    print(vocab[inds_ascending[i]], end=\", \")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Most positive words: favorite, superb, noir, knowing, loved, \n",
    "\n",
    "# Most negative words: disappointing, waste, worst, boring, lame,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression and Probabilities\n",
    "- interpreting raw model output as a (predicted) probability\n",
    "\n",
    "With regularization:\n",
    "- coefficients are smaller, which means less confident predictions (decreased probabilities)\n",
    "- also affects orientation of the boundary (like slope)\n",
    "\n",
    "How are these probabilities computed?\n",
    "- logistic regression predicitons: sign of raw model output\n",
    "- logistic regression probabilities: \"squashed\" raw model output\n",
    "    - b/n 0 and 1\n",
    "\n",
    "Looks like an ECDF...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - with more Regularization > smaller coefficients > less confident predictions > \n",
    "- raw model outputs closer to zero > probabilities closer to 0.5 after the raw model output is squashed through the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Regularization and probabilities\n",
    "In this exercise, you will observe the effects of changing the regularization stength on the predicted probabilities. A 2D binary classification dataset is already loaded into the environment as X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute maximum predicted probability\n",
    "\n",
    "# Set the regularization strength\n",
    "model = LogisticRegression(C=1)\n",
    "\n",
    "# Fit and plot\n",
    "model.fit(X,y)\n",
    "plot_classifier(X,y,model,proba=True)\n",
    "\n",
    "# Predict probabilities on training points\n",
    "prob = model.predict_proba(X)\n",
    "print(\"Maximum predicted probability\", np.max(prob))\n",
    "# Maximum predicted probability 0.9761229966765974\n",
    "\n",
    "# Create a model with C=0.1 and examine how the plot and probabilities \n",
    "# change.\n",
    "# Maximum predicted probability 0.8990965659596716\n",
    "# Smaller C leads to:\n",
    "# - decreased probabilities (less confident predictions)\n",
    "# - shifted boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing easy and difficult examples\n",
    "In this exercise, you'll visualize the examples that the logistic regression model is most, and least, confident about by looking at the largest, and smallest, predicted probabilities. \n",
    "\n",
    "The handwritten digits dataset is already loaded into the variables X and y. The show_digit function takes in an integer index and plots the corresponding image, with some extra information displayed above the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "\n",
    "# Get predicted probabilities\n",
    "proba = lr.predict_proba(X)\n",
    "\n",
    "# Sort the example indices by their maximum probability\n",
    "proba_inds = np.argsort(np.max(proba,axis=1))\n",
    "\n",
    "# Show the most confident (least ambiguous) digit\n",
    "show_digit(proba_inds[-1], lr)\n",
    "\n",
    "# Show the least confident (most ambiguous) digit\n",
    "show_digit(proba_inds[0], lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class logistic regression (2+ classes)\n",
    "2 methods:\n",
    "1. Combining binary classifiers with one-vs-rest strategy\n",
    "2. Multinomial/softmax/cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-vs-rest strategy\n",
    "lr0.fit(X, y==0)\n",
    "lr1.fit(X, y==1)\n",
    "lr2.fit(X, y==2)\n",
    "\n",
    "# get raw model output\n",
    "lr0.decision_function(X)[0]\n",
    "# 6.124\n",
    "lr1.decision_function(X)[0]\n",
    "# -5.429\n",
    "lr2.decision_function(X)[0]\n",
    "# -7.532\n",
    "\n",
    "# Use largest raw model output\n",
    "# Which is lr0. It's more confident that the class is 0 than others.\n",
    "\n",
    "#Check answer:\n",
    "# One-vs-rest is the default strategy of sklearn logistic regression\n",
    "lr.fit(X, y)\n",
    "lr.predict(X)[0]\n",
    "# 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-vs-rest vs Multinomial/softmax/cross entropy loss\n",
    "\n",
    "One-vs-rest\n",
    "- fit a binary classifier for each class\n",
    "- predict with all, take largest output\n",
    "- pro: simple, modular\n",
    "- con: not directly optimizing accuracy\n",
    "- common for SVMs as well\n",
    "- can produce probabilities\n",
    "Multinomial/softmax/cross entropy loss\n",
    "- fit a single classifier for all classes (one time fit)\n",
    "- prediction directly outputs best class\n",
    "- con: more complicated, new code\n",
    "- pro: tackle the problem directly\n",
    "- possible for SVMs, but less common\n",
    "- can produce probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model coefficients for multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-vs-rest by default\n",
    "lr_ovr = LogisticRegression()\n",
    "lr_ovr.fit(X,y)\n",
    "lr_ovr.coef_.shape\n",
    "# (3,13)\n",
    "# for 3 classes: 1 coeff per feature per class and 1 intercept per class\n",
    "lr_ovr.intercept_.shape\n",
    "# (3,)\n",
    "\n",
    "# multinomial\n",
    "# solver specifies algorithm to minimize loss\n",
    "lr_mn = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\")\n",
    "lr_mn.fit(X,y)\n",
    "lr_mn.coef_.shape\n",
    "# (3,13)\n",
    "lr_ovr.intercept_.shape\n",
    "# (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice counting coefficients\n",
    "If you fit a logistic regression model on a classification problem with 3 classes and 100 features, how many coefficients would you have, including intercepts?\n",
    "\n",
    "3x100 + 3 = 303 coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting multi-class logistic regression\n",
    "In this exercise, you'll fit the two types of multi-class logistic regression, one-vs-rest and softmax/multinomial, on the handwritten digits data set and compare the results. \n",
    "\n",
    "The handwritten digits dataset is already loaded and split into X_train, y_train, X_test, and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit one-vs-rest logistic regression classifier\n",
    "lr_ovr = LogisticRegression()\n",
    "lr_ovr.fit(X_train, y_train)\n",
    "\n",
    "print(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "print(\"OVR test accuracy    :\", lr_ovr.score(X_test, y_test))\n",
    "\n",
    "# Fit softmax classifier\n",
    "lr_mn = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\")\n",
    "lr_mn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "print(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))\n",
    "\n",
    "# OVR training accuracy: 0.9948032665181886\n",
    "# OVR test accuracy    : 0.9644444444444444\n",
    "# Softmax training accuracy: 1.0\n",
    "# Softmax test accuracy    : 0.9688888888888889\n",
    "\n",
    "# the accuracies for both methods are similar on this data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing multi-class logistic regression\n",
    "In this exercise we'll continue with the two types of multi-class logistic regression, but on a toy 2D data set specifically designed to break the one-vs-rest scheme. \n",
    "\n",
    "The data set is loaded into X_train and y_train. The two logistic regression objects, lr_mn and lr_ovr, are already instantiated (with C=100), fit, and plotted. Notice that lr_ovr never predicts the dark blue class... yikes! Let's explore why this happens by plotting one of the binary classifiers that it's using behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training accuracies\n",
    "print(\"Softmax     training accuracy:\", lr_mn.score(X_train, y_train))\n",
    "print(\"One-vs-rest training accuracy:\", lr_ovr.score(X_train, y_train))\n",
    "\n",
    "# Create the binary classifier (class 1 vs. rest)\n",
    "lr_class_1 = LogisticRegression(C=100)\n",
    "lr_class_1.fit(X_train, y_train==1)\n",
    "\n",
    "# Plot the binary classifier (class 1 vs. rest)\n",
    "plot_classifier(X_train, y_train==1, lr_class_1)\n",
    "\n",
    "# Softmax     training accuracy: 0.996\n",
    "# One-vs-rest training accuracy: 0.916\n",
    "\n",
    "# Although it didn't work well here,\n",
    "# in general, one-vs-rest often works well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-vs-rest SVM\n",
    "As motivation for the next and final chapter on support vector machines, we'll repeat the previous exercise with a non-linear SVM. \n",
    "\n",
    "Once again, the data is loaded into X_train, y_train, X_test, and y_test . Instead of using LinearSVC, we'll now use scikit-learn's SVC object, which is a non-linear \"kernel\" SVM (much more on what this means in Chapter 4!). Again, your task is to create a plot of the binary classifier for class 1 vs. rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use SVC instead of LinearSVC from now on\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create/plot the binary classifier (class 1 vs. rest)\n",
    "svm_class_1 = SVC()\n",
    "svm_class_1.fit(X_train, y_train==1)\n",
    "plot_classifier(X_train, y_train==1, svm_class_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Support Vector Machines - SVM\n",
    "- hinge loss and L2 regularization\n",
    "\n",
    "What is an SVM?\n",
    "- linear classifiers\n",
    "- trained using: hinge loss and L2 regularization\n",
    "What are support vectors?\n",
    "- support vector - a training example NOT in the flat part of the loss diagram\n",
    "- support vector - an example that is incorrectly classified OR correctly classified but close to the boundary\n",
    "- if an example is not a support vector, removing it has no effect on the model\n",
    "- Having a small number of support vectors makes kernel SVMs really fast\n",
    "Max-margin viewpoint\n",
    "- the SVM maximizes the \"margin\" for linearly separable datasets\n",
    "- Margin: distance from the boundary to the closest points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of removing examples - remove non support vectors\n",
    "Support vectors are defined as training examples that influence the decision boundary. In this exercise, you'll observe this behavior by removing non support vectors from the training set. \n",
    "\n",
    "The wine quality dataset is already loaded into X and y (first two features only). (Note: we specify lims in plot_classifier so that the two plots are forced to use the same axis limits and can be compared directly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear SVM\n",
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X,y)\n",
    "plot_classifier(X, y, svm, lims=(11,15,0,6))\n",
    "\n",
    "# Make a new data set keeping only the support vectors\n",
    "print(\"Number of original examples\", len(X))\n",
    "print(\"Number of support vectors\", len(svm.support_))\n",
    "X_small = X[svm.support_]\n",
    "y_small = y[svm.support_]\n",
    "\n",
    "# Train a new SVM using only the support vectors\n",
    "svm_small = SVC(kernel=\"linear\")\n",
    "svm_small.fit(X_small,y_small)\n",
    "plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))\n",
    "\n",
    "# Number of original examples 178\n",
    "# Number of support vectors 81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVMs\n",
    "- Transform your features\n",
    "    ie. transformed feature = (original feature)**2\n",
    "- Fitting a linear model in a transformed space corresponds to fitting a nonlinear model in the original space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel SVMs\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(gamma=1) # default is kernel=\"rbf\"\n",
    "\n",
    "# gamma controls the smoothness of the boundary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV warm-up\n",
    "In the video we saw that increasing the RBF kernel hyperparameter gamma increases training accuracy. In this exercise we'll search for the gamma that maximizes cross-validation accuracy using scikit-learn's GridSearchCV. \n",
    "\n",
    "A binary version of the handwritten digits dataset, in which you're just trying to predict whether or not an image is a \"2\", is already loaded into the variables X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "# Using default C=1\n",
    "parameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "searcher.fit(X, y)\n",
    "\n",
    "# Report the best parameters\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "\n",
    "# Best CV params {'gamma': 0.001}\n",
    "\n",
    "# Larger values of gamma are better for training accuracy, but \n",
    "# cross-validation helped us find something different (and better!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jointly tuning gamma and C with GridSearchCV\n",
    "In the previous exercise the best value of gamma was 0.001 using the default value of C, which is 1. \n",
    "\n",
    "In this exercise you'll search for the best combination of C and gamma using GridSearchCV. As in the previous exercise, the 2-vs-not-2 digits dataset is already loaded, but this time it's split into the variables X_train, y_train, X_test, and y_test. Even though cross-validation already splits the training set into parts, it's often a good idea to hold out a separate test set to make sure the cross-validation results are sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an RBF SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "parameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n",
    "searcher = GridSearchCV(svm, parameters)\n",
    "searcher.fit(X_train,y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "\n",
    "# Report the test accuracy using these best parameters\n",
    "print(\"Test accuracy of best grid search hypers:\", \n",
    "      searcher.score(X_test, y_test))\n",
    "\n",
    "# Best CV params {'gamma': 0.0001, 'C': 10}\n",
    "# Best CV accuracy 0.9988864142538976\n",
    "# Test accuracy of best grid search hypers: 0.9988876529477196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing logistic regression and SVM\n",
    "Logistic regression\n",
    "- is a linear classifier\n",
    "- can use with kernels, but SLOW\n",
    "- Outputs meaningful probabilities (more NATURAL)\n",
    "- Can be extended to multi-class\n",
    "- all data points affect fit\n",
    "- L2 or L1 regularization\n",
    "\n",
    "SVM\n",
    "- also a linear classifier\n",
    "- can use with kernels, and FAST\n",
    "- Does not naturally output probabilities (NOT natural)\n",
    "- can be extended to multi-class\n",
    "- special property: Only \"support vectors\" affect fit\n",
    "- conventionally just L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: logistic regression\n",
    "in sklearn:\n",
    "- linear_model.LogisticRegression\n",
    "Key hyperparameters:\n",
    "- C (inverse regularization strength)\n",
    "- penalty (type of regularization - L1 and L2)\n",
    "- multi_class (type of multi-class)\n",
    "- ...and more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: SVM in sklearn\n",
    "- svm.LinearSVC and svm.SVC\n",
    "Key hyperparameters:\n",
    "- C (inverse regularization strength)\n",
    "- kernel (type of kernel)\n",
    "- gamma (inverse RBF smoothness)\n",
    "    - smaller gamma leads to smoother/simpler boundaries\n",
    "    - bigger gamma leads to more complex boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier (SGD=stochastic gradient descent)\n",
    "- SGDClassifier: scales well to large datasets\n",
    "    - ***** One advantage of SGDClassifier is that it's very fast - this would have taken a lot longer with LogisticRegression or LinearSVC.\n",
    "- just to specify the 'loss'\n",
    "\n",
    "Note:\n",
    "- SGDClassifier hyperparameter 'alpha' is like '1/C'\n",
    "    - bigger alpha > more regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGDClassfier\n",
    "from skelearn.linear_model import SGDClassifier\n",
    "logreg = SGDClassifier(loss='log')\n",
    "linsvm = SGDClassifier(loss='hinge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SGDClassifier\n",
    "In this final coding exercise, you'll do a hyperparameter search over the regularization type, regularization strength, and the loss (logistic regression vs. linear SVM) using SGDClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set random_state=0 for reproducibility \n",
    "linear_classifier = SGDClassifier(random_state=0)\n",
    "\n",
    "# Instantiate the GridSearchCV object and run the search\n",
    "# Search over the regularization strength, the hinge vs. log losses,\n",
    "# and L1 vs. L2 regularization.\n",
    "parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n",
    "             'loss':['hinge','log'], 'penalty':['l1','l2']}\n",
    "searcher = GridSearchCV(linear_classifier, parameters, cv=10)\n",
    "searcher.fit(X_train, y_train)\n",
    "\n",
    "# Report the best parameters and the corresponding score\n",
    "print(\"Best CV params\", searcher.best_params_)\n",
    "print(\"Best CV accuracy\", searcher.best_score_)\n",
    "print(\"Test accuracy of best grid search hypers:\", \n",
    "      searcher.score(X_test, y_test))\n",
    "\n",
    "# Best CV params {'alpha': 0.0001, 'loss': 'hinge', 'penalty': 'l1'}\n",
    "# Best CV accuracy 0.94351630867144\n",
    "# Test accuracy of best grid search hypers: 0.9592592592592593\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
