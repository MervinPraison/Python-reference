{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions to consider\n",
    "- how to deal with misspellings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Skip polyglot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP pipeline\n",
    "Objective: create a NLP pipeline that is reusable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other things to check\n",
    "- n-grams - ie. word pairs in 2-gram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Amazon product reviews\n",
    "Data source:\n",
    "- https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M\n",
    "- amazon_review_full_csv.tar.gz\n",
    "\n",
    "Data overview:\n",
    "- Amazon reviews full score dataset is constructed by randomly taking 600,000 training samples and 130,000 testing samples for each review score from 1 to 5. \n",
    "- In total there are 3,000,000 training samples and 650,000 testing samples.\n",
    "- The files train.csv and test.csv contain all the training samples as comma-sparated values. \n",
    "- There are 3 columns in them, corresponding to class index (1 to 5), review title and review text. \n",
    "- The review title and text are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). New lines are escaped by a backslash followed with an \"n\" character, that is \"\\n\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:03:20.383198Z",
     "start_time": "2018-11-26T07:03:17.363795Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import (word_tokenize, sent_tokenize, \n",
    "regexp_tokenize)\n",
    "# for tweets\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from gensim import corpora, models, similarities\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "import spacy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import (CountVectorizer, \n",
    "TfidfVectorizer)\n",
    "# NB works well with CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we need to display images\n",
    "from IPython.display import Image, SVG\n",
    "SVG(filename='Images/nlp_linelength.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:03:26.724045Z",
     "start_time": "2018-11-26T07:03:26.716250Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = r'/Users/joe/Documents/GitHub/portfolio/support files/\\\n",
    "amazon_review_full_csv'\n",
    "trainfile = 'train.csv'\n",
    "testfile = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:03:28.345056Z",
     "start_time": "2018-11-26T07:03:28.339090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/joe/Documents/GitHub/Python-reference'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:03:48.471614Z",
     "start_time": "2018-11-26T07:03:33.859854Z"
    }
   },
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(os.path.join(filepath, trainfile), sep=\",\",\n",
    "                  names=('rating','title','review'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:03:51.808481Z",
     "start_time": "2018-11-26T07:03:48.473750Z"
    }
   },
   "outputs": [],
   "source": [
    "testdf = pd.read_csv(os.path.join(filepath, testfile), sep=\",\",\n",
    "                  names=('rating','title','review'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:03:51.813852Z",
     "start_time": "2018-11-26T07:03:51.810308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000000, 3) (650000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(traindf.shape, testdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:03:53.585266Z",
     "start_time": "2018-11-26T07:03:53.573341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>more like funchuck</td>\n",
       "      <td>Gave this to my dad for a gag gift after direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Inspiring</td>\n",
       "      <td>I hope a lot of people hear this cd. We need m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Chrono Cross OST</td>\n",
       "      <td>The music of Yasunori Misuda is without questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Too good to be true</td>\n",
       "      <td>Probably the greatest soundtrack in history! U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                  title  \\\n",
       "0       3                     more like funchuck   \n",
       "1       5                              Inspiring   \n",
       "2       5  The best soundtrack ever to anything.   \n",
       "3       4                       Chrono Cross OST   \n",
       "4       5                    Too good to be true   \n",
       "\n",
       "                                              review  \n",
       "0  Gave this to my dad for a gag gift after direc...  \n",
       "1  I hope a lot of people hear this cd. We need m...  \n",
       "2  I'm reading a lot of reviews saying that this ...  \n",
       "3  The music of Yasunori Misuda is without questi...  \n",
       "4  Probably the greatest soundtrack in history! U...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T04:55:22.920476Z",
     "start_time": "2018-11-26T04:55:22.909130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>mens ultrasheer</td>\n",
       "      <td>This model may be ok for sedentary types, but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Surprisingly delightful</td>\n",
       "      <td>This is a fast read filled with unexpected hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Works, but not as advertised</td>\n",
       "      <td>I bought one of these chargers..the instructio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Oh dear</td>\n",
       "      <td>I was excited to find a book ostensibly about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Incorrect disc!</td>\n",
       "      <td>I am a big JVC fan, but I do not like this mod...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                         title  \\\n",
       "0       1               mens ultrasheer   \n",
       "1       4       Surprisingly delightful   \n",
       "2       2  Works, but not as advertised   \n",
       "3       2                       Oh dear   \n",
       "4       2               Incorrect disc!   \n",
       "\n",
       "                                              review  \n",
       "0  This model may be ok for sedentary types, but ...  \n",
       "1  This is a fast read filled with unexpected hum...  \n",
       "2  I bought one of these chargers..the instructio...  \n",
       "3  I was excited to find a book ostensibly about ...  \n",
       "4  I am a big JVC fan, but I do not like this mod...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:03:58.012816Z",
     "start_time": "2018-11-26T07:03:58.009147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gave this to my dad for a gag gift after directing \"Nunsense,\" he got a reall kick out of it!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.iloc[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:03:58.842373Z",
     "start_time": "2018-11-26T07:03:58.828191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gave',\n",
       " 'this',\n",
       " 'to',\n",
       " 'my',\n",
       " 'dad',\n",
       " 'for',\n",
       " 'a',\n",
       " 'gag',\n",
       " 'gift',\n",
       " 'after',\n",
       " 'directing',\n",
       " '``',\n",
       " 'Nunsense',\n",
       " ',',\n",
       " \"''\",\n",
       " 'he',\n",
       " 'got',\n",
       " 'a',\n",
       " 'reall',\n",
       " 'kick',\n",
       " 'out',\n",
       " 'of',\n",
       " 'it',\n",
       " '!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(traindf.iloc[0,2])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T04:48:53.433722Z",
     "start_time": "2018-11-26T04:48:53.116182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a25e1c9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a list of word lengths\n",
    "word_lengths = [len(w) for w in words]\n",
    "\n",
    "plt.hist(word_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:04:03.686483Z",
     "start_time": "2018-11-26T07:04:03.681025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I hope a lot of people hear this cd.',\n",
       " 'We need more strong and positive vibes like this.',\n",
       " 'Great vocals, fresh tunes, cross-cultural happiness.',\n",
       " 'Her blues is from the gut.',\n",
       " 'The pop sounds are catchy and mature.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(traindf.iloc[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create gensim dictionary and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:04:06.254392Z",
     "start_time": "2018-11-26T07:04:06.250301Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gave this to my dad for a gag gift after directing \"Nunsense,\" he got a reall kick out of it!',\n",
       " 'I hope a lot of people hear this cd. We need more strong and positive vibes like this. Great vocals, fresh tunes, cross-cultural happiness. Her blues is from the gut. The pop sounds are catchy and mature.',\n",
       " \"I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\",\n",
       " 'The music of Yasunori Misuda is without question my close second below the great Nobuo Uematsu.Chrono Cross OST is a wonderful creation filled with rich orchestra and synthesized sounds. While ambiance is one of the music\\'s major factors, yet at times it\\'s very uplifting and vigorous. Some of my favourite tracks include; \"Scars Left by Time, The Girl who Stole the Stars, and Another World\".',\n",
       " \"Probably the greatest soundtrack in history! Usually it's better to have played the game first but this is so enjoyable anyway! I worked so hard getting this soundtrack and after spending [money] to get it it was really worth every penny!! Get this OST! it's amazing! The first few tracks will have you dancing around with delight (especially Scars Left by Time)!! BUY IT NOW!!\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.review.head(5).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:04:14.941061Z",
     "start_time": "2018-11-26T07:04:14.927608Z"
    }
   },
   "outputs": [],
   "source": [
    "# test\n",
    "my_documents = traindf.review.head(10).values.tolist()\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "# print(dictionary.token2id)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T04:57:40.854741Z",
     "start_time": "2018-11-26T04:57:40.832915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 2),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1)],\n",
       " [(2, 2),\n",
       "  (4, 1),\n",
       "  (18, 1),\n",
       "  (21, 2),\n",
       "  (23, 5),\n",
       "  (24, 2),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1),\n",
       "  (50, 2),\n",
       "  (51, 1),\n",
       "  (52, 1),\n",
       "  (53, 1),\n",
       "  (54, 1)],\n",
       " [(2, 2),\n",
       "  (4, 3),\n",
       "  (8, 2),\n",
       "  (14, 1),\n",
       "  (16, 1),\n",
       "  (18, 1),\n",
       "  (21, 5),\n",
       "  (22, 4),\n",
       "  (23, 3),\n",
       "  (24, 3),\n",
       "  (25, 1),\n",
       "  (28, 1),\n",
       "  (38, 6),\n",
       "  (39, 5),\n",
       "  (41, 1),\n",
       "  (50, 3),\n",
       "  (55, 1),\n",
       "  (56, 1),\n",
       "  (57, 1),\n",
       "  (58, 2),\n",
       "  (59, 1),\n",
       "  (60, 1),\n",
       "  (61, 1),\n",
       "  (62, 1),\n",
       "  (63, 1),\n",
       "  (64, 1),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (68, 1),\n",
       "  (69, 1),\n",
       "  (70, 1),\n",
       "  (71, 1),\n",
       "  (72, 1),\n",
       "  (73, 1),\n",
       "  (74, 1),\n",
       "  (75, 1),\n",
       "  (76, 1),\n",
       "  (77, 1),\n",
       "  (78, 1),\n",
       "  (79, 1),\n",
       "  (80, 1),\n",
       "  (81, 1),\n",
       "  (82, 1),\n",
       "  (83, 1),\n",
       "  (84, 1),\n",
       "  (85, 1),\n",
       "  (86, 1),\n",
       "  (87, 1),\n",
       "  (88, 1),\n",
       "  (89, 1),\n",
       "  (90, 1),\n",
       "  (91, 1),\n",
       "  (92, 1),\n",
       "  (93, 1),\n",
       "  (94, 1),\n",
       "  (95, 1),\n",
       "  (96, 1),\n",
       "  (97, 1),\n",
       "  (98, 1),\n",
       "  (99, 1),\n",
       "  (100, 1),\n",
       "  (101, 1),\n",
       "  (102, 3),\n",
       "  (103, 1),\n",
       "  (104, 1),\n",
       "  (105, 1),\n",
       "  (106, 1),\n",
       "  (107, 1),\n",
       "  (108, 1),\n",
       "  (109, 1),\n",
       "  (110, 1)],\n",
       " [(1, 1),\n",
       "  (2, 3),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (14, 1),\n",
       "  (16, 2),\n",
       "  (18, 3),\n",
       "  (23, 3),\n",
       "  (24, 3),\n",
       "  (32, 1),\n",
       "  (39, 3),\n",
       "  (48, 1),\n",
       "  (50, 5),\n",
       "  (59, 2),\n",
       "  (82, 2),\n",
       "  (86, 1),\n",
       "  (108, 1),\n",
       "  (111, 1),\n",
       "  (112, 1),\n",
       "  (113, 1),\n",
       "  (114, 1),\n",
       "  (115, 1),\n",
       "  (116, 1),\n",
       "  (117, 1),\n",
       "  (118, 1),\n",
       "  (119, 1),\n",
       "  (120, 1),\n",
       "  (121, 1),\n",
       "  (122, 1),\n",
       "  (123, 1),\n",
       "  (124, 1),\n",
       "  (125, 1),\n",
       "  (126, 1),\n",
       "  (127, 1),\n",
       "  (128, 1),\n",
       "  (129, 1),\n",
       "  (130, 1),\n",
       "  (131, 1),\n",
       "  (132, 1),\n",
       "  (133, 1),\n",
       "  (134, 1),\n",
       "  (135, 1),\n",
       "  (136, 1),\n",
       "  (137, 1),\n",
       "  (138, 1),\n",
       "  (139, 1),\n",
       "  (140, 1),\n",
       "  (141, 1),\n",
       "  (142, 1),\n",
       "  (143, 1),\n",
       "  (144, 1),\n",
       "  (145, 1),\n",
       "  (146, 1),\n",
       "  (147, 1),\n",
       "  (148, 1),\n",
       "  (149, 1),\n",
       "  (150, 1),\n",
       "  (151, 1),\n",
       "  (152, 1)],\n",
       " [(0, 10),\n",
       "  (5, 1),\n",
       "  (14, 5),\n",
       "  (21, 3),\n",
       "  (22, 2),\n",
       "  (24, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (50, 3),\n",
       "  (59, 2),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (69, 1),\n",
       "  (75, 1),\n",
       "  (80, 1),\n",
       "  (84, 1),\n",
       "  (89, 1),\n",
       "  (99, 2),\n",
       "  (105, 1),\n",
       "  (110, 1),\n",
       "  (116, 1),\n",
       "  (125, 1),\n",
       "  (130, 1),\n",
       "  (133, 1),\n",
       "  (139, 1),\n",
       "  (141, 1),\n",
       "  (148, 1),\n",
       "  (153, 1),\n",
       "  (154, 1),\n",
       "  (155, 1),\n",
       "  (156, 1),\n",
       "  (157, 1),\n",
       "  (158, 1),\n",
       "  (159, 1),\n",
       "  (160, 1),\n",
       "  (161, 1),\n",
       "  (162, 1),\n",
       "  (163, 1),\n",
       "  (164, 1),\n",
       "  (165, 1),\n",
       "  (166, 2),\n",
       "  (167, 1),\n",
       "  (168, 2),\n",
       "  (169, 1),\n",
       "  (170, 1),\n",
       "  (171, 1),\n",
       "  (172, 2),\n",
       "  (173, 1),\n",
       "  (174, 1),\n",
       "  (175, 1),\n",
       "  (176, 1),\n",
       "  (177, 2),\n",
       "  (178, 1),\n",
       "  (179, 1),\n",
       "  (180, 1),\n",
       "  (181, 1),\n",
       "  (182, 1)],\n",
       " [(2, 1),\n",
       "  (4, 1),\n",
       "  (18, 2),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 3),\n",
       "  (28, 1),\n",
       "  (38, 2),\n",
       "  (39, 1),\n",
       "  (50, 2),\n",
       "  (59, 3),\n",
       "  (64, 1),\n",
       "  (69, 3),\n",
       "  (82, 1),\n",
       "  (96, 1),\n",
       "  (102, 2),\n",
       "  (177, 1),\n",
       "  (183, 1),\n",
       "  (184, 1),\n",
       "  (185, 1),\n",
       "  (186, 1),\n",
       "  (187, 1),\n",
       "  (188, 1),\n",
       "  (189, 1),\n",
       "  (190, 1),\n",
       "  (191, 1),\n",
       "  (192, 1),\n",
       "  (193, 1),\n",
       "  (194, 1),\n",
       "  (195, 1),\n",
       "  (196, 1),\n",
       "  (197, 1),\n",
       "  (198, 1),\n",
       "  (199, 1)],\n",
       " [(0, 3),\n",
       "  (1, 2),\n",
       "  (2, 4),\n",
       "  (3, 2),\n",
       "  (4, 9),\n",
       "  (14, 2),\n",
       "  (16, 1),\n",
       "  (18, 4),\n",
       "  (21, 3),\n",
       "  (22, 6),\n",
       "  (23, 4),\n",
       "  (24, 5),\n",
       "  (25, 1),\n",
       "  (31, 1),\n",
       "  (38, 5),\n",
       "  (39, 2),\n",
       "  (50, 2),\n",
       "  (54, 1),\n",
       "  (59, 1),\n",
       "  (61, 1),\n",
       "  (63, 1),\n",
       "  (74, 1),\n",
       "  (75, 2),\n",
       "  (83, 1),\n",
       "  (85, 1),\n",
       "  (86, 2),\n",
       "  (92, 2),\n",
       "  (95, 1),\n",
       "  (110, 2),\n",
       "  (113, 1),\n",
       "  (116, 2),\n",
       "  (135, 1),\n",
       "  (148, 1),\n",
       "  (158, 1),\n",
       "  (165, 1),\n",
       "  (172, 1),\n",
       "  (185, 1),\n",
       "  (186, 1),\n",
       "  (189, 1),\n",
       "  (200, 5),\n",
       "  (201, 1),\n",
       "  (202, 1),\n",
       "  (203, 1),\n",
       "  (204, 1),\n",
       "  (205, 1),\n",
       "  (206, 1),\n",
       "  (207, 1),\n",
       "  (208, 1),\n",
       "  (209, 1),\n",
       "  (210, 1),\n",
       "  (211, 1),\n",
       "  (212, 5),\n",
       "  (213, 2),\n",
       "  (214, 1),\n",
       "  (215, 1),\n",
       "  (216, 1),\n",
       "  (217, 1),\n",
       "  (218, 1),\n",
       "  (219, 1),\n",
       "  (220, 2),\n",
       "  (221, 1),\n",
       "  (222, 1),\n",
       "  (223, 1),\n",
       "  (224, 2),\n",
       "  (225, 1),\n",
       "  (226, 1),\n",
       "  (227, 1),\n",
       "  (228, 1),\n",
       "  (229, 1),\n",
       "  (230, 1),\n",
       "  (231, 1),\n",
       "  (232, 2),\n",
       "  (233, 1),\n",
       "  (234, 1),\n",
       "  (235, 1),\n",
       "  (236, 1),\n",
       "  (237, 1),\n",
       "  (238, 2),\n",
       "  (239, 2),\n",
       "  (240, 1),\n",
       "  (241, 1),\n",
       "  (242, 1),\n",
       "  (243, 1),\n",
       "  (244, 1),\n",
       "  (245, 1),\n",
       "  (246, 1),\n",
       "  (247, 1),\n",
       "  (248, 1),\n",
       "  (249, 1),\n",
       "  (250, 1),\n",
       "  (251, 1),\n",
       "  (252, 1),\n",
       "  (253, 1),\n",
       "  (254, 1),\n",
       "  (255, 1),\n",
       "  (256, 1),\n",
       "  (257, 1),\n",
       "  (258, 2),\n",
       "  (259, 1),\n",
       "  (260, 1),\n",
       "  (261, 1),\n",
       "  (262, 1),\n",
       "  (263, 1),\n",
       "  (264, 1),\n",
       "  (265, 1),\n",
       "  (266, 1)],\n",
       " [(2, 3),\n",
       "  (4, 1),\n",
       "  (8, 1),\n",
       "  (14, 7),\n",
       "  (18, 2),\n",
       "  (22, 2),\n",
       "  (23, 8),\n",
       "  (38, 10),\n",
       "  (39, 1),\n",
       "  (50, 4),\n",
       "  (54, 1),\n",
       "  (66, 2),\n",
       "  (67, 1),\n",
       "  (75, 1),\n",
       "  (81, 1),\n",
       "  (85, 2),\n",
       "  (96, 3),\n",
       "  (114, 1),\n",
       "  (139, 1),\n",
       "  (158, 1),\n",
       "  (172, 1),\n",
       "  (180, 1),\n",
       "  (185, 1),\n",
       "  (187, 1),\n",
       "  (212, 2),\n",
       "  (239, 1),\n",
       "  (248, 3),\n",
       "  (263, 1),\n",
       "  (267, 1),\n",
       "  (268, 1),\n",
       "  (269, 1),\n",
       "  (270, 1),\n",
       "  (271, 1),\n",
       "  (272, 1),\n",
       "  (273, 1),\n",
       "  (274, 1),\n",
       "  (275, 1),\n",
       "  (276, 1),\n",
       "  (277, 1),\n",
       "  (278, 1),\n",
       "  (279, 1),\n",
       "  (280, 1),\n",
       "  (281, 1),\n",
       "  (282, 1),\n",
       "  (283, 1),\n",
       "  (284, 1),\n",
       "  (285, 1),\n",
       "  (286, 1),\n",
       "  (287, 1),\n",
       "  (288, 1),\n",
       "  (289, 1),\n",
       "  (290, 1),\n",
       "  (291, 1),\n",
       "  (292, 1),\n",
       "  (293, 1),\n",
       "  (294, 1),\n",
       "  (295, 1)],\n",
       " [(2, 2),\n",
       "  (4, 2),\n",
       "  (8, 2),\n",
       "  (18, 1),\n",
       "  (21, 2),\n",
       "  (22, 1),\n",
       "  (23, 3),\n",
       "  (24, 2),\n",
       "  (38, 2),\n",
       "  (58, 1),\n",
       "  (139, 1),\n",
       "  (144, 1),\n",
       "  (212, 1),\n",
       "  (275, 1),\n",
       "  (284, 1),\n",
       "  (296, 1),\n",
       "  (297, 1),\n",
       "  (298, 1),\n",
       "  (299, 1),\n",
       "  (300, 1),\n",
       "  (301, 1),\n",
       "  (302, 1),\n",
       "  (303, 1),\n",
       "  (304, 1),\n",
       "  (305, 1),\n",
       "  (306, 1),\n",
       "  (307, 1),\n",
       "  (308, 1),\n",
       "  (309, 1),\n",
       "  (310, 1),\n",
       "  (311, 1)],\n",
       " [(0, 1),\n",
       "  (2, 3),\n",
       "  (4, 4),\n",
       "  (5, 1),\n",
       "  (8, 2),\n",
       "  (14, 2),\n",
       "  (18, 3),\n",
       "  (21, 2),\n",
       "  (22, 2),\n",
       "  (23, 7),\n",
       "  (24, 1),\n",
       "  (38, 5),\n",
       "  (39, 2),\n",
       "  (50, 5),\n",
       "  (58, 1),\n",
       "  (61, 2),\n",
       "  (66, 1),\n",
       "  (72, 1),\n",
       "  (75, 1),\n",
       "  (84, 1),\n",
       "  (85, 1),\n",
       "  (86, 2),\n",
       "  (110, 1),\n",
       "  (144, 1),\n",
       "  (166, 1),\n",
       "  (172, 1),\n",
       "  (177, 1),\n",
       "  (180, 1),\n",
       "  (184, 3),\n",
       "  (185, 1),\n",
       "  (195, 1),\n",
       "  (212, 1),\n",
       "  (234, 1),\n",
       "  (239, 1),\n",
       "  (268, 1),\n",
       "  (271, 1),\n",
       "  (283, 1),\n",
       "  (285, 1),\n",
       "  (311, 1),\n",
       "  (312, 1),\n",
       "  (313, 1),\n",
       "  (314, 1),\n",
       "  (315, 1),\n",
       "  (316, 1),\n",
       "  (317, 1),\n",
       "  (318, 1),\n",
       "  (319, 1),\n",
       "  (320, 1),\n",
       "  (321, 1),\n",
       "  (322, 1),\n",
       "  (323, 1),\n",
       "  (324, 1),\n",
       "  (325, 1),\n",
       "  (326, 1),\n",
       "  (327, 1),\n",
       "  (328, 1),\n",
       "  (329, 1),\n",
       "  (330, 1),\n",
       "  (331, 1),\n",
       "  (332, 1),\n",
       "  (333, 1),\n",
       "  (334, 1),\n",
       "  (335, 1),\n",
       "  (336, 1),\n",
       "  (337, 1),\n",
       "  (338, 1),\n",
       "  (339, 1),\n",
       "  (340, 1),\n",
       "  (341, 1),\n",
       "  (342, 1),\n",
       "  (343, 1),\n",
       "  (344, 1),\n",
       "  (345, 1),\n",
       "  (346, 1),\n",
       "  (347, 1)]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:09:40.538812Z",
     "start_time": "2018-11-26T07:09:11.144629Z"
    }
   },
   "outputs": [],
   "source": [
    "# documents contain movie reviews\n",
    "# note: abbreviated to 50000 documents\n",
    "my_documents = traindf.review.head(50000).values.tolist()\n",
    "\n",
    "# simple brief example tokenizing and lowercase\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "# can also remove punctuation and stop words\n",
    "\n",
    "# start corpus: use Dictionary class to map id to each token\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:12:07.204050Z",
     "start_time": "2018-11-26T07:12:04.534934Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a gensim corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# view corpus - a list of lists, each list item is 1 document\n",
    "# form: (id, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:12:15.800568Z",
     "start_time": "2018-11-26T07:12:15.782024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " \"''\": 1,\n",
       " ',': 2,\n",
       " '``': 3,\n",
       " 'a': 4,\n",
       " 'after': 5,\n",
       " 'dad': 6,\n",
       " 'directing': 7,\n",
       " 'for': 8,\n",
       " 'gag': 9,\n",
       " 'gave': 10,\n",
       " 'gift': 11,\n",
       " 'got': 12,\n",
       " 'he': 13,\n",
       " 'it': 14,\n",
       " 'kick': 15,\n",
       " 'my': 16,\n",
       " 'nunsense': 17,\n",
       " 'of': 18,\n",
       " 'out': 19,\n",
       " 'reall': 20,\n",
       " 'this': 21,\n",
       " 'to': 22,\n",
       " '.': 23,\n",
       " 'and': 24,\n",
       " 'are': 25,\n",
       " 'blues': 26,\n",
       " 'catchy': 27,\n",
       " 'cd': 28,\n",
       " 'cross-cultural': 29,\n",
       " 'fresh': 30,\n",
       " 'from': 31,\n",
       " 'great': 32,\n",
       " 'gut': 33,\n",
       " 'happiness': 34,\n",
       " 'hear': 35,\n",
       " 'her': 36,\n",
       " 'hope': 37,\n",
       " 'i': 38,\n",
       " 'is': 39,\n",
       " 'like': 40,\n",
       " 'lot': 41,\n",
       " 'mature': 42,\n",
       " 'more': 43,\n",
       " 'need': 44,\n",
       " 'people': 45,\n",
       " 'pop': 46,\n",
       " 'positive': 47,\n",
       " 'sounds': 48,\n",
       " 'strong': 49,\n",
       " 'the': 50,\n",
       " 'tunes': 51,\n",
       " 'vibes': 52,\n",
       " 'vocals': 53,\n",
       " 'we': 54,\n",
       " \"'\": 55,\n",
       " \"'d\": 56,\n",
       " \"'game\": 57,\n",
       " \"'m\": 58,\n",
       " \"'s\": 59,\n",
       " 'any': 60,\n",
       " 'be': 61,\n",
       " 'beauty': 62,\n",
       " 'been': 63,\n",
       " 'best': 64,\n",
       " 'bit': 65,\n",
       " 'but': 66,\n",
       " 'buy': 67,\n",
       " 'disagree': 68,\n",
       " 'every': 69,\n",
       " 'fade.the': 70,\n",
       " 'feel': 71,\n",
       " 'figured': 72,\n",
       " 'going': 73,\n",
       " 'if': 74,\n",
       " 'in': 75,\n",
       " 'its': 76,\n",
       " 'listening': 77,\n",
       " 'masterpiece': 78,\n",
       " 'mitsuda': 79,\n",
       " 'money': 80,\n",
       " 'much': 81,\n",
       " 'music': 82,\n",
       " 'must': 83,\n",
       " 'now': 84,\n",
       " 'on': 85,\n",
       " 'one': 86,\n",
       " 'only': 87,\n",
       " 'opinino': 88,\n",
       " 'penny': 89,\n",
       " 'pretty': 90,\n",
       " 'price': 91,\n",
       " 'reading': 92,\n",
       " 'refuses': 93,\n",
       " 'review': 94,\n",
       " 'reviews': 95,\n",
       " 'say': 96,\n",
       " 'saying': 97,\n",
       " 'simply': 98,\n",
       " 'soundtrack': 99,\n",
       " 'staggering': 100,\n",
       " 'tag': 101,\n",
       " 'that': 102,\n",
       " 'timeless': 103,\n",
       " 'ultimate': 104,\n",
       " 'worth': 105,\n",
       " 'would': 106,\n",
       " 'write': 107,\n",
       " 'yasunori': 108,\n",
       " 'years': 109,\n",
       " 'you': 110,\n",
       " ';': 111,\n",
       " 'ambiance': 112,\n",
       " 'another': 113,\n",
       " 'at': 114,\n",
       " 'below': 115,\n",
       " 'by': 116,\n",
       " 'close': 117,\n",
       " 'creation': 118,\n",
       " 'cross': 119,\n",
       " 'factors': 120,\n",
       " 'favourite': 121,\n",
       " 'filled': 122,\n",
       " 'girl': 123,\n",
       " 'include': 124,\n",
       " 'left': 125,\n",
       " 'major': 126,\n",
       " 'misuda': 127,\n",
       " 'nobuo': 128,\n",
       " 'orchestra': 129,\n",
       " 'ost': 130,\n",
       " 'question': 131,\n",
       " 'rich': 132,\n",
       " 'scars': 133,\n",
       " 'second': 134,\n",
       " 'some': 135,\n",
       " 'stars': 136,\n",
       " 'stole': 137,\n",
       " 'synthesized': 138,\n",
       " 'time': 139,\n",
       " 'times': 140,\n",
       " 'tracks': 141,\n",
       " 'uematsu.chrono': 142,\n",
       " 'uplifting': 143,\n",
       " 'very': 144,\n",
       " 'vigorous': 145,\n",
       " 'while': 146,\n",
       " 'who': 147,\n",
       " 'with': 148,\n",
       " 'without': 149,\n",
       " 'wonderful': 150,\n",
       " 'world': 151,\n",
       " 'yet': 152,\n",
       " '(': 153,\n",
       " ')': 154,\n",
       " '[': 155,\n",
       " ']': 156,\n",
       " 'amazing': 157,\n",
       " 'anyway': 158,\n",
       " 'around': 159,\n",
       " 'better': 160,\n",
       " 'dancing': 161,\n",
       " 'delight': 162,\n",
       " 'enjoyable': 163,\n",
       " 'especially': 164,\n",
       " 'few': 165,\n",
       " 'first': 166,\n",
       " 'game': 167,\n",
       " 'get': 168,\n",
       " 'getting': 169,\n",
       " 'greatest': 170,\n",
       " 'hard': 171,\n",
       " 'have': 172,\n",
       " 'history': 173,\n",
       " 'played': 174,\n",
       " 'probably': 175,\n",
       " 'really': 176,\n",
       " 'so': 177,\n",
       " 'spending': 178,\n",
       " 'usually': 179,\n",
       " 'was': 180,\n",
       " 'will': 181,\n",
       " 'worked': 182,\n",
       " 'about': 183,\n",
       " 'all': 184,\n",
       " 'an': 185,\n",
       " 'can': 186,\n",
       " 'could': 187,\n",
       " 'day': 188,\n",
       " 'even': 189,\n",
       " 'ever': 190,\n",
       " 'expensive': 191,\n",
       " 'import.some': 192,\n",
       " 'listen': 193,\n",
       " 'minute': 194,\n",
       " 'not': 195,\n",
       " 'reason': 196,\n",
       " 'there': 197,\n",
       " 'track': 198,\n",
       " 'version': 199,\n",
       " '--': 200,\n",
       " '-stay': 201,\n",
       " '5': 202,\n",
       " '8th': 203,\n",
       " 'am': 204,\n",
       " 'amazon': 205,\n",
       " 'anyone': 206,\n",
       " 'as': 207,\n",
       " 'away': 208,\n",
       " 'bad': 209,\n",
       " 'believe': 210,\n",
       " 'bits': 211,\n",
       " 'book': 212,\n",
       " 'ca': 213,\n",
       " 'contest': 214,\n",
       " 'definitely': 215,\n",
       " 'enough': 216,\n",
       " 'entered': 217,\n",
       " 'evening': 218,\n",
       " 'family': 219,\n",
       " 'far': 220,\n",
       " 'friend': 221,\n",
       " 'friends': 222,\n",
       " 'grade': 223,\n",
       " 'haddon': 224,\n",
       " 'heard': 225,\n",
       " 'herself': 226,\n",
       " 'hysterics': 227,\n",
       " 'imagine': 228,\n",
       " 'into': 229,\n",
       " 'joke': 230,\n",
       " 'kill': 231,\n",
       " 'kind': 232,\n",
       " 'know': 233,\n",
       " 'maybe': 234,\n",
       " 'mockingbird': 235,\n",
       " 'mood': 236,\n",
       " 'most': 237,\n",
       " 'ms.': 238,\n",
       " \"n't\": 239,\n",
       " 'never': 240,\n",
       " 'offer': 241,\n",
       " 'or': 242,\n",
       " 'paper': 243,\n",
       " 'paragraphs': 244,\n",
       " 'perhaps': 245,\n",
       " 'pieces': 246,\n",
       " 'quite': 247,\n",
       " 'read': 248,\n",
       " 'self-published': 249,\n",
       " 'sells': 250,\n",
       " 'send': 251,\n",
       " 'someone': 252,\n",
       " 'spent': 253,\n",
       " 'star': 254,\n",
       " 'sure': 255,\n",
       " 'term': 256,\n",
       " 'them': 257,\n",
       " 'thing': 258,\n",
       " 'those': 259,\n",
       " 'unless': 260,\n",
       " 'want': 261,\n",
       " 'were': 262,\n",
       " 'whole': 263,\n",
       " 'why': 264,\n",
       " 'worst': 265,\n",
       " 'written': 266,\n",
       " 'also': 267,\n",
       " 'back': 268,\n",
       " 'beloved.sincerly': 269,\n",
       " 'church': 270,\n",
       " 'cover': 271,\n",
       " 'dissapointed': 272,\n",
       " 'down': 273,\n",
       " 'enjoyed': 274,\n",
       " 'errors': 275,\n",
       " 'faults': 276,\n",
       " 'gives': 277,\n",
       " 'hours': 278,\n",
       " 'interesting': 279,\n",
       " 'jaylynn': 280,\n",
       " 'looking': 281,\n",
       " 'love': 282,\n",
       " 'others': 283,\n",
       " 'paid': 284,\n",
       " 'point': 285,\n",
       " 'put': 286,\n",
       " 'r': 287,\n",
       " 'sad': 288,\n",
       " 'see': 289,\n",
       " 'since': 290,\n",
       " 'spend': 291,\n",
       " 'today': 292,\n",
       " 'too': 293,\n",
       " 'two': 294,\n",
       " 'view': 295,\n",
       " 'absolutely': 296,\n",
       " 'actually': 297,\n",
       " 'add': 298,\n",
       " 'author': 299,\n",
       " 'complete': 300,\n",
       " 'disappointed': 301,\n",
       " 'embarrassed': 302,\n",
       " 'grammar': 303,\n",
       " 'nothing': 304,\n",
       " 'pathetic': 305,\n",
       " 'plot': 306,\n",
       " 'poor': 307,\n",
       " 'totally': 308,\n",
       " 'typographical': 309,\n",
       " 'up': 310,\n",
       " 'waste': 311,\n",
       " '$': 312,\n",
       " '-': 313,\n",
       " '10.95': 314,\n",
       " 'absolute': 315,\n",
       " 'beginning': 316,\n",
       " 'beware': 317,\n",
       " 'churning': 318,\n",
       " 'clear': 319,\n",
       " 'did': 320,\n",
       " 'discerning': 321,\n",
       " 'doubt': 322,\n",
       " 'drivel': 323,\n",
       " 'featured': 324,\n",
       " 'glad': 325,\n",
       " 'guess': 326,\n",
       " 'has': 327,\n",
       " 'intentional': 328,\n",
       " 'lover': 329,\n",
       " 'makes': 330,\n",
       " 'missing': 331,\n",
       " 'novel': 332,\n",
       " 'over-heated': 333,\n",
       " 'page': 334,\n",
       " 'phew': 335,\n",
       " 'prominently': 336,\n",
       " 'prose': 337,\n",
       " 'purposes': 338,\n",
       " 'quick': 339,\n",
       " 're-read': 340,\n",
       " 'removed': 341,\n",
       " 'romance': 342,\n",
       " 'satiric': 343,\n",
       " 'trouble': 344,\n",
       " 'typo': 345,\n",
       " 'wait': 346,\n",
       " 'when': 347,\n",
       " '7th': 348,\n",
       " 'age': 349,\n",
       " 'chapter': 350,\n",
       " 'decided': 351,\n",
       " 'distracted': 352,\n",
       " 'do': 353,\n",
       " 'example': 354,\n",
       " 'faith': 355,\n",
       " 'good': 356,\n",
       " 'grader': 357,\n",
       " 'grammatical': 358,\n",
       " 'had': 359,\n",
       " 'hand': 360,\n",
       " 'horrible': 361,\n",
       " 'house': 362,\n",
       " 'keep': 363,\n",
       " 'lean': 364,\n",
       " 'least': 365,\n",
       " 'mark': 366,\n",
       " 'mentioned': 367,\n",
       " 'misspelling': 368,\n",
       " 'pencil': 369,\n",
       " 'per': 370,\n",
       " 'please': 371,\n",
       " 'points': 372,\n",
       " 'relatives': 373,\n",
       " 'reviewer': 374,\n",
       " 'seems': 375,\n",
       " 'she': 376,\n",
       " 'skills': 377,\n",
       " 'spelling': 378,\n",
       " 'their': 379,\n",
       " 'twice': 380,\n",
       " 'wasting': 381,\n",
       " 'weak': 382,\n",
       " 'writing': 383,\n",
       " 'your': 384,\n",
       " \"'em\": 385,\n",
       " \"'finds\": 386,\n",
       " '4': 387,\n",
       " 'always': 388,\n",
       " 'anymore': 389,\n",
       " 'big': 390,\n",
       " 'case': 391,\n",
       " 'chefs': 392,\n",
       " 'coastal': 393,\n",
       " 'comical': 394,\n",
       " 'cool': 395,\n",
       " 'cuban': 396,\n",
       " 'effects': 397,\n",
       " 'emotional': 398,\n",
       " 'fireballing': 399,\n",
       " 'flashbacks': 400,\n",
       " 'folks': 401,\n",
       " 'gets': 402,\n",
       " 'gig': 403,\n",
       " 'handed': 404,\n",
       " 'honest': 405,\n",
       " 'hysterical': 406,\n",
       " 'idenity': 407,\n",
       " 'interaction': 408,\n",
       " 'italian': 409,\n",
       " 'kitchen': 410,\n",
       " 'latino': 411,\n",
       " 'maintenance': 412,\n",
       " 'make': 413,\n",
       " 'man': 414,\n",
       " 'me': 415,\n",
       " 'might': 416,\n",
       " 'mix': 417,\n",
       " 'motorcycle': 418,\n",
       " 'often': 419,\n",
       " 'owner': 420,\n",
       " 'perfect': 421,\n",
       " 'pitcher': 422,\n",
       " 'players': 423,\n",
       " 'plays': 424,\n",
       " 'resort': 425,\n",
       " 'right': 426,\n",
       " 'roster': 427,\n",
       " 'salsa': 428,\n",
       " 'sea': 429,\n",
       " 'searching': 430,\n",
       " 'sizzling': 431,\n",
       " 'sound': 432,\n",
       " 'special': 433,\n",
       " 'sponsored': 434,\n",
       " 'story': 435,\n",
       " 'stumbles': 436,\n",
       " 'talking': 437,\n",
       " 'team': 438,\n",
       " 'they': 439,\n",
       " 'three': 440,\n",
       " 'through': 441,\n",
       " 'young': 442,\n",
       " 'zen': 443,\n",
       " '...': 444,\n",
       " 'care': 445,\n",
       " 'excellent': 446,\n",
       " 'feet': 447,\n",
       " 'garment': 448,\n",
       " 'integrity': 449,\n",
       " 'long': 450,\n",
       " 'longer': 451,\n",
       " 'loose': 452,\n",
       " 'package': 453,\n",
       " 'proper': 454,\n",
       " 'shifts': 455,\n",
       " 'states': 456,\n",
       " 'stockings': 457,\n",
       " 'than': 458,\n",
       " 'tight': 459,\n",
       " '3': 460,\n",
       " 'actual': 461,\n",
       " 'almost': 462,\n",
       " 'arrived': 463,\n",
       " 'bought': 464,\n",
       " 'box': 465,\n",
       " 'brand': 466,\n",
       " 'bronze': 467,\n",
       " 'certain': 468,\n",
       " 'compression': 469,\n",
       " 'fits': 470,\n",
       " 'jobst': 471,\n",
       " 'less': 472,\n",
       " 'nor': 473,\n",
       " 'okay': 474,\n",
       " 'ordered': 475,\n",
       " 'pair': 476,\n",
       " 'pairs': 477,\n",
       " 'quality': 478,\n",
       " 'recieve': 479,\n",
       " 'store': 480,\n",
       " 'sun': 481,\n",
       " 'tags': 482,\n",
       " 'took': 483,\n",
       " 'weeks': 484,\n",
       " 'chart': 485,\n",
       " 'check': 486,\n",
       " 'go': 487,\n",
       " 'internet..it': 488,\n",
       " 'item': 489,\n",
       " 'recomended': 490,\n",
       " 'sheer': 491,\n",
       " 'should': 492,\n",
       " 'sizes': 493,\n",
       " 'smaller': 494,\n",
       " 'tried': 495,\n",
       " 'what': 496,\n",
       " '1': 497,\n",
       " 'court': 498,\n",
       " 'facts': 499,\n",
       " 'full': 500,\n",
       " 'general': 501,\n",
       " 'intrigue': 502,\n",
       " 'james': 503,\n",
       " 'key': 504,\n",
       " 'overview': 505,\n",
       " 'provides': 506,\n",
       " \"'old\": 507,\n",
       " \"'ve\": 508,\n",
       " '16mm': 509,\n",
       " 'acclaimed': 510,\n",
       " 'appear': 511,\n",
       " 'award': 512,\n",
       " 'black': 513,\n",
       " 'bootleg': 514,\n",
       " 'bridge': 515,\n",
       " 'bright': 516,\n",
       " 'captain': 517,\n",
       " 'clouded': 518,\n",
       " 'combined': 519,\n",
       " 'comes': 520,\n",
       " 'commands': 521,\n",
       " 'condition': 522,\n",
       " 'contrasts': 523,\n",
       " 'copy': 524,\n",
       " 'corner': 525,\n",
       " 'countryside': 526,\n",
       " 'critically': 527,\n",
       " 'crystal': 528,\n",
       " 'cue': 529,\n",
       " 'dark': 530,\n",
       " 'dealer.if': 531,\n",
       " 'defining': 532,\n",
       " 'droppings': 533,\n",
       " 'dull': 534,\n",
       " 'dvd': 535,\n",
       " 'early': 536,\n",
       " 'enunciation': 537,\n",
       " 'events': 538,\n",
       " 'everything': 539,\n",
       " 'film': 540,\n",
       " 'ground': 541,\n",
       " 'haze': 542,\n",
       " 'his': 543,\n",
       " 'home': 544,\n",
       " 'image': 545,\n",
       " 'immediate.here': 546,\n",
       " 'insect': 547,\n",
       " 'library': 548,\n",
       " 'light': 549,\n",
       " 'lighting': 550,\n",
       " 'memory': 551,\n",
       " 'mists': 552,\n",
       " 'morning': 553,\n",
       " 'muddy': 554,\n",
       " 'none': 555,\n",
       " 'packaging': 556,\n",
       " 'pixelations': 557,\n",
       " 'presentation': 558,\n",
       " 'public': 559,\n",
       " 'random': 560,\n",
       " 'rather': 561,\n",
       " 'reasonably': 562,\n",
       " 'reel.just': 563,\n",
       " 'remember': 564,\n",
       " 'resolution': 565,\n",
       " 'scenes': 566,\n",
       " 'scratches': 567,\n",
       " 'seen': 568,\n",
       " 'set': 569,\n",
       " 'somewhere': 570,\n",
       " 'standard': 571,\n",
       " 'straight': 572,\n",
       " 'street': 573,\n",
       " 'surrounding': 574,\n",
       " 'timbre': 575,\n",
       " 'vague': 576,\n",
       " 'visuals': 577,\n",
       " 'visuals.after': 578,\n",
       " 'water': 579,\n",
       " 'white': 580,\n",
       " 'winning': 581,\n",
       " 'youtube': 582,\n",
       " 'ages': 583,\n",
       " 'ando': 584,\n",
       " 'bombarded': 585,\n",
       " 'clothing': 586,\n",
       " 'country': 587,\n",
       " 'creating': 588,\n",
       " 'culture': 589,\n",
       " 'daughters': 590,\n",
       " 'developing': 591,\n",
       " 'each': 592,\n",
       " 'elders': 593,\n",
       " 'etc': 594,\n",
       " 'excesses': 595,\n",
       " 'fads': 596,\n",
       " 'firm': 597,\n",
       " 'foundation': 598,\n",
       " 'generation': 599,\n",
       " 'groups': 600,\n",
       " 'healthy': 601,\n",
       " 'hopes': 602,\n",
       " 'however': 603,\n",
       " 'hybrid': 604,\n",
       " 'identity': 605,\n",
       " 'itto': 606,\n",
       " 'japan': 607,\n",
       " 'learn': 608,\n",
       " 'lives': 609,\n",
       " 'members': 610,\n",
       " 'memorable': 611,\n",
       " 'new': 612,\n",
       " 'old': 613,\n",
       " 'our': 614,\n",
       " 'own': 615,\n",
       " 'poignantly': 616,\n",
       " 'preserving': 617,\n",
       " 'pure': 618,\n",
       " 'recommended': 619,\n",
       " 'respect': 620,\n",
       " 'retains': 621,\n",
       " 'rising': 622,\n",
       " 'rock': 623,\n",
       " 'seem': 624,\n",
       " 'shopping': 625,\n",
       " 'sons': 626,\n",
       " 'steven': 627,\n",
       " 'stevern': 628,\n",
       " 'thatjapanese': 629,\n",
       " 'tradition': 630,\n",
       " 'values': 631,\n",
       " 'wardell': 632,\n",
       " 'western': 633,\n",
       " 'writes': 634,\n",
       " 'x': 635,\n",
       " \"'mama\": 636,\n",
       " 'angel': 637,\n",
       " 'blue': 638,\n",
       " 'days': 639,\n",
       " 'find': 640,\n",
       " 'hair': 641,\n",
       " 'just': 642,\n",
       " 'lanna': 643,\n",
       " 'listened': 644,\n",
       " 'neck.roy': 645,\n",
       " 'o': 646,\n",
       " 'off': 647,\n",
       " 'rose': 648,\n",
       " 'same': 649,\n",
       " 'singer': 650,\n",
       " 'song': 651,\n",
       " 'songs': 652,\n",
       " 'talent': 653,\n",
       " 'thought': 654,\n",
       " 'trully': 655,\n",
       " 'album': 656,\n",
       " 'albums': 657,\n",
       " 'based': 658,\n",
       " 'couple': 659,\n",
       " 'fact': 660,\n",
       " 'fans': 661,\n",
       " 'harsh': 662,\n",
       " 'hearing': 663,\n",
       " 'it.the': 664,\n",
       " 'liner': 665,\n",
       " 'music.well': 666,\n",
       " 'notes': 667,\n",
       " 'orbison': 668,\n",
       " 'other': 669,\n",
       " 'roy': 670,\n",
       " 'take': 671,\n",
       " 'value': 672,\n",
       " 'wrote': 673,\n",
       " \"'bootleg\": 674,\n",
       " '4cd': 675,\n",
       " 'because': 676,\n",
       " 'better.having': 677,\n",
       " 'concert': 678,\n",
       " 'disappointing': 679,\n",
       " 'does': 680,\n",
       " 'live': 681,\n",
       " 'mean': 682,\n",
       " 'overall': 683,\n",
       " 'recording': 684,\n",
       " 'saw': 685,\n",
       " 'slightly': 686,\n",
       " 'sounded': 687,\n",
       " 'title': 688,\n",
       " 'aa': 689,\n",
       " 'aaa': 690,\n",
       " 'apply': 691,\n",
       " 'batteries': 692,\n",
       " 'became': 693,\n",
       " 'button': 694,\n",
       " 'buttons': 695,\n",
       " 'charge': 696,\n",
       " 'charger': 697,\n",
       " 'charges': 698,\n",
       " 'crayon': 699,\n",
       " 'duct': 700,\n",
       " 'end': 701,\n",
       " 'fine': 702,\n",
       " 'flip': 703,\n",
       " 'four': 704,\n",
       " 'hold': 705,\n",
       " 'horizontal': 706,\n",
       " 'how': 707,\n",
       " 'huge': 708,\n",
       " 'little': 709,\n",
       " 'mechanism': 710,\n",
       " 'painful': 711,\n",
       " 'pressure': 712,\n",
       " 'problem': 713,\n",
       " 'push': 714,\n",
       " 'securing': 715,\n",
       " 'segment': 716,\n",
       " 'tape': 717,\n",
       " 'using': 718,\n",
       " 'wo': 719,\n",
       " 'wrap': 720,\n",
       " '140': 721,\n",
       " '2100': 722,\n",
       " '64': 723,\n",
       " 'bye': 724,\n",
       " 'carry': 725,\n",
       " 'greetings': 726,\n",
       " 'k6000': 727,\n",
       " 'kodak': 728,\n",
       " 'mah': 729,\n",
       " 'minutes': 730,\n",
       " 'nimh': 731,\n",
       " 'which': 732,\n",
       " '1850': 733,\n",
       " 'battery': 734,\n",
       " 'cheap': 735,\n",
       " 'come': 736,\n",
       " 'drain': 737,\n",
       " 'gps': 738,\n",
       " 'hour.btw': 739,\n",
       " 'pay': 740,\n",
       " 'themselves': 741,\n",
       " 'these': 742,\n",
       " 'camera': 743,\n",
       " 'digital': 744,\n",
       " 'handy': 745,\n",
       " 'life': 746,\n",
       " 'product': 747,\n",
       " '2003': 748,\n",
       " 'alkaline': 749,\n",
       " 'convenient': 750,\n",
       " 'design': 751,\n",
       " 'disposables': 752,\n",
       " 'elsewhere': 753,\n",
       " 'jul': 754,\n",
       " 'look': 755,\n",
       " 'nice': 756,\n",
       " 'ok': 757,\n",
       " 'power': 758,\n",
       " 'staying': 759,\n",
       " 'well': 760,\n",
       " 'year': 761,\n",
       " '?': 762,\n",
       " 'before': 763,\n",
       " 'capacity': 764,\n",
       " 'charge/discharge': 765,\n",
       " 'charged': 766,\n",
       " 'cycled': 767,\n",
       " 'cycles': 768,\n",
       " 'doing': 769,\n",
       " 'finally': 770,\n",
       " 'higher': 771,\n",
       " 'lasts': 772,\n",
       " 'level': 773,\n",
       " 'may': 774,\n",
       " 'ni-mh': 775,\n",
       " 'pleasantly': 776,\n",
       " 'rated': 777,\n",
       " 'reach': 778,\n",
       " 'recharged': 779,\n",
       " 'row': 780,\n",
       " 'said': 781,\n",
       " 'short': 782,\n",
       " 'surprised': 783,\n",
       " 'try': 784,\n",
       " 'until': 785,\n",
       " '2': 786,\n",
       " '4.0.': 787,\n",
       " 'armando': 788,\n",
       " 'ass': 789,\n",
       " 'awesome': 790,\n",
       " 'boost': 791,\n",
       " 'call': 792,\n",
       " 'cent': 793,\n",
       " 'cologne': 794,\n",
       " 'coming': 795,\n",
       " 'dramatica': 796,\n",
       " 'efforts': 797,\n",
       " 'euro': 798,\n",
       " 'eventually': 799,\n",
       " 'fail': 800,\n",
       " 'german': 801,\n",
       " 'germany': 802,\n",
       " 'guide': 803,\n",
       " 'happy': 804,\n",
       " 'having': 805,\n",
       " 'highly': 806,\n",
       " 'let': 807,\n",
       " 'lost': 808,\n",
       " 'made': 809,\n",
       " 'making': 810,\n",
       " 'mother': 811,\n",
       " 'native': 812,\n",
       " 'online': 813,\n",
       " 'payed': 814,\n",
       " 'recommend': 815,\n",
       " 'roundabout': 816,\n",
       " 'save': 817,\n",
       " 'saved': 818,\n",
       " 'software': 819,\n",
       " 'speaker': 820,\n",
       " 'ten': 821,\n",
       " 'thank': 822,\n",
       " 'though': 823,\n",
       " 'tongue': 824,\n",
       " 'use': 825,\n",
       " 'wish': 826,\n",
       " 'work': 827,\n",
       " 'yours': 828,\n",
       " \"'re\": 829,\n",
       " 'both': 830,\n",
       " 'brother': 831,\n",
       " 'choice': 832,\n",
       " 'christmas': 833,\n",
       " 'combination': 834,\n",
       " 'easy': 835,\n",
       " 'electronics': 836,\n",
       " 'jvc': 837,\n",
       " 'liked': 838,\n",
       " 'modes': 839,\n",
       " 'name': 840,\n",
       " 'operates': 841,\n",
       " 'playback': 842,\n",
       " 'player': 843,\n",
       " 'purchased': 844,\n",
       " 'remote': 845,\n",
       " 'several': 846,\n",
       " 'trusted': 847,\n",
       " 'tv': 848,\n",
       " 'vcr': 849,\n",
       " 'wife': 850,\n",
       " ':': 851,\n",
       " 'agree': 852,\n",
       " 'awkward': 853,\n",
       " 'comments': 854,\n",
       " 'complaints': 855,\n",
       " 'complicated': 856,\n",
       " 'deciding': 857,\n",
       " 'dvd-land': 858,\n",
       " 'enter': 859,\n",
       " 'gotten': 860,\n",
       " 'hang': 861,\n",
       " 'heavily': 862,\n",
       " 'how-to': 863,\n",
       " 'intuitive': 864,\n",
       " 'it.two': 865,\n",
       " 'join': 866,\n",
       " 'manual': 867,\n",
       " 'many': 868,\n",
       " 'movies': 869,\n",
       " 'myself': 870,\n",
       " 'options': 871,\n",
       " 'present': 872,\n",
       " 'rely': 873,\n",
       " 'rest': 874,\n",
       " 'scroll': 875,\n",
       " 'selection': 876,\n",
       " 'set-up': 877,\n",
       " 'setting': 878,\n",
       " 'something': 879,\n",
       " 'start': 880,\n",
       " 'technically-minded': 881,\n",
       " 'think': 882,\n",
       " 'timer': 883,\n",
       " 'tv/vhs/dvd': 884,\n",
       " 'vcr/dvd': 885,\n",
       " 'vhs': 886,\n",
       " 'wanting': 887,\n",
       " '6': 888,\n",
       " 'alarm': 889,\n",
       " 'cents': 890,\n",
       " 'clock': 891,\n",
       " 'display': 892,\n",
       " 'displays': 893,\n",
       " 'drawback': 894,\n",
       " 'ep': 895,\n",
       " 'fairly': 896,\n",
       " 'fit': 897,\n",
       " 'hour': 898,\n",
       " 'hr': 899,\n",
       " 'lack': 900,\n",
       " 'lettering': 901,\n",
       " 'lp': 902,\n",
       " 'mode': 903,\n",
       " 'orange': 904,\n",
       " 'record': 905,\n",
       " 'reminds': 906,\n",
       " 'seconds': 907,\n",
       " 'sp': 908,\n",
       " 'speeds': 909,\n",
       " 'stuff': 910,\n",
       " 'taping': 911,\n",
       " 'trivial': 912,\n",
       " 'ugly': 913,\n",
       " 'universal': 914,\n",
       " 'works': 915,\n",
       " 'began': 916,\n",
       " 'disc': 917,\n",
       " 'giving': 918,\n",
       " 'here': 919,\n",
       " 'hte': 920,\n",
       " 'incorrect': 921,\n",
       " 'problems': 922,\n",
       " 'quit': 923,\n",
       " 'shot': 924,\n",
       " 'side': 925,\n",
       " 'sign': 926,\n",
       " 'sometimes': 927,\n",
       " 'sony': 928,\n",
       " 'sticking': 929,\n",
       " 'still': 930,\n",
       " 'understand': 931,\n",
       " 'useless': 932,\n",
       " '12': 933,\n",
       " 'allow': 934,\n",
       " 'although': 935,\n",
       " 'capability': 936,\n",
       " 'component': 937,\n",
       " 'input': 938,\n",
       " 'machine': 939,\n",
       " 'mp3': 940,\n",
       " 'playing': 941,\n",
       " 'primarily': 942,\n",
       " 'replaced': 943,\n",
       " 'select': 944,\n",
       " 'sub-par': 945,\n",
       " 'tivo': 946,\n",
       " 'turns': 947,\n",
       " 'versatile': 948,\n",
       " 'wants': 949,\n",
       " 'year-old': 950,\n",
       " 'atrocious': 951,\n",
       " 'brags': 952,\n",
       " 'color': 953,\n",
       " 'desired': 954,\n",
       " 'either': 955,\n",
       " 'experience': 956,\n",
       " 'factory': 957,\n",
       " 'fall': 958,\n",
       " 'features': 959,\n",
       " 'frequency': 960,\n",
       " 'inventor': 961,\n",
       " 'noisy': 962,\n",
       " 'pre-records': 963,\n",
       " 'recordings': 964,\n",
       " 'response': 965,\n",
       " 'saddled': 966,\n",
       " 'shame': 967,\n",
       " 'smeared': 968,\n",
       " 'tuner': 969,\n",
       " 'unit': 970,\n",
       " 'wise': 971,\n",
       " 'concepts': 972,\n",
       " 'ee': 973,\n",
       " 'engineers': 974,\n",
       " 'found': 975,\n",
       " 'students': 976,\n",
       " 'useful': 977,\n",
       " 'algorithm': 978,\n",
       " 'appears': 979,\n",
       " 'authors': 980,\n",
       " 'bothersome': 981,\n",
       " 'buzo': 982,\n",
       " 'co-creator': 983,\n",
       " 'commonplace': 984,\n",
       " 'comprehensive': 985,\n",
       " 'content': 986,\n",
       " 'demand': 987,\n",
       " 'fortunately': 988,\n",
       " 'gray': 989,\n",
       " 'hardcover': 990,\n",
       " 'images': 991,\n",
       " 'itself': 992,\n",
       " 'laughably': 993,\n",
       " 'lbg': 994,\n",
       " 'linde': 995,\n",
       " 'manifests': 996,\n",
       " 'near': 997,\n",
       " 'print': 998,\n",
       " 'printing': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check: look at dictionary token and id\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:12:32.401767Z",
     "start_time": "2018-11-26T07:12:32.398547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 2), (4, 1), (18, 1), (21, 2), (23, 5), (24, 2), (25, 1), (26, 1), (27, 1), (28, 1)]\n"
     ]
    }
   ],
   "source": [
    "# sanity check: look at first 10 word ids with their frequency counts \n",
    "#  from the 2nd document\n",
    "print(corpus[1][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a tf-idf model with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:12:38.285679Z",
     "start_time": "2018-11-26T07:12:37.506189Z"
    }
   },
   "outputs": [],
   "source": [
    "# use bag of words corpus and translate to TfidfModel\n",
    "tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:12:40.888348Z",
     "start_time": "2018-11-26T07:12:40.876685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.01986542514235891),\n",
       " (4, 0.009670564372163796),\n",
       " (18, 0.014603081325142419),\n",
       " (21, 0.02394427960261586),\n",
       " (23, 0.008009350152255625),\n",
       " (24, 0.017120182403592487),\n",
       " (25, 0.046731095776159325),\n",
       " (26, 0.2274864647257741),\n",
       " (27, 0.22033262254283312),\n",
       " (28, 0.11403784680523987),\n",
       " (29, 0.4018838776964095),\n",
       " (30, 0.2094822997039569),\n",
       " (31, 0.06301992607999961),\n",
       " (32, 0.06821228045601305),\n",
       " (33, 0.31253946803324956),\n",
       " (34, 0.2649682185098172),\n",
       " (35, 0.15830301418490395),\n",
       " (36, 0.10167012931352057),\n",
       " (37, 0.15161403571746326),\n",
       " (38, 0.012424377323207442),\n",
       " (39, 0.01580335768549762),\n",
       " (40, 0.05819096864793154),\n",
       " (41, 0.11496164131924472),\n",
       " (42, 0.24284948776090634),\n",
       " (43, 0.06847880826489389),\n",
       " (44, 0.12664786489785124),\n",
       " (45, 0.10388046230349512),\n",
       " (46, 0.18538486816050764),\n",
       " (47, 0.20506821676966816),\n",
       " (48, 0.16293449892253717),\n",
       " (49, 0.17253149721844768),\n",
       " (50, 0.008353822879747334),\n",
       " (51, 0.21516137711421968),\n",
       " (52, 0.32465879579786927),\n",
       " (53, 0.20263771196076216),\n",
       " (54, 0.09759696550608683)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reference each document like a dictionary\n",
    "# displays [(token_id, token_weights)]\n",
    "tfidf[corpus[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-26T07:15:40.291787Z",
     "start_time": "2018-11-26T07:15:40.250803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.01986542514235891), (4, 0.009670564372163796), (18, 0.014603081325142419), (21, 0.02394427960261586), (23, 0.008009350152255625)]\n",
      "cross-cultural 0.4018838776964095\n",
      "vibes 0.32465879579786927\n",
      "gut 0.31253946803324956\n",
      "happiness 0.2649682185098172\n",
      "mature 0.24284948776090634\n"
     ]
    }
   ],
   "source": [
    "# Save the 2nd document: doc\n",
    "doc = corpus[1]\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, \n",
    "                              key=lambda w: w[1], \n",
    "                              reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing\n",
    "Remove\n",
    "- stop words\n",
    "- non-alpha characters\n",
    "- lemmatize\n",
    "- perform bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim\n",
    "Advantages\n",
    "- uses top academic models to perform complex tasks\n",
    "    - building document or word vectors\n",
    "    - performing topic identification and document comparison\n",
    "- LDA used for topic analysis and modeling\n",
    "- corpus/corpora = set of texts used to perform NLP tasks\n",
    "- gensim models can be easily saved, updated, and reused\n",
    "- dictionary can also be updated\n",
    "    - with new texts\n",
    "    - words that meet certain thresholds\n",
    "    - then use for feature exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# use bag of words corpus and translate to TfidfModel\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# reference each document like a dictionary\n",
    "# displays [(token_id, token_weights)]\n",
    "tfidf[corpus[1]]\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, \n",
    "                              key=lambda w: w[1], \n",
    "                              reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-entity recognition (NER)\n",
    "- Who? What? When? Where?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "Advantages using spaCy for NER\n",
    "- focus on creating NLP pipelines to generate model and corpora\n",
    "- informal language corpora\n",
    "    - easily find entities in Tweets and chat messages\n",
    "\n",
    "- NLP library similar to gensim, with different implementations\n",
    "- additional NER compared to nltk\n",
    "    - NORP, CARDINAL, MONEY, WORKOFART, LANGUAGE, EVENT\n",
    "- displaCy\n",
    "    - entity recognition visualization tool to view parse trees\n",
    "    - which uses Node.js to create interactive text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "# Additional args to improve execution time\n",
    "nlp = spacy.load('en', tagger=False, parser=False, \n",
    "                 matcher=False)\n",
    "\n",
    "# load new document\n",
    "doc = nlp(\"\"\"Berlin is the capital of Germany;\n",
    "and the residence of Chancellor Angela Merkel.\"\"\")\n",
    "\n",
    "# named entities are stored in .ents\n",
    "print(doc.ents)\n",
    "\n",
    "# check out each label (.label_) using indexing\n",
    "print(doc.ents[0], doc.ents[0].label_)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into a dataframe\n",
    "df = ...\n",
    "# set target label\n",
    "y = df['Sci-Fi']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "df['plot'], y, test_size=0.33, random_state=53)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer - bag-of-words\n",
    "\n",
    "# create a count vectorizer and remove stop_words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "# create bag-of-words vectors on train/test sets\n",
    "# fit_transform will create a bag-of-words dictionary \n",
    "#  and vectors for each document\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "# note: if you have unknown words in test set only, may\n",
    "#  need more data or remove those words from the test dataset\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf vectors for documents\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                   max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect vectors in pandas dataframe\n",
    "\n",
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, \n",
    "                        columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, \n",
    "                        columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweaking alpha\n",
    "- try gridsearchCV ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other considerations\n",
    "Sentiment Analysis\n",
    "- complex problems sarcasm\n",
    "- difficulty with negation\n",
    "    - ie. I liked it, but it could have been better\n",
    "- separate communities may use the same words differently\n",
    "Language biases\n",
    "- prejudices in text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
