{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions to consider\n",
    "- how to deal with misspellings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Skip polyglot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP pipeline\n",
    "Objective: create a NLP pipeline that is reusable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other things to check\n",
    "- n-grams - ie. word pairs in 2-gram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Amazon product reviews\n",
    "Data source:\n",
    "- https://drive.google.com/drive/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M\n",
    "- amazon_review_full_csv.tar.gz\n",
    "\n",
    "Data overview:\n",
    "- Amazon reviews full score dataset is constructed by randomly taking 600,000 training samples and 130,000 testing samples for each review score from 1 to 5. \n",
    "- In total there are 3,000,000 training samples and 650,000 testing samples.\n",
    "- The files train.csv and test.csv contain all the training samples as comma-sparated values. \n",
    "- There are 3 columns in them, corresponding to class index (1 to 5), review title and review text. \n",
    "- The review title and text are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). New lines are escaped by a backslash followed with an \"n\" character, that is \"\\n\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:08:58.008805Z",
     "start_time": "2018-11-29T04:08:57.820668Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import (word_tokenize, sent_tokenize, \n",
    "regexp_tokenize)\n",
    "# for tweets\n",
    "# from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from gensim import corpora, models, similarities\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "import spacy\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import (CountVectorizer, \n",
    "TfidfVectorizer)\n",
    "# NB works well with CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if need to display images\n",
    "from IPython.display import Image, SVG\n",
    "SVG(filename='Images/nlp_linelength.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:01:57.704243Z",
     "start_time": "2018-11-29T04:01:57.695776Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = r'/Users/joe/Documents/GitHub/portfolio/support files/\\\n",
    "amazon_review_full_csv'\n",
    "trainfile = 'train.csv'\n",
    "testfile = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:01:58.690047Z",
     "start_time": "2018-11-29T04:01:58.685574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/joe/Documents/GitHub/Python-reference'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:02:14.181570Z",
     "start_time": "2018-11-29T04:01:59.874078Z"
    }
   },
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(os.path.join(filepath, trainfile), sep=\",\",\n",
    "                  names=('rating','title','review'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:02:17.297144Z",
     "start_time": "2018-11-29T04:02:14.184241Z"
    }
   },
   "outputs": [],
   "source": [
    "testdf = pd.read_csv(os.path.join(filepath, testfile), sep=\",\",\n",
    "                  names=('rating','title','review'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:02:22.058043Z",
     "start_time": "2018-11-29T04:02:22.054229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000000, 3) (650000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(traindf.shape, testdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:03:33.552638Z",
     "start_time": "2018-11-29T04:03:33.548844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 3) (10000, 3)\n"
     ]
    }
   ],
   "source": [
    "# limit size of data\n",
    "traindf = traindf.head(100000)\n",
    "testdf = traindf.head(10000)\n",
    "print(traindf.shape, testdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:03:36.569301Z",
     "start_time": "2018-11-29T04:03:36.558371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>more like funchuck</td>\n",
       "      <td>Gave this to my dad for a gag gift after direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Inspiring</td>\n",
       "      <td>I hope a lot of people hear this cd. We need m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Chrono Cross OST</td>\n",
       "      <td>The music of Yasunori Misuda is without questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Too good to be true</td>\n",
       "      <td>Probably the greatest soundtrack in history! U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                  title  \\\n",
       "0       3                     more like funchuck   \n",
       "1       5                              Inspiring   \n",
       "2       5  The best soundtrack ever to anything.   \n",
       "3       4                       Chrono Cross OST   \n",
       "4       5                    Too good to be true   \n",
       "\n",
       "                                              review  \n",
       "0  Gave this to my dad for a gag gift after direc...  \n",
       "1  I hope a lot of people hear this cd. We need m...  \n",
       "2  I'm reading a lot of reviews saying that this ...  \n",
       "3  The music of Yasunori Misuda is without questi...  \n",
       "4  Probably the greatest soundtrack in history! U...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:03:37.262233Z",
     "start_time": "2018-11-29T04:03:37.253890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>more like funchuck</td>\n",
       "      <td>Gave this to my dad for a gag gift after direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Inspiring</td>\n",
       "      <td>I hope a lot of people hear this cd. We need m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Chrono Cross OST</td>\n",
       "      <td>The music of Yasunori Misuda is without questi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Too good to be true</td>\n",
       "      <td>Probably the greatest soundtrack in history! U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                  title  \\\n",
       "0       3                     more like funchuck   \n",
       "1       5                              Inspiring   \n",
       "2       5  The best soundtrack ever to anything.   \n",
       "3       4                       Chrono Cross OST   \n",
       "4       5                    Too good to be true   \n",
       "\n",
       "                                              review  \n",
       "0  Gave this to my dad for a gag gift after direc...  \n",
       "1  I hope a lot of people hear this cd. We need m...  \n",
       "2  I'm reading a lot of reviews saying that this ...  \n",
       "3  The music of Yasunori Misuda is without questi...  \n",
       "4  Probably the greatest soundtrack in history! U...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T03:36:23.994814Z",
     "start_time": "2018-11-29T03:36:23.984864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gave this to my dad for a gag gift after directing \"Nunsense,\" he got a reall kick out of it!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.iloc[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T03:36:24.774583Z",
     "start_time": "2018-11-29T03:36:24.754284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gave',\n",
       " 'this',\n",
       " 'to',\n",
       " 'my',\n",
       " 'dad',\n",
       " 'for',\n",
       " 'a',\n",
       " 'gag',\n",
       " 'gift',\n",
       " 'after',\n",
       " 'directing',\n",
       " '``',\n",
       " 'Nunsense',\n",
       " ',',\n",
       " \"''\",\n",
       " 'he',\n",
       " 'got',\n",
       " 'a',\n",
       " 'reall',\n",
       " 'kick',\n",
       " 'out',\n",
       " 'of',\n",
       " 'it',\n",
       " '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(traindf.iloc[0,2])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T03:36:27.273504Z",
     "start_time": "2018-11-29T03:36:27.127700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a22a0a7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a list of word lengths\n",
    "word_lengths = [len(w) for w in words]\n",
    "\n",
    "plt.hist(word_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T03:36:29.287271Z",
     "start_time": "2018-11-29T03:36:29.282192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I hope a lot of people hear this cd.',\n",
       " 'We need more strong and positive vibes like this.',\n",
       " 'Great vocals, fresh tunes, cross-cultural happiness.',\n",
       " 'Her blues is from the gut.',\n",
       " 'The pop sounds are catchy and mature.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(traindf.iloc[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create gensim dictionary and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T03:36:32.770028Z",
     "start_time": "2018-11-29T03:36:32.766718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gave this to my dad for a gag gift after directing \"Nunsense,\" he got a reall kick out of it!',\n",
       " 'I hope a lot of people hear this cd. We need more strong and positive vibes like this. Great vocals, fresh tunes, cross-cultural happiness. Her blues is from the gut. The pop sounds are catchy and mature.',\n",
       " \"I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\",\n",
       " 'The music of Yasunori Misuda is without question my close second below the great Nobuo Uematsu.Chrono Cross OST is a wonderful creation filled with rich orchestra and synthesized sounds. While ambiance is one of the music\\'s major factors, yet at times it\\'s very uplifting and vigorous. Some of my favourite tracks include; \"Scars Left by Time, The Girl who Stole the Stars, and Another World\".',\n",
       " \"Probably the greatest soundtrack in history! Usually it's better to have played the game first but this is so enjoyable anyway! I worked so hard getting this soundtrack and after spending [money] to get it it was really worth every penny!! Get this OST! it's amazing! The first few tracks will have you dancing around with delight (especially Scars Left by Time)!! BUY IT NOW!!\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.review.head(5).values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T03:36:37.644027Z",
     "start_time": "2018-11-29T03:36:37.632200Z"
    }
   },
   "outputs": [],
   "source": [
    "# test\n",
    "my_documents = traindf.review.head(10).values.tolist()\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "# print(dictionary.token2id)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T03:37:13.367783Z",
     "start_time": "2018-11-29T03:37:13.343101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 2),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1)],\n",
       " [(2, 2),\n",
       "  (4, 1),\n",
       "  (18, 1),\n",
       "  (21, 2),\n",
       "  (23, 5),\n",
       "  (24, 2),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1),\n",
       "  (50, 2),\n",
       "  (51, 1),\n",
       "  (52, 1),\n",
       "  (53, 1),\n",
       "  (54, 1)],\n",
       " [(2, 2),\n",
       "  (4, 3),\n",
       "  (8, 2),\n",
       "  (14, 1),\n",
       "  (16, 1),\n",
       "  (18, 1),\n",
       "  (21, 5),\n",
       "  (22, 4),\n",
       "  (23, 3),\n",
       "  (24, 3),\n",
       "  (25, 1),\n",
       "  (28, 1),\n",
       "  (38, 6),\n",
       "  (39, 5),\n",
       "  (41, 1),\n",
       "  (50, 3),\n",
       "  (55, 1),\n",
       "  (56, 1),\n",
       "  (57, 1),\n",
       "  (58, 2),\n",
       "  (59, 1),\n",
       "  (60, 1),\n",
       "  (61, 1),\n",
       "  (62, 1),\n",
       "  (63, 1),\n",
       "  (64, 1),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (68, 1),\n",
       "  (69, 1),\n",
       "  (70, 1),\n",
       "  (71, 1),\n",
       "  (72, 1),\n",
       "  (73, 1),\n",
       "  (74, 1),\n",
       "  (75, 1),\n",
       "  (76, 1),\n",
       "  (77, 1),\n",
       "  (78, 1),\n",
       "  (79, 1),\n",
       "  (80, 1),\n",
       "  (81, 1),\n",
       "  (82, 1),\n",
       "  (83, 1),\n",
       "  (84, 1),\n",
       "  (85, 1),\n",
       "  (86, 1),\n",
       "  (87, 1),\n",
       "  (88, 1),\n",
       "  (89, 1),\n",
       "  (90, 1),\n",
       "  (91, 1),\n",
       "  (92, 1),\n",
       "  (93, 1),\n",
       "  (94, 1),\n",
       "  (95, 1),\n",
       "  (96, 1),\n",
       "  (97, 1),\n",
       "  (98, 1),\n",
       "  (99, 1),\n",
       "  (100, 1),\n",
       "  (101, 1),\n",
       "  (102, 3),\n",
       "  (103, 1),\n",
       "  (104, 1),\n",
       "  (105, 1),\n",
       "  (106, 1),\n",
       "  (107, 1),\n",
       "  (108, 1),\n",
       "  (109, 1),\n",
       "  (110, 1)],\n",
       " [(1, 1),\n",
       "  (2, 3),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (14, 1),\n",
       "  (16, 2),\n",
       "  (18, 3),\n",
       "  (23, 3),\n",
       "  (24, 3),\n",
       "  (32, 1),\n",
       "  (39, 3),\n",
       "  (48, 1),\n",
       "  (50, 5),\n",
       "  (59, 2),\n",
       "  (82, 2),\n",
       "  (86, 1),\n",
       "  (108, 1),\n",
       "  (111, 1),\n",
       "  (112, 1),\n",
       "  (113, 1),\n",
       "  (114, 1),\n",
       "  (115, 1),\n",
       "  (116, 1),\n",
       "  (117, 1),\n",
       "  (118, 1),\n",
       "  (119, 1),\n",
       "  (120, 1),\n",
       "  (121, 1),\n",
       "  (122, 1),\n",
       "  (123, 1),\n",
       "  (124, 1),\n",
       "  (125, 1),\n",
       "  (126, 1),\n",
       "  (127, 1),\n",
       "  (128, 1),\n",
       "  (129, 1),\n",
       "  (130, 1),\n",
       "  (131, 1),\n",
       "  (132, 1),\n",
       "  (133, 1),\n",
       "  (134, 1),\n",
       "  (135, 1),\n",
       "  (136, 1),\n",
       "  (137, 1),\n",
       "  (138, 1),\n",
       "  (139, 1),\n",
       "  (140, 1),\n",
       "  (141, 1),\n",
       "  (142, 1),\n",
       "  (143, 1),\n",
       "  (144, 1),\n",
       "  (145, 1),\n",
       "  (146, 1),\n",
       "  (147, 1),\n",
       "  (148, 1),\n",
       "  (149, 1),\n",
       "  (150, 1),\n",
       "  (151, 1),\n",
       "  (152, 1)],\n",
       " [(0, 10),\n",
       "  (5, 1),\n",
       "  (14, 5),\n",
       "  (21, 3),\n",
       "  (22, 2),\n",
       "  (24, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (50, 3),\n",
       "  (59, 2),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (69, 1),\n",
       "  (75, 1),\n",
       "  (80, 1),\n",
       "  (84, 1),\n",
       "  (89, 1),\n",
       "  (99, 2),\n",
       "  (105, 1),\n",
       "  (110, 1),\n",
       "  (116, 1),\n",
       "  (125, 1),\n",
       "  (130, 1),\n",
       "  (133, 1),\n",
       "  (139, 1),\n",
       "  (141, 1),\n",
       "  (148, 1),\n",
       "  (153, 1),\n",
       "  (154, 1),\n",
       "  (155, 1),\n",
       "  (156, 1),\n",
       "  (157, 1),\n",
       "  (158, 1),\n",
       "  (159, 1),\n",
       "  (160, 1),\n",
       "  (161, 1),\n",
       "  (162, 1),\n",
       "  (163, 1),\n",
       "  (164, 1),\n",
       "  (165, 1),\n",
       "  (166, 2),\n",
       "  (167, 1),\n",
       "  (168, 2),\n",
       "  (169, 1),\n",
       "  (170, 1),\n",
       "  (171, 1),\n",
       "  (172, 2),\n",
       "  (173, 1),\n",
       "  (174, 1),\n",
       "  (175, 1),\n",
       "  (176, 1),\n",
       "  (177, 2),\n",
       "  (178, 1),\n",
       "  (179, 1),\n",
       "  (180, 1),\n",
       "  (181, 1),\n",
       "  (182, 1)],\n",
       " [(2, 1),\n",
       "  (4, 1),\n",
       "  (18, 2),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 3),\n",
       "  (28, 1),\n",
       "  (38, 2),\n",
       "  (39, 1),\n",
       "  (50, 2),\n",
       "  (59, 3),\n",
       "  (64, 1),\n",
       "  (69, 3),\n",
       "  (82, 1),\n",
       "  (96, 1),\n",
       "  (102, 2),\n",
       "  (177, 1),\n",
       "  (183, 1),\n",
       "  (184, 1),\n",
       "  (185, 1),\n",
       "  (186, 1),\n",
       "  (187, 1),\n",
       "  (188, 1),\n",
       "  (189, 1),\n",
       "  (190, 1),\n",
       "  (191, 1),\n",
       "  (192, 1),\n",
       "  (193, 1),\n",
       "  (194, 1),\n",
       "  (195, 1),\n",
       "  (196, 1),\n",
       "  (197, 1),\n",
       "  (198, 1),\n",
       "  (199, 1)],\n",
       " [(0, 3),\n",
       "  (1, 2),\n",
       "  (2, 4),\n",
       "  (3, 2),\n",
       "  (4, 9),\n",
       "  (14, 2),\n",
       "  (16, 1),\n",
       "  (18, 4),\n",
       "  (21, 3),\n",
       "  (22, 6),\n",
       "  (23, 4),\n",
       "  (24, 5),\n",
       "  (25, 1),\n",
       "  (31, 1),\n",
       "  (38, 5),\n",
       "  (39, 2),\n",
       "  (50, 2),\n",
       "  (54, 1),\n",
       "  (59, 1),\n",
       "  (61, 1),\n",
       "  (63, 1),\n",
       "  (74, 1),\n",
       "  (75, 2),\n",
       "  (83, 1),\n",
       "  (85, 1),\n",
       "  (86, 2),\n",
       "  (92, 2),\n",
       "  (95, 1),\n",
       "  (110, 2),\n",
       "  (113, 1),\n",
       "  (116, 2),\n",
       "  (135, 1),\n",
       "  (148, 1),\n",
       "  (158, 1),\n",
       "  (165, 1),\n",
       "  (172, 1),\n",
       "  (185, 1),\n",
       "  (186, 1),\n",
       "  (189, 1),\n",
       "  (200, 5),\n",
       "  (201, 1),\n",
       "  (202, 1),\n",
       "  (203, 1),\n",
       "  (204, 1),\n",
       "  (205, 1),\n",
       "  (206, 1),\n",
       "  (207, 1),\n",
       "  (208, 1),\n",
       "  (209, 1),\n",
       "  (210, 1),\n",
       "  (211, 1),\n",
       "  (212, 5),\n",
       "  (213, 2),\n",
       "  (214, 1),\n",
       "  (215, 1),\n",
       "  (216, 1),\n",
       "  (217, 1),\n",
       "  (218, 1),\n",
       "  (219, 1),\n",
       "  (220, 2),\n",
       "  (221, 1),\n",
       "  (222, 1),\n",
       "  (223, 1),\n",
       "  (224, 2),\n",
       "  (225, 1),\n",
       "  (226, 1),\n",
       "  (227, 1),\n",
       "  (228, 1),\n",
       "  (229, 1),\n",
       "  (230, 1),\n",
       "  (231, 1),\n",
       "  (232, 2),\n",
       "  (233, 1),\n",
       "  (234, 1),\n",
       "  (235, 1),\n",
       "  (236, 1),\n",
       "  (237, 1),\n",
       "  (238, 2),\n",
       "  (239, 2),\n",
       "  (240, 1),\n",
       "  (241, 1),\n",
       "  (242, 1),\n",
       "  (243, 1),\n",
       "  (244, 1),\n",
       "  (245, 1),\n",
       "  (246, 1),\n",
       "  (247, 1),\n",
       "  (248, 1),\n",
       "  (249, 1),\n",
       "  (250, 1),\n",
       "  (251, 1),\n",
       "  (252, 1),\n",
       "  (253, 1),\n",
       "  (254, 1),\n",
       "  (255, 1),\n",
       "  (256, 1),\n",
       "  (257, 1),\n",
       "  (258, 2),\n",
       "  (259, 1),\n",
       "  (260, 1),\n",
       "  (261, 1),\n",
       "  (262, 1),\n",
       "  (263, 1),\n",
       "  (264, 1),\n",
       "  (265, 1),\n",
       "  (266, 1)],\n",
       " [(2, 3),\n",
       "  (4, 1),\n",
       "  (8, 1),\n",
       "  (14, 7),\n",
       "  (18, 2),\n",
       "  (22, 2),\n",
       "  (23, 8),\n",
       "  (38, 10),\n",
       "  (39, 1),\n",
       "  (50, 4),\n",
       "  (54, 1),\n",
       "  (66, 2),\n",
       "  (67, 1),\n",
       "  (75, 1),\n",
       "  (81, 1),\n",
       "  (85, 2),\n",
       "  (96, 3),\n",
       "  (114, 1),\n",
       "  (139, 1),\n",
       "  (158, 1),\n",
       "  (172, 1),\n",
       "  (180, 1),\n",
       "  (185, 1),\n",
       "  (187, 1),\n",
       "  (212, 2),\n",
       "  (239, 1),\n",
       "  (248, 3),\n",
       "  (263, 1),\n",
       "  (267, 1),\n",
       "  (268, 1),\n",
       "  (269, 1),\n",
       "  (270, 1),\n",
       "  (271, 1),\n",
       "  (272, 1),\n",
       "  (273, 1),\n",
       "  (274, 1),\n",
       "  (275, 1),\n",
       "  (276, 1),\n",
       "  (277, 1),\n",
       "  (278, 1),\n",
       "  (279, 1),\n",
       "  (280, 1),\n",
       "  (281, 1),\n",
       "  (282, 1),\n",
       "  (283, 1),\n",
       "  (284, 1),\n",
       "  (285, 1),\n",
       "  (286, 1),\n",
       "  (287, 1),\n",
       "  (288, 1),\n",
       "  (289, 1),\n",
       "  (290, 1),\n",
       "  (291, 1),\n",
       "  (292, 1),\n",
       "  (293, 1),\n",
       "  (294, 1),\n",
       "  (295, 1)],\n",
       " [(2, 2),\n",
       "  (4, 2),\n",
       "  (8, 2),\n",
       "  (18, 1),\n",
       "  (21, 2),\n",
       "  (22, 1),\n",
       "  (23, 3),\n",
       "  (24, 2),\n",
       "  (38, 2),\n",
       "  (58, 1),\n",
       "  (139, 1),\n",
       "  (144, 1),\n",
       "  (212, 1),\n",
       "  (275, 1),\n",
       "  (284, 1),\n",
       "  (296, 1),\n",
       "  (297, 1),\n",
       "  (298, 1),\n",
       "  (299, 1),\n",
       "  (300, 1),\n",
       "  (301, 1),\n",
       "  (302, 1),\n",
       "  (303, 1),\n",
       "  (304, 1),\n",
       "  (305, 1),\n",
       "  (306, 1),\n",
       "  (307, 1),\n",
       "  (308, 1),\n",
       "  (309, 1),\n",
       "  (310, 1),\n",
       "  (311, 1)],\n",
       " [(0, 1),\n",
       "  (2, 3),\n",
       "  (4, 4),\n",
       "  (5, 1),\n",
       "  (8, 2),\n",
       "  (14, 2),\n",
       "  (18, 3),\n",
       "  (21, 2),\n",
       "  (22, 2),\n",
       "  (23, 7),\n",
       "  (24, 1),\n",
       "  (38, 5),\n",
       "  (39, 2),\n",
       "  (50, 5),\n",
       "  (58, 1),\n",
       "  (61, 2),\n",
       "  (66, 1),\n",
       "  (72, 1),\n",
       "  (75, 1),\n",
       "  (84, 1),\n",
       "  (85, 1),\n",
       "  (86, 2),\n",
       "  (110, 1),\n",
       "  (144, 1),\n",
       "  (166, 1),\n",
       "  (172, 1),\n",
       "  (177, 1),\n",
       "  (180, 1),\n",
       "  (184, 3),\n",
       "  (185, 1),\n",
       "  (195, 1),\n",
       "  (212, 1),\n",
       "  (234, 1),\n",
       "  (239, 1),\n",
       "  (268, 1),\n",
       "  (271, 1),\n",
       "  (283, 1),\n",
       "  (285, 1),\n",
       "  (311, 1),\n",
       "  (312, 1),\n",
       "  (313, 1),\n",
       "  (314, 1),\n",
       "  (315, 1),\n",
       "  (316, 1),\n",
       "  (317, 1),\n",
       "  (318, 1),\n",
       "  (319, 1),\n",
       "  (320, 1),\n",
       "  (321, 1),\n",
       "  (322, 1),\n",
       "  (323, 1),\n",
       "  (324, 1),\n",
       "  (325, 1),\n",
       "  (326, 1),\n",
       "  (327, 1),\n",
       "  (328, 1),\n",
       "  (329, 1),\n",
       "  (330, 1),\n",
       "  (331, 1),\n",
       "  (332, 1),\n",
       "  (333, 1),\n",
       "  (334, 1),\n",
       "  (335, 1),\n",
       "  (336, 1),\n",
       "  (337, 1),\n",
       "  (338, 1),\n",
       "  (339, 1),\n",
       "  (340, 1),\n",
       "  (341, 1),\n",
       "  (342, 1),\n",
       "  (343, 1),\n",
       "  (344, 1),\n",
       "  (345, 1),\n",
       "  (346, 1),\n",
       "  (347, 1)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T03:37:14.943037Z",
     "start_time": "2018-11-29T03:37:14.940036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 2), (4, 1), (18, 1), (21, 2), (23, 5), (24, 2), (25, 1), (26, 1), (27, 1), (28, 1)]\n"
     ]
    }
   ],
   "source": [
    "# sanity check: look at first 10 word ids with their frequency counts \n",
    "#  from the 2nd document\n",
    "print(corpus[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert text preprocessing? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents contain movie reviews\n",
    "# note: abbreviated to 50000 documents\n",
    "my_documents = traindf.review.head(50000).values.tolist()\n",
    "\n",
    "# simple brief example tokenizing and lowercase\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "\n",
    "#################\n",
    "# options for further text processing\n",
    "# can also remove punctuation and stop words\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))\n",
    "#######\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))\n",
    "#################\n",
    "\n",
    "# start corpus: use Dictionary class to map id to each token\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "# create a gensim corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# view corpus - a list of lists, each list item is 1 document\n",
    "# form: (id, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. gensim: create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:04:21.154164Z",
     "start_time": "2018-11-29T04:03:48.338963Z"
    }
   },
   "outputs": [],
   "source": [
    "# documents contain movie reviews\n",
    "# note: abbreviated to 50000 documents\n",
    "my_documents = traindf.review.head(50000).values.tolist()\n",
    "\n",
    "# lowercase and word tokenizing\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "\n",
    "# start corpus: use Dictionary class to map id to each token\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "# create a gensim corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# view corpus - a list of lists, each list item is 1 document\n",
    "# form: (id, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:04:21.172351Z",
     "start_time": "2018-11-29T04:04:21.155839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " \"''\": 1,\n",
       " ',': 2,\n",
       " '``': 3,\n",
       " 'a': 4,\n",
       " 'after': 5,\n",
       " 'dad': 6,\n",
       " 'directing': 7,\n",
       " 'for': 8,\n",
       " 'gag': 9,\n",
       " 'gave': 10,\n",
       " 'gift': 11,\n",
       " 'got': 12,\n",
       " 'he': 13,\n",
       " 'it': 14,\n",
       " 'kick': 15,\n",
       " 'my': 16,\n",
       " 'nunsense': 17,\n",
       " 'of': 18,\n",
       " 'out': 19,\n",
       " 'reall': 20,\n",
       " 'this': 21,\n",
       " 'to': 22,\n",
       " '.': 23,\n",
       " 'and': 24,\n",
       " 'are': 25,\n",
       " 'blues': 26,\n",
       " 'catchy': 27,\n",
       " 'cd': 28,\n",
       " 'cross-cultural': 29,\n",
       " 'fresh': 30,\n",
       " 'from': 31,\n",
       " 'great': 32,\n",
       " 'gut': 33,\n",
       " 'happiness': 34,\n",
       " 'hear': 35,\n",
       " 'her': 36,\n",
       " 'hope': 37,\n",
       " 'i': 38,\n",
       " 'is': 39,\n",
       " 'like': 40,\n",
       " 'lot': 41,\n",
       " 'mature': 42,\n",
       " 'more': 43,\n",
       " 'need': 44,\n",
       " 'people': 45,\n",
       " 'pop': 46,\n",
       " 'positive': 47,\n",
       " 'sounds': 48,\n",
       " 'strong': 49,\n",
       " 'the': 50,\n",
       " 'tunes': 51,\n",
       " 'vibes': 52,\n",
       " 'vocals': 53,\n",
       " 'we': 54,\n",
       " \"'\": 55,\n",
       " \"'d\": 56,\n",
       " \"'game\": 57,\n",
       " \"'m\": 58,\n",
       " \"'s\": 59,\n",
       " 'any': 60,\n",
       " 'be': 61,\n",
       " 'beauty': 62,\n",
       " 'been': 63,\n",
       " 'best': 64,\n",
       " 'bit': 65,\n",
       " 'but': 66,\n",
       " 'buy': 67,\n",
       " 'disagree': 68,\n",
       " 'every': 69,\n",
       " 'fade.the': 70,\n",
       " 'feel': 71,\n",
       " 'figured': 72,\n",
       " 'going': 73,\n",
       " 'if': 74,\n",
       " 'in': 75,\n",
       " 'its': 76,\n",
       " 'listening': 77,\n",
       " 'masterpiece': 78,\n",
       " 'mitsuda': 79,\n",
       " 'money': 80,\n",
       " 'much': 81,\n",
       " 'music': 82,\n",
       " 'must': 83,\n",
       " 'now': 84,\n",
       " 'on': 85,\n",
       " 'one': 86,\n",
       " 'only': 87,\n",
       " 'opinino': 88,\n",
       " 'penny': 89,\n",
       " 'pretty': 90,\n",
       " 'price': 91,\n",
       " 'reading': 92,\n",
       " 'refuses': 93,\n",
       " 'review': 94,\n",
       " 'reviews': 95,\n",
       " 'say': 96,\n",
       " 'saying': 97,\n",
       " 'simply': 98,\n",
       " 'soundtrack': 99,\n",
       " 'staggering': 100,\n",
       " 'tag': 101,\n",
       " 'that': 102,\n",
       " 'timeless': 103,\n",
       " 'ultimate': 104,\n",
       " 'worth': 105,\n",
       " 'would': 106,\n",
       " 'write': 107,\n",
       " 'yasunori': 108,\n",
       " 'years': 109,\n",
       " 'you': 110,\n",
       " ';': 111,\n",
       " 'ambiance': 112,\n",
       " 'another': 113,\n",
       " 'at': 114,\n",
       " 'below': 115,\n",
       " 'by': 116,\n",
       " 'close': 117,\n",
       " 'creation': 118,\n",
       " 'cross': 119,\n",
       " 'factors': 120,\n",
       " 'favourite': 121,\n",
       " 'filled': 122,\n",
       " 'girl': 123,\n",
       " 'include': 124,\n",
       " 'left': 125,\n",
       " 'major': 126,\n",
       " 'misuda': 127,\n",
       " 'nobuo': 128,\n",
       " 'orchestra': 129,\n",
       " 'ost': 130,\n",
       " 'question': 131,\n",
       " 'rich': 132,\n",
       " 'scars': 133,\n",
       " 'second': 134,\n",
       " 'some': 135,\n",
       " 'stars': 136,\n",
       " 'stole': 137,\n",
       " 'synthesized': 138,\n",
       " 'time': 139,\n",
       " 'times': 140,\n",
       " 'tracks': 141,\n",
       " 'uematsu.chrono': 142,\n",
       " 'uplifting': 143,\n",
       " 'very': 144,\n",
       " 'vigorous': 145,\n",
       " 'while': 146,\n",
       " 'who': 147,\n",
       " 'with': 148,\n",
       " 'without': 149,\n",
       " 'wonderful': 150,\n",
       " 'world': 151,\n",
       " 'yet': 152,\n",
       " '(': 153,\n",
       " ')': 154,\n",
       " '[': 155,\n",
       " ']': 156,\n",
       " 'amazing': 157,\n",
       " 'anyway': 158,\n",
       " 'around': 159,\n",
       " 'better': 160,\n",
       " 'dancing': 161,\n",
       " 'delight': 162,\n",
       " 'enjoyable': 163,\n",
       " 'especially': 164,\n",
       " 'few': 165,\n",
       " 'first': 166,\n",
       " 'game': 167,\n",
       " 'get': 168,\n",
       " 'getting': 169,\n",
       " 'greatest': 170,\n",
       " 'hard': 171,\n",
       " 'have': 172,\n",
       " 'history': 173,\n",
       " 'played': 174,\n",
       " 'probably': 175,\n",
       " 'really': 176,\n",
       " 'so': 177,\n",
       " 'spending': 178,\n",
       " 'usually': 179,\n",
       " 'was': 180,\n",
       " 'will': 181,\n",
       " 'worked': 182,\n",
       " 'about': 183,\n",
       " 'all': 184,\n",
       " 'an': 185,\n",
       " 'can': 186,\n",
       " 'could': 187,\n",
       " 'day': 188,\n",
       " 'even': 189,\n",
       " 'ever': 190,\n",
       " 'expensive': 191,\n",
       " 'import.some': 192,\n",
       " 'listen': 193,\n",
       " 'minute': 194,\n",
       " 'not': 195,\n",
       " 'reason': 196,\n",
       " 'there': 197,\n",
       " 'track': 198,\n",
       " 'version': 199,\n",
       " '--': 200,\n",
       " '-stay': 201,\n",
       " '5': 202,\n",
       " '8th': 203,\n",
       " 'am': 204,\n",
       " 'amazon': 205,\n",
       " 'anyone': 206,\n",
       " 'as': 207,\n",
       " 'away': 208,\n",
       " 'bad': 209,\n",
       " 'believe': 210,\n",
       " 'bits': 211,\n",
       " 'book': 212,\n",
       " 'ca': 213,\n",
       " 'contest': 214,\n",
       " 'definitely': 215,\n",
       " 'enough': 216,\n",
       " 'entered': 217,\n",
       " 'evening': 218,\n",
       " 'family': 219,\n",
       " 'far': 220,\n",
       " 'friend': 221,\n",
       " 'friends': 222,\n",
       " 'grade': 223,\n",
       " 'haddon': 224,\n",
       " 'heard': 225,\n",
       " 'herself': 226,\n",
       " 'hysterics': 227,\n",
       " 'imagine': 228,\n",
       " 'into': 229,\n",
       " 'joke': 230,\n",
       " 'kill': 231,\n",
       " 'kind': 232,\n",
       " 'know': 233,\n",
       " 'maybe': 234,\n",
       " 'mockingbird': 235,\n",
       " 'mood': 236,\n",
       " 'most': 237,\n",
       " 'ms.': 238,\n",
       " \"n't\": 239,\n",
       " 'never': 240,\n",
       " 'offer': 241,\n",
       " 'or': 242,\n",
       " 'paper': 243,\n",
       " 'paragraphs': 244,\n",
       " 'perhaps': 245,\n",
       " 'pieces': 246,\n",
       " 'quite': 247,\n",
       " 'read': 248,\n",
       " 'self-published': 249,\n",
       " 'sells': 250,\n",
       " 'send': 251,\n",
       " 'someone': 252,\n",
       " 'spent': 253,\n",
       " 'star': 254,\n",
       " 'sure': 255,\n",
       " 'term': 256,\n",
       " 'them': 257,\n",
       " 'thing': 258,\n",
       " 'those': 259,\n",
       " 'unless': 260,\n",
       " 'want': 261,\n",
       " 'were': 262,\n",
       " 'whole': 263,\n",
       " 'why': 264,\n",
       " 'worst': 265,\n",
       " 'written': 266,\n",
       " 'also': 267,\n",
       " 'back': 268,\n",
       " 'beloved.sincerly': 269,\n",
       " 'church': 270,\n",
       " 'cover': 271,\n",
       " 'dissapointed': 272,\n",
       " 'down': 273,\n",
       " 'enjoyed': 274,\n",
       " 'errors': 275,\n",
       " 'faults': 276,\n",
       " 'gives': 277,\n",
       " 'hours': 278,\n",
       " 'interesting': 279,\n",
       " 'jaylynn': 280,\n",
       " 'looking': 281,\n",
       " 'love': 282,\n",
       " 'others': 283,\n",
       " 'paid': 284,\n",
       " 'point': 285,\n",
       " 'put': 286,\n",
       " 'r': 287,\n",
       " 'sad': 288,\n",
       " 'see': 289,\n",
       " 'since': 290,\n",
       " 'spend': 291,\n",
       " 'today': 292,\n",
       " 'too': 293,\n",
       " 'two': 294,\n",
       " 'view': 295,\n",
       " 'absolutely': 296,\n",
       " 'actually': 297,\n",
       " 'add': 298,\n",
       " 'author': 299,\n",
       " 'complete': 300,\n",
       " 'disappointed': 301,\n",
       " 'embarrassed': 302,\n",
       " 'grammar': 303,\n",
       " 'nothing': 304,\n",
       " 'pathetic': 305,\n",
       " 'plot': 306,\n",
       " 'poor': 307,\n",
       " 'totally': 308,\n",
       " 'typographical': 309,\n",
       " 'up': 310,\n",
       " 'waste': 311,\n",
       " '$': 312,\n",
       " '-': 313,\n",
       " '10.95': 314,\n",
       " 'absolute': 315,\n",
       " 'beginning': 316,\n",
       " 'beware': 317,\n",
       " 'churning': 318,\n",
       " 'clear': 319,\n",
       " 'did': 320,\n",
       " 'discerning': 321,\n",
       " 'doubt': 322,\n",
       " 'drivel': 323,\n",
       " 'featured': 324,\n",
       " 'glad': 325,\n",
       " 'guess': 326,\n",
       " 'has': 327,\n",
       " 'intentional': 328,\n",
       " 'lover': 329,\n",
       " 'makes': 330,\n",
       " 'missing': 331,\n",
       " 'novel': 332,\n",
       " 'over-heated': 333,\n",
       " 'page': 334,\n",
       " 'phew': 335,\n",
       " 'prominently': 336,\n",
       " 'prose': 337,\n",
       " 'purposes': 338,\n",
       " 'quick': 339,\n",
       " 're-read': 340,\n",
       " 'removed': 341,\n",
       " 'romance': 342,\n",
       " 'satiric': 343,\n",
       " 'trouble': 344,\n",
       " 'typo': 345,\n",
       " 'wait': 346,\n",
       " 'when': 347,\n",
       " '7th': 348,\n",
       " 'age': 349,\n",
       " 'chapter': 350,\n",
       " 'decided': 351,\n",
       " 'distracted': 352,\n",
       " 'do': 353,\n",
       " 'example': 354,\n",
       " 'faith': 355,\n",
       " 'good': 356,\n",
       " 'grader': 357,\n",
       " 'grammatical': 358,\n",
       " 'had': 359,\n",
       " 'hand': 360,\n",
       " 'horrible': 361,\n",
       " 'house': 362,\n",
       " 'keep': 363,\n",
       " 'lean': 364,\n",
       " 'least': 365,\n",
       " 'mark': 366,\n",
       " 'mentioned': 367,\n",
       " 'misspelling': 368,\n",
       " 'pencil': 369,\n",
       " 'per': 370,\n",
       " 'please': 371,\n",
       " 'points': 372,\n",
       " 'relatives': 373,\n",
       " 'reviewer': 374,\n",
       " 'seems': 375,\n",
       " 'she': 376,\n",
       " 'skills': 377,\n",
       " 'spelling': 378,\n",
       " 'their': 379,\n",
       " 'twice': 380,\n",
       " 'wasting': 381,\n",
       " 'weak': 382,\n",
       " 'writing': 383,\n",
       " 'your': 384,\n",
       " \"'em\": 385,\n",
       " \"'finds\": 386,\n",
       " '4': 387,\n",
       " 'always': 388,\n",
       " 'anymore': 389,\n",
       " 'big': 390,\n",
       " 'case': 391,\n",
       " 'chefs': 392,\n",
       " 'coastal': 393,\n",
       " 'comical': 394,\n",
       " 'cool': 395,\n",
       " 'cuban': 396,\n",
       " 'effects': 397,\n",
       " 'emotional': 398,\n",
       " 'fireballing': 399,\n",
       " 'flashbacks': 400,\n",
       " 'folks': 401,\n",
       " 'gets': 402,\n",
       " 'gig': 403,\n",
       " 'handed': 404,\n",
       " 'honest': 405,\n",
       " 'hysterical': 406,\n",
       " 'idenity': 407,\n",
       " 'interaction': 408,\n",
       " 'italian': 409,\n",
       " 'kitchen': 410,\n",
       " 'latino': 411,\n",
       " 'maintenance': 412,\n",
       " 'make': 413,\n",
       " 'man': 414,\n",
       " 'me': 415,\n",
       " 'might': 416,\n",
       " 'mix': 417,\n",
       " 'motorcycle': 418,\n",
       " 'often': 419,\n",
       " 'owner': 420,\n",
       " 'perfect': 421,\n",
       " 'pitcher': 422,\n",
       " 'players': 423,\n",
       " 'plays': 424,\n",
       " 'resort': 425,\n",
       " 'right': 426,\n",
       " 'roster': 427,\n",
       " 'salsa': 428,\n",
       " 'sea': 429,\n",
       " 'searching': 430,\n",
       " 'sizzling': 431,\n",
       " 'sound': 432,\n",
       " 'special': 433,\n",
       " 'sponsored': 434,\n",
       " 'story': 435,\n",
       " 'stumbles': 436,\n",
       " 'talking': 437,\n",
       " 'team': 438,\n",
       " 'they': 439,\n",
       " 'three': 440,\n",
       " 'through': 441,\n",
       " 'young': 442,\n",
       " 'zen': 443,\n",
       " '...': 444,\n",
       " 'care': 445,\n",
       " 'excellent': 446,\n",
       " 'feet': 447,\n",
       " 'garment': 448,\n",
       " 'integrity': 449,\n",
       " 'long': 450,\n",
       " 'longer': 451,\n",
       " 'loose': 452,\n",
       " 'package': 453,\n",
       " 'proper': 454,\n",
       " 'shifts': 455,\n",
       " 'states': 456,\n",
       " 'stockings': 457,\n",
       " 'than': 458,\n",
       " 'tight': 459,\n",
       " '3': 460,\n",
       " 'actual': 461,\n",
       " 'almost': 462,\n",
       " 'arrived': 463,\n",
       " 'bought': 464,\n",
       " 'box': 465,\n",
       " 'brand': 466,\n",
       " 'bronze': 467,\n",
       " 'certain': 468,\n",
       " 'compression': 469,\n",
       " 'fits': 470,\n",
       " 'jobst': 471,\n",
       " 'less': 472,\n",
       " 'nor': 473,\n",
       " 'okay': 474,\n",
       " 'ordered': 475,\n",
       " 'pair': 476,\n",
       " 'pairs': 477,\n",
       " 'quality': 478,\n",
       " 'recieve': 479,\n",
       " 'store': 480,\n",
       " 'sun': 481,\n",
       " 'tags': 482,\n",
       " 'took': 483,\n",
       " 'weeks': 484,\n",
       " 'chart': 485,\n",
       " 'check': 486,\n",
       " 'go': 487,\n",
       " 'internet..it': 488,\n",
       " 'item': 489,\n",
       " 'recomended': 490,\n",
       " 'sheer': 491,\n",
       " 'should': 492,\n",
       " 'sizes': 493,\n",
       " 'smaller': 494,\n",
       " 'tried': 495,\n",
       " 'what': 496,\n",
       " '1': 497,\n",
       " 'court': 498,\n",
       " 'facts': 499,\n",
       " 'full': 500,\n",
       " 'general': 501,\n",
       " 'intrigue': 502,\n",
       " 'james': 503,\n",
       " 'key': 504,\n",
       " 'overview': 505,\n",
       " 'provides': 506,\n",
       " \"'old\": 507,\n",
       " \"'ve\": 508,\n",
       " '16mm': 509,\n",
       " 'acclaimed': 510,\n",
       " 'appear': 511,\n",
       " 'award': 512,\n",
       " 'black': 513,\n",
       " 'bootleg': 514,\n",
       " 'bridge': 515,\n",
       " 'bright': 516,\n",
       " 'captain': 517,\n",
       " 'clouded': 518,\n",
       " 'combined': 519,\n",
       " 'comes': 520,\n",
       " 'commands': 521,\n",
       " 'condition': 522,\n",
       " 'contrasts': 523,\n",
       " 'copy': 524,\n",
       " 'corner': 525,\n",
       " 'countryside': 526,\n",
       " 'critically': 527,\n",
       " 'crystal': 528,\n",
       " 'cue': 529,\n",
       " 'dark': 530,\n",
       " 'dealer.if': 531,\n",
       " 'defining': 532,\n",
       " 'droppings': 533,\n",
       " 'dull': 534,\n",
       " 'dvd': 535,\n",
       " 'early': 536,\n",
       " 'enunciation': 537,\n",
       " 'events': 538,\n",
       " 'everything': 539,\n",
       " 'film': 540,\n",
       " 'ground': 541,\n",
       " 'haze': 542,\n",
       " 'his': 543,\n",
       " 'home': 544,\n",
       " 'image': 545,\n",
       " 'immediate.here': 546,\n",
       " 'insect': 547,\n",
       " 'library': 548,\n",
       " 'light': 549,\n",
       " 'lighting': 550,\n",
       " 'memory': 551,\n",
       " 'mists': 552,\n",
       " 'morning': 553,\n",
       " 'muddy': 554,\n",
       " 'none': 555,\n",
       " 'packaging': 556,\n",
       " 'pixelations': 557,\n",
       " 'presentation': 558,\n",
       " 'public': 559,\n",
       " 'random': 560,\n",
       " 'rather': 561,\n",
       " 'reasonably': 562,\n",
       " 'reel.just': 563,\n",
       " 'remember': 564,\n",
       " 'resolution': 565,\n",
       " 'scenes': 566,\n",
       " 'scratches': 567,\n",
       " 'seen': 568,\n",
       " 'set': 569,\n",
       " 'somewhere': 570,\n",
       " 'standard': 571,\n",
       " 'straight': 572,\n",
       " 'street': 573,\n",
       " 'surrounding': 574,\n",
       " 'timbre': 575,\n",
       " 'vague': 576,\n",
       " 'visuals': 577,\n",
       " 'visuals.after': 578,\n",
       " 'water': 579,\n",
       " 'white': 580,\n",
       " 'winning': 581,\n",
       " 'youtube': 582,\n",
       " 'ages': 583,\n",
       " 'ando': 584,\n",
       " 'bombarded': 585,\n",
       " 'clothing': 586,\n",
       " 'country': 587,\n",
       " 'creating': 588,\n",
       " 'culture': 589,\n",
       " 'daughters': 590,\n",
       " 'developing': 591,\n",
       " 'each': 592,\n",
       " 'elders': 593,\n",
       " 'etc': 594,\n",
       " 'excesses': 595,\n",
       " 'fads': 596,\n",
       " 'firm': 597,\n",
       " 'foundation': 598,\n",
       " 'generation': 599,\n",
       " 'groups': 600,\n",
       " 'healthy': 601,\n",
       " 'hopes': 602,\n",
       " 'however': 603,\n",
       " 'hybrid': 604,\n",
       " 'identity': 605,\n",
       " 'itto': 606,\n",
       " 'japan': 607,\n",
       " 'learn': 608,\n",
       " 'lives': 609,\n",
       " 'members': 610,\n",
       " 'memorable': 611,\n",
       " 'new': 612,\n",
       " 'old': 613,\n",
       " 'our': 614,\n",
       " 'own': 615,\n",
       " 'poignantly': 616,\n",
       " 'preserving': 617,\n",
       " 'pure': 618,\n",
       " 'recommended': 619,\n",
       " 'respect': 620,\n",
       " 'retains': 621,\n",
       " 'rising': 622,\n",
       " 'rock': 623,\n",
       " 'seem': 624,\n",
       " 'shopping': 625,\n",
       " 'sons': 626,\n",
       " 'steven': 627,\n",
       " 'stevern': 628,\n",
       " 'thatjapanese': 629,\n",
       " 'tradition': 630,\n",
       " 'values': 631,\n",
       " 'wardell': 632,\n",
       " 'western': 633,\n",
       " 'writes': 634,\n",
       " 'x': 635,\n",
       " \"'mama\": 636,\n",
       " 'angel': 637,\n",
       " 'blue': 638,\n",
       " 'days': 639,\n",
       " 'find': 640,\n",
       " 'hair': 641,\n",
       " 'just': 642,\n",
       " 'lanna': 643,\n",
       " 'listened': 644,\n",
       " 'neck.roy': 645,\n",
       " 'o': 646,\n",
       " 'off': 647,\n",
       " 'rose': 648,\n",
       " 'same': 649,\n",
       " 'singer': 650,\n",
       " 'song': 651,\n",
       " 'songs': 652,\n",
       " 'talent': 653,\n",
       " 'thought': 654,\n",
       " 'trully': 655,\n",
       " 'album': 656,\n",
       " 'albums': 657,\n",
       " 'based': 658,\n",
       " 'couple': 659,\n",
       " 'fact': 660,\n",
       " 'fans': 661,\n",
       " 'harsh': 662,\n",
       " 'hearing': 663,\n",
       " 'it.the': 664,\n",
       " 'liner': 665,\n",
       " 'music.well': 666,\n",
       " 'notes': 667,\n",
       " 'orbison': 668,\n",
       " 'other': 669,\n",
       " 'roy': 670,\n",
       " 'take': 671,\n",
       " 'value': 672,\n",
       " 'wrote': 673,\n",
       " \"'bootleg\": 674,\n",
       " '4cd': 675,\n",
       " 'because': 676,\n",
       " 'better.having': 677,\n",
       " 'concert': 678,\n",
       " 'disappointing': 679,\n",
       " 'does': 680,\n",
       " 'live': 681,\n",
       " 'mean': 682,\n",
       " 'overall': 683,\n",
       " 'recording': 684,\n",
       " 'saw': 685,\n",
       " 'slightly': 686,\n",
       " 'sounded': 687,\n",
       " 'title': 688,\n",
       " 'aa': 689,\n",
       " 'aaa': 690,\n",
       " 'apply': 691,\n",
       " 'batteries': 692,\n",
       " 'became': 693,\n",
       " 'button': 694,\n",
       " 'buttons': 695,\n",
       " 'charge': 696,\n",
       " 'charger': 697,\n",
       " 'charges': 698,\n",
       " 'crayon': 699,\n",
       " 'duct': 700,\n",
       " 'end': 701,\n",
       " 'fine': 702,\n",
       " 'flip': 703,\n",
       " 'four': 704,\n",
       " 'hold': 705,\n",
       " 'horizontal': 706,\n",
       " 'how': 707,\n",
       " 'huge': 708,\n",
       " 'little': 709,\n",
       " 'mechanism': 710,\n",
       " 'painful': 711,\n",
       " 'pressure': 712,\n",
       " 'problem': 713,\n",
       " 'push': 714,\n",
       " 'securing': 715,\n",
       " 'segment': 716,\n",
       " 'tape': 717,\n",
       " 'using': 718,\n",
       " 'wo': 719,\n",
       " 'wrap': 720,\n",
       " '140': 721,\n",
       " '2100': 722,\n",
       " '64': 723,\n",
       " 'bye': 724,\n",
       " 'carry': 725,\n",
       " 'greetings': 726,\n",
       " 'k6000': 727,\n",
       " 'kodak': 728,\n",
       " 'mah': 729,\n",
       " 'minutes': 730,\n",
       " 'nimh': 731,\n",
       " 'which': 732,\n",
       " '1850': 733,\n",
       " 'battery': 734,\n",
       " 'cheap': 735,\n",
       " 'come': 736,\n",
       " 'drain': 737,\n",
       " 'gps': 738,\n",
       " 'hour.btw': 739,\n",
       " 'pay': 740,\n",
       " 'themselves': 741,\n",
       " 'these': 742,\n",
       " 'camera': 743,\n",
       " 'digital': 744,\n",
       " 'handy': 745,\n",
       " 'life': 746,\n",
       " 'product': 747,\n",
       " '2003': 748,\n",
       " 'alkaline': 749,\n",
       " 'convenient': 750,\n",
       " 'design': 751,\n",
       " 'disposables': 752,\n",
       " 'elsewhere': 753,\n",
       " 'jul': 754,\n",
       " 'look': 755,\n",
       " 'nice': 756,\n",
       " 'ok': 757,\n",
       " 'power': 758,\n",
       " 'staying': 759,\n",
       " 'well': 760,\n",
       " 'year': 761,\n",
       " '?': 762,\n",
       " 'before': 763,\n",
       " 'capacity': 764,\n",
       " 'charge/discharge': 765,\n",
       " 'charged': 766,\n",
       " 'cycled': 767,\n",
       " 'cycles': 768,\n",
       " 'doing': 769,\n",
       " 'finally': 770,\n",
       " 'higher': 771,\n",
       " 'lasts': 772,\n",
       " 'level': 773,\n",
       " 'may': 774,\n",
       " 'ni-mh': 775,\n",
       " 'pleasantly': 776,\n",
       " 'rated': 777,\n",
       " 'reach': 778,\n",
       " 'recharged': 779,\n",
       " 'row': 780,\n",
       " 'said': 781,\n",
       " 'short': 782,\n",
       " 'surprised': 783,\n",
       " 'try': 784,\n",
       " 'until': 785,\n",
       " '2': 786,\n",
       " '4.0.': 787,\n",
       " 'armando': 788,\n",
       " 'ass': 789,\n",
       " 'awesome': 790,\n",
       " 'boost': 791,\n",
       " 'call': 792,\n",
       " 'cent': 793,\n",
       " 'cologne': 794,\n",
       " 'coming': 795,\n",
       " 'dramatica': 796,\n",
       " 'efforts': 797,\n",
       " 'euro': 798,\n",
       " 'eventually': 799,\n",
       " 'fail': 800,\n",
       " 'german': 801,\n",
       " 'germany': 802,\n",
       " 'guide': 803,\n",
       " 'happy': 804,\n",
       " 'having': 805,\n",
       " 'highly': 806,\n",
       " 'let': 807,\n",
       " 'lost': 808,\n",
       " 'made': 809,\n",
       " 'making': 810,\n",
       " 'mother': 811,\n",
       " 'native': 812,\n",
       " 'online': 813,\n",
       " 'payed': 814,\n",
       " 'recommend': 815,\n",
       " 'roundabout': 816,\n",
       " 'save': 817,\n",
       " 'saved': 818,\n",
       " 'software': 819,\n",
       " 'speaker': 820,\n",
       " 'ten': 821,\n",
       " 'thank': 822,\n",
       " 'though': 823,\n",
       " 'tongue': 824,\n",
       " 'use': 825,\n",
       " 'wish': 826,\n",
       " 'work': 827,\n",
       " 'yours': 828,\n",
       " \"'re\": 829,\n",
       " 'both': 830,\n",
       " 'brother': 831,\n",
       " 'choice': 832,\n",
       " 'christmas': 833,\n",
       " 'combination': 834,\n",
       " 'easy': 835,\n",
       " 'electronics': 836,\n",
       " 'jvc': 837,\n",
       " 'liked': 838,\n",
       " 'modes': 839,\n",
       " 'name': 840,\n",
       " 'operates': 841,\n",
       " 'playback': 842,\n",
       " 'player': 843,\n",
       " 'purchased': 844,\n",
       " 'remote': 845,\n",
       " 'several': 846,\n",
       " 'trusted': 847,\n",
       " 'tv': 848,\n",
       " 'vcr': 849,\n",
       " 'wife': 850,\n",
       " ':': 851,\n",
       " 'agree': 852,\n",
       " 'awkward': 853,\n",
       " 'comments': 854,\n",
       " 'complaints': 855,\n",
       " 'complicated': 856,\n",
       " 'deciding': 857,\n",
       " 'dvd-land': 858,\n",
       " 'enter': 859,\n",
       " 'gotten': 860,\n",
       " 'hang': 861,\n",
       " 'heavily': 862,\n",
       " 'how-to': 863,\n",
       " 'intuitive': 864,\n",
       " 'it.two': 865,\n",
       " 'join': 866,\n",
       " 'manual': 867,\n",
       " 'many': 868,\n",
       " 'movies': 869,\n",
       " 'myself': 870,\n",
       " 'options': 871,\n",
       " 'present': 872,\n",
       " 'rely': 873,\n",
       " 'rest': 874,\n",
       " 'scroll': 875,\n",
       " 'selection': 876,\n",
       " 'set-up': 877,\n",
       " 'setting': 878,\n",
       " 'something': 879,\n",
       " 'start': 880,\n",
       " 'technically-minded': 881,\n",
       " 'think': 882,\n",
       " 'timer': 883,\n",
       " 'tv/vhs/dvd': 884,\n",
       " 'vcr/dvd': 885,\n",
       " 'vhs': 886,\n",
       " 'wanting': 887,\n",
       " '6': 888,\n",
       " 'alarm': 889,\n",
       " 'cents': 890,\n",
       " 'clock': 891,\n",
       " 'display': 892,\n",
       " 'displays': 893,\n",
       " 'drawback': 894,\n",
       " 'ep': 895,\n",
       " 'fairly': 896,\n",
       " 'fit': 897,\n",
       " 'hour': 898,\n",
       " 'hr': 899,\n",
       " 'lack': 900,\n",
       " 'lettering': 901,\n",
       " 'lp': 902,\n",
       " 'mode': 903,\n",
       " 'orange': 904,\n",
       " 'record': 905,\n",
       " 'reminds': 906,\n",
       " 'seconds': 907,\n",
       " 'sp': 908,\n",
       " 'speeds': 909,\n",
       " 'stuff': 910,\n",
       " 'taping': 911,\n",
       " 'trivial': 912,\n",
       " 'ugly': 913,\n",
       " 'universal': 914,\n",
       " 'works': 915,\n",
       " 'began': 916,\n",
       " 'disc': 917,\n",
       " 'giving': 918,\n",
       " 'here': 919,\n",
       " 'hte': 920,\n",
       " 'incorrect': 921,\n",
       " 'problems': 922,\n",
       " 'quit': 923,\n",
       " 'shot': 924,\n",
       " 'side': 925,\n",
       " 'sign': 926,\n",
       " 'sometimes': 927,\n",
       " 'sony': 928,\n",
       " 'sticking': 929,\n",
       " 'still': 930,\n",
       " 'understand': 931,\n",
       " 'useless': 932,\n",
       " '12': 933,\n",
       " 'allow': 934,\n",
       " 'although': 935,\n",
       " 'capability': 936,\n",
       " 'component': 937,\n",
       " 'input': 938,\n",
       " 'machine': 939,\n",
       " 'mp3': 940,\n",
       " 'playing': 941,\n",
       " 'primarily': 942,\n",
       " 'replaced': 943,\n",
       " 'select': 944,\n",
       " 'sub-par': 945,\n",
       " 'tivo': 946,\n",
       " 'turns': 947,\n",
       " 'versatile': 948,\n",
       " 'wants': 949,\n",
       " 'year-old': 950,\n",
       " 'atrocious': 951,\n",
       " 'brags': 952,\n",
       " 'color': 953,\n",
       " 'desired': 954,\n",
       " 'either': 955,\n",
       " 'experience': 956,\n",
       " 'factory': 957,\n",
       " 'fall': 958,\n",
       " 'features': 959,\n",
       " 'frequency': 960,\n",
       " 'inventor': 961,\n",
       " 'noisy': 962,\n",
       " 'pre-records': 963,\n",
       " 'recordings': 964,\n",
       " 'response': 965,\n",
       " 'saddled': 966,\n",
       " 'shame': 967,\n",
       " 'smeared': 968,\n",
       " 'tuner': 969,\n",
       " 'unit': 970,\n",
       " 'wise': 971,\n",
       " 'concepts': 972,\n",
       " 'ee': 973,\n",
       " 'engineers': 974,\n",
       " 'found': 975,\n",
       " 'students': 976,\n",
       " 'useful': 977,\n",
       " 'algorithm': 978,\n",
       " 'appears': 979,\n",
       " 'authors': 980,\n",
       " 'bothersome': 981,\n",
       " 'buzo': 982,\n",
       " 'co-creator': 983,\n",
       " 'commonplace': 984,\n",
       " 'comprehensive': 985,\n",
       " 'content': 986,\n",
       " 'demand': 987,\n",
       " 'fortunately': 988,\n",
       " 'gray': 989,\n",
       " 'hardcover': 990,\n",
       " 'images': 991,\n",
       " 'itself': 992,\n",
       " 'laughably': 993,\n",
       " 'lbg': 994,\n",
       " 'linde': 995,\n",
       " 'manifests': 996,\n",
       " 'near': 997,\n",
       " 'print': 998,\n",
       " 'printing': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check: look at dictionary token and id\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:04:27.026327Z",
     "start_time": "2018-11-29T04:04:27.023523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 2), (4, 1), (18, 1), (21, 2), (23, 5), (24, 2), (25, 1), (26, 1), (27, 1), (28, 1)]\n"
     ]
    }
   ],
   "source": [
    "# sanity check: look at first 10 word ids with their frequency counts \n",
    "#  from the 2nd document\n",
    "print(corpus[1][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. gensim: create a tf-idf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:04:31.158852Z",
     "start_time": "2018-11-29T04:04:30.343424Z"
    }
   },
   "outputs": [],
   "source": [
    "# use bag of words corpus and translate to TfidfModel\n",
    "tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:04:33.094234Z",
     "start_time": "2018-11-29T04:04:33.088343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.01986542514235891),\n",
       " (4, 0.009670564372163796),\n",
       " (18, 0.014603081325142419),\n",
       " (21, 0.02394427960261586),\n",
       " (23, 0.008009350152255625),\n",
       " (24, 0.017120182403592487),\n",
       " (25, 0.046731095776159325),\n",
       " (26, 0.2274864647257741),\n",
       " (27, 0.22033262254283312),\n",
       " (28, 0.11403784680523987),\n",
       " (29, 0.4018838776964095),\n",
       " (30, 0.2094822997039569),\n",
       " (31, 0.06301992607999961),\n",
       " (32, 0.06821228045601305),\n",
       " (33, 0.31253946803324956),\n",
       " (34, 0.2649682185098172),\n",
       " (35, 0.15830301418490395),\n",
       " (36, 0.10167012931352057),\n",
       " (37, 0.15161403571746326),\n",
       " (38, 0.012424377323207442),\n",
       " (39, 0.01580335768549762),\n",
       " (40, 0.05819096864793154),\n",
       " (41, 0.11496164131924472),\n",
       " (42, 0.24284948776090634),\n",
       " (43, 0.06847880826489389),\n",
       " (44, 0.12664786489785124),\n",
       " (45, 0.10388046230349512),\n",
       " (46, 0.18538486816050764),\n",
       " (47, 0.20506821676966816),\n",
       " (48, 0.16293449892253717),\n",
       " (49, 0.17253149721844768),\n",
       " (50, 0.008353822879747334),\n",
       " (51, 0.21516137711421968),\n",
       " (52, 0.32465879579786927),\n",
       " (53, 0.20263771196076216),\n",
       " (54, 0.09759696550608683)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reference each document like a dictionary\n",
    "# displays [(token_id, token_weights)]\n",
    "tfidf[corpus[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. determine new significant terms for your corpus by applying gensim's tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:04:42.130554Z",
     "start_time": "2018-11-29T04:04:42.090840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.01986542514235891), (4, 0.009670564372163796), (18, 0.014603081325142419), (21, 0.02394427960261586), (23, 0.008009350152255625)]\n",
      "cross-cultural 0.4018838776964095\n",
      "vibes 0.32465879579786927\n",
      "gut 0.31253946803324956\n",
      "happiness 0.2649682185098172\n",
      "mature 0.24284948776090634\n"
     ]
    }
   ],
   "source": [
    "# Save the 2nd document: doc\n",
    "doc = corpus[1]\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, \n",
    "                              key=lambda w: w[1], \n",
    "                              reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing\n",
    "Remove\n",
    "- stop words\n",
    "- non-alpha characters\n",
    "- lemmatize\n",
    "- perform bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))\n",
    "\n",
    "# [(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 68), \n",
    "#  ('to', 63), ('a', 60), ('in', 44), ('and', 41), ('(', 40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim\n",
    "Advantages\n",
    "- uses top academic models to perform complex tasks\n",
    "    - building document or word vectors\n",
    "    - performing topic identification and document comparison\n",
    "- LDA used for topic analysis and modeling\n",
    "- corpus/corpora = set of texts used to perform NLP tasks\n",
    "- gensim models can be easily saved, updated, and reused\n",
    "- dictionary can also be updated\n",
    "    - with new texts\n",
    "    - words that meet certain thresholds\n",
    "    - then use for feature exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bag of words corpus and translate to TfidfModel\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# reference each document like a dictionary\n",
    "# displays [(token_id, token_weights)]\n",
    "tfidf[corpus[1]]\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, \n",
    "                              key=lambda w: w[1], \n",
    "                              reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-entity recognition (NER)\n",
    "- Who? What? When? Where?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "Advantages using spaCy for NER\n",
    "- focus on creating NLP pipelines to generate model and corpora\n",
    "- informal language corpora\n",
    "    - easily find entities in Tweets and chat messages\n",
    "\n",
    "- NLP library similar to gensim, with different implementations\n",
    "- additional NER compared to nltk\n",
    "    - NORP, CARDINAL, MONEY, WORKOFART, LANGUAGE, EVENT\n",
    "- displaCy\n",
    "    - entity recognition visualization tool to view parse trees\n",
    "    - which uses Node.js to create interactive text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "# Additional args to improve execution time\n",
    "nlp = spacy.load('en', tagger=False, parser=False, \n",
    "                 matcher=False)\n",
    "\n",
    "# load new document\n",
    "doc = nlp(\"\"\"Berlin is the capital of Germany;\n",
    "and the residence of Chancellor Angela Merkel.\"\"\")\n",
    "\n",
    "# named entities are stored in .ents\n",
    "print(doc.ents)\n",
    "\n",
    "# check out each label (.label_) using indexing\n",
    "print(doc.ents[0], doc.ents[0].label_)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:05:09.194921Z",
     "start_time": "2018-11-29T04:05:09.173384Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data into a dataframe\n",
    "# df = ...\n",
    "\n",
    "# set text\n",
    "X = traindf['review']\n",
    "# set target label\n",
    "y = traindf['rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.33, random_state=53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:05:49.647654Z",
     "start_time": "2018-11-29T04:05:44.807077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '000cps', '000fortunately', '000if', '000kb', '000sg', '000th', '001', '002']\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer - bag-of-words\n",
    "\n",
    "# create a count vectorizer and remove stop_words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "# create bag-of-words vectors on train/test sets\n",
    "# fit_transform will create a bag-of-words dictionary \n",
    "#  and vectors for each document\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "# note: if you have unknown words in test set only, may\n",
    "#  need more data or remove those words from the test dataset\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:30:56.897472Z",
     "start_time": "2018-11-29T04:30:46.033396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '000cps', '000fortunately', '000if', '000kb', '000sg', '000th', '001', '002']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# tfidf vectors for documents\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                   max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:08:01.115208Z",
     "start_time": "2018-11-29T04:07:52.624402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  000  000cps  000fortunately  000if  000kb  000sg  000th  001  002  \\\n",
      "0   0    0       0               0      0      0      0      0    0    0   \n",
      "1   0    0       0               0      0      0      0      0    0    0   \n",
      "2   0    0       0               0      0      0      0      0    0    0   \n",
      "3   0    0       0               0      0      0      0      0    0    0   \n",
      "4   0    0       0               0      0      0      0      0    0    0   \n",
      "\n",
      "   ...    ér  ésta  éste  éstos  était  étoiles  étranger  été  éxitos  único  \n",
      "0  ...     0     0     0      0      0        0         0    0       0      0  \n",
      "1  ...     0     0     0      0      0        0         0    0       0      0  \n",
      "2  ...     0     0     0      0      0        0         0    0       0      0  \n",
      "3  ...     0     0     0      0      0        0         0    0       0      0  \n",
      "4  ...     0     0     0      0      0        0         0    0       0      0  \n",
      "\n",
      "[5 rows x 87592 columns]\n",
      "    00  000  000cps  000fortunately  000if  000kb  000sg  000th  001  002  \\\n",
      "0  0.0  0.0     0.0             0.0    0.0    0.0    0.0    0.0  0.0  0.0   \n",
      "1  0.0  0.0     0.0             0.0    0.0    0.0    0.0    0.0  0.0  0.0   \n",
      "2  0.0  0.0     0.0             0.0    0.0    0.0    0.0    0.0  0.0  0.0   \n",
      "3  0.0  0.0     0.0             0.0    0.0    0.0    0.0    0.0  0.0  0.0   \n",
      "4  0.0  0.0     0.0             0.0    0.0    0.0    0.0    0.0  0.0  0.0   \n",
      "\n",
      "   ...     ér  ésta  éste  éstos  était  étoiles  étranger  été  éxitos  único  \n",
      "0  ...    0.0   0.0   0.0    0.0    0.0      0.0       0.0  0.0     0.0    0.0  \n",
      "1  ...    0.0   0.0   0.0    0.0    0.0      0.0       0.0  0.0     0.0    0.0  \n",
      "2  ...    0.0   0.0   0.0    0.0    0.0      0.0       0.0  0.0     0.0    0.0  \n",
      "3  ...    0.0   0.0   0.0    0.0    0.0      0.0       0.0  0.0     0.0    0.0  \n",
      "4  ...    0.0   0.0   0.0    0.0    0.0      0.0       0.0  0.0     0.0    0.0  \n",
      "\n",
      "[5 rows x 87592 columns]\n",
      "set()\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# inspect vectors in pandas dataframe\n",
    "\n",
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, \n",
    "                        columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, \n",
    "                        columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweaking alpha\n",
    "- try gridsearchCV ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:09:07.973912Z",
     "start_time": "2018-11-29T04:09:07.474761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.3543939393939394\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.40145454545454545\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.40793939393939394\n",
      "\n",
      "Alpha:  0.30000000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.41193939393939394\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.4155151515151515\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.4166060606060606\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.4172121212121212\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.4195757575757576\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.4210606060606061\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.42193939393939395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune alpha with gridsearch - tinkering here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:19:39.662522Z",
     "start_time": "2018-11-29T04:19:38.871770Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Model Parameters: {'nb__alpha': 1}\n"
     ]
    }
   ],
   "source": [
    "# # Create the list of alphas: alphas\n",
    "# alphas = np.arange(0,1,0.1)\n",
    "\n",
    "# # Define train_and_predict()\n",
    "# def train_and_predict(alpha):\n",
    "#     # Instantiate the classifier: nb_classifier\n",
    "#     nb_classifier = MultinomialNB(alpha=alpha)\n",
    "#     # Fit to the training data\n",
    "#     nb_classifier.fit(tfidf_train, y_train)\n",
    "#     # Predict the labels: pred\n",
    "#     pred = nb_classifier.predict(tfidf_test)\n",
    "#     # Compute accuracy: score\n",
    "#     score = metrics.accuracy_score(y_test, pred)\n",
    "#     return score\n",
    "\n",
    "# CV and scaling in a pipeline\n",
    "steps = [('nb', MultinomialNB())]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Specify hyperparameter space using a dictionary\n",
    "parameters = {'nb__alpha':[0, 1, 0.1]}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "cv.fit(tfidf_train, y_train)\n",
    "pred = cv.predict(tfidf_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "# print best parameters\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))\n",
    "# print(\"MultinomialNB training accuracy:\", cv.score(X_train, y_train))\n",
    "# print(\"Test Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
    "# print(classification_report(y_test, pred))\n",
    "# print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:31:44.306446Z",
     "start_time": "2018-11-29T04:31:44.250273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42193939393939395\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB(alpha=alpha)\n",
    "# Fit to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "# Predict the labels: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "# Compute accuracy: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:31:47.968616Z",
     "start_time": "2018-11-29T04:31:47.777572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [(-11.96514592841074, '000cps'), (-11.96514592841074, '000kb'), (-11.96514592841074, '000sg'), (-11.96514592841074, '000th'), (-11.96514592841074, '001'), (-11.96514592841074, '002'), (-11.96514592841074, '003'), (-11.96514592841074, '004g'), (-11.96514592841074, '008'), (-11.96514592841074, '0080'), (-11.96514592841074, '0083'), (-11.96514592841074, '00mini'), (-11.96514592841074, '00movie'), (-11.96514592841074, '0130944106'), (-11.96514592841074, '0140194703'), (-11.96514592841074, '014044100x'), (-11.96514592841074, '0140449191'), (-11.96514592841074, '015'), (-11.96514592841074, '016071'), (-11.96514592841074, '0199558292there')]\n",
      "2 [(-7.048749421112344, 'better'), (-7.040163503384698, 'work'), (-7.0336663506418695, 'worst'), (-6.981009578428602, 'really'), (-6.979421783593806, 'bought'), (-6.947990337473621, 'dvd'), (-6.9006697846590574, 'did'), (-6.808287645648438, 'product'), (-6.802725100397831, 'good'), (-6.722444636014009, 'bad'), (-6.721830825864573, 'waste'), (-6.682676843231165, 'buy'), (-6.610047599347548, 'time'), (-6.584719115876378, 'read'), (-6.524927358877093, 'don'), (-6.509631613535032, 'money'), (-6.447941156542855, 'like'), (-6.40553621061421, 'just'), (-6.102540718239108, 'movie'), (-5.813352574610484, 'book')]\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:32:16.811405Z",
     "start_time": "2018-11-29T04:32:16.705032Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '000cps',\n",
       " '000fortunately',\n",
       " '000if',\n",
       " '000kb',\n",
       " '000sg',\n",
       " '000th',\n",
       " '001',\n",
       " '002',\n",
       " '003',\n",
       " '004g',\n",
       " '007',\n",
       " '008',\n",
       " '0080',\n",
       " '0083',\n",
       " '00am',\n",
       " '00mini',\n",
       " '00movie',\n",
       " '00plus',\n",
       " '00pm',\n",
       " '01',\n",
       " '010',\n",
       " '0130944106',\n",
       " '0140194703',\n",
       " '014044100x',\n",
       " '0140449191',\n",
       " '015',\n",
       " '016',\n",
       " '016071',\n",
       " '0199558292there',\n",
       " '02',\n",
       " '020',\n",
       " '023',\n",
       " '028',\n",
       " '029',\n",
       " '0291i',\n",
       " '0291it',\n",
       " '03',\n",
       " '0316',\n",
       " '0316341142',\n",
       " '034',\n",
       " '036',\n",
       " '0385658389',\n",
       " '03movie',\n",
       " '03s',\n",
       " '04',\n",
       " '04bought',\n",
       " '04s',\n",
       " '05',\n",
       " '0517220784',\n",
       " '059',\n",
       " '05warning',\n",
       " '06',\n",
       " '061009',\n",
       " '0618260584',\n",
       " '06rate',\n",
       " '06s',\n",
       " '07',\n",
       " '0745',\n",
       " '0746',\n",
       " '075',\n",
       " '07i',\n",
       " '08',\n",
       " '081',\n",
       " '0830',\n",
       " '089',\n",
       " '08th',\n",
       " '09',\n",
       " '090',\n",
       " '0920205',\n",
       " '0967979013',\n",
       " '0conclusionthis',\n",
       " '0dictionary',\n",
       " '0f',\n",
       " '0go',\n",
       " '0l',\n",
       " '0mbps',\n",
       " '0mm',\n",
       " '0ne',\n",
       " '0o0n',\n",
       " '0s9',\n",
       " '0sx',\n",
       " '0this',\n",
       " '0x0444',\n",
       " '0x04a7',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '10000000',\n",
       " '10000stars',\n",
       " '1000m',\n",
       " '1000mah',\n",
       " '1000pp',\n",
       " '1000s',\n",
       " '1000w',\n",
       " '1000x',\n",
       " '1001',\n",
       " '100409',\n",
       " '100cd',\n",
       " '100ct',\n",
       " '100ft',\n",
       " '100hz',\n",
       " '100is',\n",
       " '100k',\n",
       " '100l',\n",
       " '100lbs',\n",
       " '100m',\n",
       " '100mg',\n",
       " '100mph',\n",
       " '100pack',\n",
       " '100s',\n",
       " '100th',\n",
       " '100w',\n",
       " '100x',\n",
       " '100yen',\n",
       " '101',\n",
       " '1010',\n",
       " '1011',\n",
       " '1012',\n",
       " '1013',\n",
       " '1015',\n",
       " '1019',\n",
       " '101st',\n",
       " '102',\n",
       " '1020',\n",
       " '1020s',\n",
       " '1022',\n",
       " '1024',\n",
       " '1024x768',\n",
       " '1025',\n",
       " '1028',\n",
       " '103',\n",
       " '10303',\n",
       " '103format',\n",
       " '104',\n",
       " '1040',\n",
       " '1041',\n",
       " '105',\n",
       " '1050061',\n",
       " '10516',\n",
       " '10567',\n",
       " '106',\n",
       " '10679',\n",
       " '107',\n",
       " '1070',\n",
       " '1071567',\n",
       " '107the',\n",
       " '108',\n",
       " '1080',\n",
       " '1080i',\n",
       " '1080p',\n",
       " '108962',\n",
       " '108g',\n",
       " '108mb',\n",
       " '108mbps',\n",
       " '108mbs',\n",
       " '108mhz',\n",
       " '109',\n",
       " '10acting',\n",
       " '10action',\n",
       " '10all',\n",
       " '10am',\n",
       " '10arrgh',\n",
       " '10art',\n",
       " '10baby',\n",
       " '10back',\n",
       " '10base2',\n",
       " '10blue',\n",
       " '10boating',\n",
       " '10body',\n",
       " '10bottom',\n",
       " '10bubblestand',\n",
       " '10by',\n",
       " '10calling',\n",
       " '10cc',\n",
       " '10cool',\n",
       " '10culture',\n",
       " '10dancer',\n",
       " '10db',\n",
       " '10detail',\n",
       " '10dollars',\n",
       " '10don',\n",
       " '10editing',\n",
       " '10employee',\n",
       " '10everything',\n",
       " '10flying',\n",
       " '10fools',\n",
       " '10ft',\n",
       " '10fun',\n",
       " '10g',\n",
       " '10gameplay',\n",
       " '10gb',\n",
       " '10goodbye',\n",
       " '10gore',\n",
       " '10hall',\n",
       " '10hello',\n",
       " '10holiday',\n",
       " '10home',\n",
       " '10hooky',\n",
       " '10hrs',\n",
       " '10i',\n",
       " '10in',\n",
       " '10indian',\n",
       " '10jellyfish',\n",
       " '10jellyfishing',\n",
       " '10k',\n",
       " '10las',\n",
       " '10lb',\n",
       " '10levon',\n",
       " '10life',\n",
       " '10madman',\n",
       " '10mb',\n",
       " '10mermaidman',\n",
       " '10million',\n",
       " '10min',\n",
       " '10mm',\n",
       " '10mo',\n",
       " '10month',\n",
       " '10mp',\n",
       " '10mth',\n",
       " '10musclebob',\n",
       " '10my',\n",
       " '10nature',\n",
       " '10naughty',\n",
       " '10neptunes',\n",
       " '10off',\n",
       " '10opposite',\n",
       " '10overall',\n",
       " '10paint',\n",
       " '10pc',\n",
       " '10penny',\n",
       " '10pickles',\n",
       " '10pizza',\n",
       " '10plankton',\n",
       " '10pm',\n",
       " '10put',\n",
       " '10razor',\n",
       " '10ripped',\n",
       " '10rock',\n",
       " '10rotten',\n",
       " '10s',\n",
       " '10sandys',\n",
       " '10sb',\n",
       " '10sec',\n",
       " '10seconds',\n",
       " '10sleepy',\n",
       " '10sound',\n",
       " '10squeaky',\n",
       " '10squidward',\n",
       " '10storyline',\n",
       " '10strawberry',\n",
       " '10tea',\n",
       " '10th',\n",
       " '10the',\n",
       " '10this',\n",
       " '10ths',\n",
       " '10too',\n",
       " '10under',\n",
       " '10valentines',\n",
       " '10walking',\n",
       " '10x',\n",
       " '10x10',\n",
       " '10x42',\n",
       " '10year',\n",
       " '10yo',\n",
       " '10your',\n",
       " '10yr',\n",
       " '10yrs',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '110000',\n",
       " '110c',\n",
       " '110th',\n",
       " '111',\n",
       " '1115',\n",
       " '1116',\n",
       " '112',\n",
       " '1120',\n",
       " '1121',\n",
       " '1124200',\n",
       " '113',\n",
       " '1137',\n",
       " '114',\n",
       " '115',\n",
       " '116',\n",
       " '117',\n",
       " '118',\n",
       " '119',\n",
       " '1196044465',\n",
       " '11a',\n",
       " '11awesome',\n",
       " '11b',\n",
       " '11dear',\n",
       " '11g',\n",
       " '11gi',\n",
       " '11it',\n",
       " '11mbps',\n",
       " '11months',\n",
       " '11or',\n",
       " '11pm',\n",
       " '11rate',\n",
       " '11s',\n",
       " '11th',\n",
       " '11well',\n",
       " '11x',\n",
       " '11x14',\n",
       " '11yr',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '1200mg',\n",
       " '1200mm',\n",
       " '120100',\n",
       " '120640',\n",
       " '120a',\n",
       " '120days',\n",
       " '120gb',\n",
       " '120hz',\n",
       " '120lbs',\n",
       " '120mg',\n",
       " '120min',\n",
       " '120s',\n",
       " '120v',\n",
       " '121',\n",
       " '1215',\n",
       " '122',\n",
       " '123',\n",
       " '1234',\n",
       " '124',\n",
       " '125',\n",
       " '125hz',\n",
       " '125lb',\n",
       " '125mbps',\n",
       " '126',\n",
       " '1263331576',\n",
       " '1265',\n",
       " '127',\n",
       " '1273528510',\n",
       " '1275',\n",
       " '128',\n",
       " '1280',\n",
       " '1284',\n",
       " '1284753490',\n",
       " '128m',\n",
       " '128mb',\n",
       " '128mm',\n",
       " '129',\n",
       " '1290798488',\n",
       " '1291690784',\n",
       " '129600',\n",
       " '12ct',\n",
       " '12hours',\n",
       " '12hp',\n",
       " '12inch',\n",
       " '12kb',\n",
       " '12month',\n",
       " '12movie',\n",
       " '12oz',\n",
       " '12p',\n",
       " '12play',\n",
       " '12rate',\n",
       " '12th',\n",
       " '12v',\n",
       " '12years',\n",
       " '12yo',\n",
       " '12yr',\n",
       " '12yrs',\n",
       " '13',\n",
       " '130',\n",
       " '1300',\n",
       " '1300mah',\n",
       " '1300n',\n",
       " '1302',\n",
       " '130920',\n",
       " '130f',\n",
       " '130gb',\n",
       " '130lbs',\n",
       " '1313',\n",
       " '1316',\n",
       " '132',\n",
       " '1321',\n",
       " '1327543257',\n",
       " '1328565109',\n",
       " '1328565145',\n",
       " '133',\n",
       " '13332',\n",
       " '1334877736',\n",
       " '1335',\n",
       " '1337',\n",
       " '135',\n",
       " '1350',\n",
       " '135mm',\n",
       " '136',\n",
       " '137',\n",
       " '1370',\n",
       " '1377',\n",
       " '137gb',\n",
       " '138',\n",
       " '138as',\n",
       " '139',\n",
       " '1394',\n",
       " '13lb',\n",
       " '13mb',\n",
       " '13mi',\n",
       " '13mm',\n",
       " '13months',\n",
       " '13th',\n",
       " '13x',\n",
       " '13yrs',\n",
       " '14',\n",
       " '140',\n",
       " '1400',\n",
       " '1400s',\n",
       " '1404',\n",
       " '140f',\n",
       " '140watt',\n",
       " '142',\n",
       " '1420',\n",
       " '1422',\n",
       " '143',\n",
       " '14383',\n",
       " '144',\n",
       " '1440',\n",
       " '1442133171',\n",
       " '145',\n",
       " '1450mah',\n",
       " '146',\n",
       " '147',\n",
       " '148',\n",
       " '1483',\n",
       " '149',\n",
       " '1490',\n",
       " '1491',\n",
       " '1492',\n",
       " '1493',\n",
       " '14days',\n",
       " '14g',\n",
       " '14ghz',\n",
       " '14hours',\n",
       " '14kt',\n",
       " '14lbs',\n",
       " '14mo',\n",
       " '14oz',\n",
       " '14th',\n",
       " '14yr',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '15000',\n",
       " '1500mah',\n",
       " '1500s',\n",
       " '1500va',\n",
       " '1501',\n",
       " '1502',\n",
       " '1507',\n",
       " '1509',\n",
       " '150fps',\n",
       " '150ft',\n",
       " '150hz',\n",
       " '150lbs',\n",
       " '150s',\n",
       " '150x',\n",
       " '151',\n",
       " '1514',\n",
       " '152',\n",
       " '153',\n",
       " '1530',\n",
       " '154',\n",
       " '155',\n",
       " '156',\n",
       " '1569945233',\n",
       " '157',\n",
       " '1571',\n",
       " '157lbs',\n",
       " '1587200953',\n",
       " '158mm',\n",
       " '159',\n",
       " '159144070x',\n",
       " '1593154461',\n",
       " '1596009411',\n",
       " '15degrees',\n",
       " '15ft',\n",
       " '15k',\n",
       " '15m',\n",
       " '15mins',\n",
       " '15month',\n",
       " '15ppm',\n",
       " '15s',\n",
       " '15th',\n",
       " '15w',\n",
       " '15x',\n",
       " '15yrs',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '1600s',\n",
       " '1609',\n",
       " '160gb',\n",
       " '160lb',\n",
       " '160megs',\n",
       " '160p6s',\n",
       " '160s',\n",
       " '161',\n",
       " '1611',\n",
       " '1611kbps',\n",
       " '1617evspk',\n",
       " '162',\n",
       " '1623',\n",
       " '1623kbps',\n",
       " '1625',\n",
       " '1628',\n",
       " '1632',\n",
       " '1633',\n",
       " '1634',\n",
       " '1640',\n",
       " '1643',\n",
       " '165',\n",
       " '165mb',\n",
       " '166',\n",
       " '1666',\n",
       " '1667',\n",
       " '167',\n",
       " '168',\n",
       " '1680',\n",
       " '1683',\n",
       " '1687',\n",
       " '169',\n",
       " '1697',\n",
       " '1699',\n",
       " '16cm',\n",
       " '16gb',\n",
       " '16k',\n",
       " '16lb',\n",
       " '16mm',\n",
       " '16s',\n",
       " '16th',\n",
       " '16ths',\n",
       " '16volt',\n",
       " '16x',\n",
       " '16x10',\n",
       " '16x9',\n",
       " '16yo',\n",
       " '16yr',\n",
       " '17',\n",
       " '170',\n",
       " '1700',\n",
       " '1700mah',\n",
       " '1700s',\n",
       " '1703',\n",
       " '1706',\n",
       " '171',\n",
       " '1710khz',\n",
       " '1711',\n",
       " '1714',\n",
       " '172',\n",
       " '1720',\n",
       " '1722',\n",
       " '1732',\n",
       " '1733',\n",
       " '17400',\n",
       " '1741',\n",
       " '1744',\n",
       " '1745',\n",
       " '175',\n",
       " '1750',\n",
       " '1753',\n",
       " '1754',\n",
       " '1757',\n",
       " '175lbs',\n",
       " '176',\n",
       " '1760',\n",
       " '1763',\n",
       " '176th',\n",
       " '177',\n",
       " '1773',\n",
       " '1774',\n",
       " '1775',\n",
       " '1776',\n",
       " '178',\n",
       " '1780',\n",
       " '179',\n",
       " '1790',\n",
       " '1792',\n",
       " '1797',\n",
       " '17a',\n",
       " '17degf',\n",
       " '17may2011',\n",
       " '17th',\n",
       " '17yr',\n",
       " '18',\n",
       " '180',\n",
       " '1800',\n",
       " '1800s',\n",
       " '1804',\n",
       " '1805',\n",
       " '180g',\n",
       " '181',\n",
       " '1810',\n",
       " '1812',\n",
       " '1815',\n",
       " '1818',\n",
       " '181cost',\n",
       " '182',\n",
       " '1820',\n",
       " '1822',\n",
       " '1825',\n",
       " '1828',\n",
       " '182s',\n",
       " '183',\n",
       " '1830',\n",
       " '1830s',\n",
       " '1831',\n",
       " '1833',\n",
       " '1840',\n",
       " '1840s',\n",
       " '1843',\n",
       " '1848',\n",
       " '185',\n",
       " '1850',\n",
       " '1850s',\n",
       " '1852',\n",
       " '1853',\n",
       " '1855',\n",
       " '1857',\n",
       " '1859',\n",
       " '1860',\n",
       " '1860s',\n",
       " '1861',\n",
       " '1863',\n",
       " '1864',\n",
       " '1865',\n",
       " '1866',\n",
       " '1867',\n",
       " '1869',\n",
       " '187',\n",
       " '1870',\n",
       " '1872',\n",
       " '1873',\n",
       " '1874',\n",
       " '1875',\n",
       " '1876',\n",
       " '1877',\n",
       " '1878',\n",
       " '1879',\n",
       " '188',\n",
       " '1880',\n",
       " '1882',\n",
       " '1883',\n",
       " '1886',\n",
       " '1887',\n",
       " '1888',\n",
       " '1889',\n",
       " '189',\n",
       " '1890',\n",
       " '1891',\n",
       " '1891andfrontiersmen',\n",
       " '1892',\n",
       " '1893',\n",
       " '1894',\n",
       " '1896',\n",
       " '1897',\n",
       " '1898',\n",
       " '18a1',\n",
       " '18ga',\n",
       " '18gauge',\n",
       " '18kbps',\n",
       " '18lb',\n",
       " '18lbs',\n",
       " '18m',\n",
       " '18oz',\n",
       " '18th',\n",
       " '18v',\n",
       " '19',\n",
       " '190',\n",
       " '1900',\n",
       " '1900s',\n",
       " '1901',\n",
       " '1902',\n",
       " '1903',\n",
       " '1905',\n",
       " '1906',\n",
       " '1907',\n",
       " '1908',\n",
       " '1909',\n",
       " '191',\n",
       " '1910',\n",
       " '1911',\n",
       " '1912',\n",
       " '1913',\n",
       " '1914',\n",
       " '1915',\n",
       " '1916',\n",
       " '1917',\n",
       " '1918',\n",
       " '1919',\n",
       " '192',\n",
       " '1920',\n",
       " '1920s',\n",
       " '1921',\n",
       " '1922',\n",
       " '1923',\n",
       " '1924',\n",
       " '1925',\n",
       " '1926',\n",
       " '1927',\n",
       " '1928',\n",
       " '1929',\n",
       " '192kbps',\n",
       " '192mb',\n",
       " '193',\n",
       " '1930',\n",
       " '1930s',\n",
       " '1931',\n",
       " '1932',\n",
       " '1933',\n",
       " '1934',\n",
       " '1935',\n",
       " '1936',\n",
       " '1937',\n",
       " '1938',\n",
       " '1939',\n",
       " '1940',\n",
       " '1940s',\n",
       " '1941',\n",
       " '1941critical',\n",
       " '1942',\n",
       " '1943',\n",
       " '1944',\n",
       " '1945',\n",
       " '1946',\n",
       " '1947',\n",
       " '1948',\n",
       " '1949',\n",
       " '195',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1951',\n",
       " '1952',\n",
       " '1953',\n",
       " '1953classic',\n",
       " '1954',\n",
       " '1955',\n",
       " '1956',\n",
       " '1957',\n",
       " '1958',\n",
       " '1959',\n",
       " '195min',\n",
       " '196',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1961',\n",
       " '1961the',\n",
       " '1962',\n",
       " '1963',\n",
       " '1964',\n",
       " '1965',\n",
       " '1966',\n",
       " '1966return',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '197',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1973bennet',\n",
       " '1974',\n",
       " '1975',\n",
       " '1975ish',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '198',\n",
       " '1980',\n",
       " '1980and',\n",
       " '1980interview',\n",
       " '1980ish',\n",
       " '1980s',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '19888',\n",
       " '1988vintage',\n",
       " '1989',\n",
       " '199',\n",
       " '1990',\n",
       " '1990ies',\n",
       " '1990s',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1994this',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '1999eternal',\n",
       " '19m',\n",
       " '19pre3',\n",
       " '19th',\n",
       " '1a',\n",
       " '1a10bc',\n",
       " '1am',\n",
       " '1and',\n",
       " '1aspect',\n",
       " '1audio',\n",
       " '1b',\n",
       " '1c5',\n",
       " '1cap',\n",
       " '1cm',\n",
       " '1ds',\n",
       " '1f4t',\n",
       " '1g',\n",
       " '1gb',\n",
       " '1gig',\n",
       " '1grove',\n",
       " '1hour',\n",
       " '1hp',\n",
       " '1hr',\n",
       " '1http',\n",
       " '1i',\n",
       " '1in',\n",
       " '1julia',\n",
       " '1k',\n",
       " '1khz',\n",
       " '1l',\n",
       " '1ld',\n",
       " '1m30',\n",
       " '1mb',\n",
       " '1minute',\n",
       " '1mm',\n",
       " '1mp',\n",
       " '1mpeg',\n",
       " '1qt',\n",
       " '1ratio',\n",
       " '1re',\n",
       " '1rst',\n",
       " '1running',\n",
       " '1s',\n",
       " '1script',\n",
       " '1sec',\n",
       " '1smileycat',\n",
       " '1st',\n",
       " '1star',\n",
       " '1surround',\n",
       " '1tb',\n",
       " '1then',\n",
       " '1vc',\n",
       " '1x',\n",
       " '1x2',\n",
       " '1x3p',\n",
       " '1yr',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '20000',\n",
       " '2000ap',\n",
       " '2000genre',\n",
       " '2000mah',\n",
       " '2000s',\n",
       " '2000w',\n",
       " '2001',\n",
       " '2001i',\n",
       " '2002',\n",
       " '2002any',\n",
       " '2002dvd',\n",
       " '2003',\n",
       " '2003reviewer',\n",
       " '2003this',\n",
       " '2004',\n",
       " '2004a',\n",
       " '2004expedition',\n",
       " '2005',\n",
       " '2005lincolnnavigator',\n",
       " '2005mercurymountaineer',\n",
       " '2005not',\n",
       " '2006',\n",
       " '20061',\n",
       " '2006aviator',\n",
       " '2006delivery',\n",
       " '2006explorer',\n",
       " '2006i',\n",
       " '2006reviewer',\n",
       " '2007',\n",
       " '2008',\n",
       " '2009',\n",
       " '2009one',\n",
       " '200count',\n",
       " '200kbps',\n",
       " '200km',\n",
       " '200lbs',\n",
       " '200mm',\n",
       " '200o',\n",
       " '200p',\n",
       " '200pgs',\n",
       " '200s',\n",
       " '200w',\n",
       " '201',\n",
       " '2010',\n",
       " '2010amazon',\n",
       " '2010better',\n",
       " '2011',\n",
       " '2012',\n",
       " '20124k',\n",
       " '2012the',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " '202',\n",
       " '2020',\n",
       " '2021',\n",
       " '2022',\n",
       " '2025',\n",
       " '203',\n",
       " '2032',\n",
       " '2035',\n",
       " '2036',\n",
       " '204',\n",
       " '2041',\n",
       " '2042',\n",
       " '2049',\n",
       " '205',\n",
       " '206',\n",
       " '2061',\n",
       " '2063',\n",
       " '2064',\n",
       " '2065',\n",
       " '207',\n",
       " '2070n',\n",
       " '208',\n",
       " '2081',\n",
       " '2090s',\n",
       " '20a',\n",
       " '20b',\n",
       " '20c',\n",
       " '20d',\n",
       " '20f',\n",
       " '20for',\n",
       " '20g',\n",
       " '20gb',\n",
       " '20ish',\n",
       " '20k',\n",
       " '20lb',\n",
       " '20lbs',\n",
       " '20mb',\n",
       " '20mins',\n",
       " '20minutes',\n",
       " '20mph',\n",
       " '20q',\n",
       " '20s',\n",
       " '20th',\n",
       " '20x',\n",
       " '21',\n",
       " '210',\n",
       " '2100',\n",
       " '210lbs',\n",
       " '210mb',\n",
       " '210mm',\n",
       " '2112',\n",
       " '2114',\n",
       " '212',\n",
       " '2125',\n",
       " '212f',\n",
       " '214',\n",
       " '217',\n",
       " '218',\n",
       " '219',\n",
       " '21min',\n",
       " '21movie',\n",
       " '21oz',\n",
       " '21st',\n",
       " '22',\n",
       " '220',\n",
       " '2200',\n",
       " '220v',\n",
       " '221',\n",
       " '2212',\n",
       " '221b',\n",
       " '222',\n",
       " '2227',\n",
       " '222ms',\n",
       " '2239',\n",
       " '224',\n",
       " ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try n-gram (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))\n",
    "\n",
    "# [(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 68), \n",
    "#  ('to', 63), ('a', 60), ('in', 44), ('and', 41), ('(', 40)]\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    \n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:40:00.872309Z",
     "start_time": "2018-11-29T04:39:28.722337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '00 00', '00 11', '00 150', '00 2000', '00 24', '00 25', '00 28', '00 3k', '00 40']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# tfidf vectors for documents\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', preprocessor=preprocess,\n",
    "                                   #ngram_range=(1,2),\n",
    "                                   max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:44:08.620969Z",
     "start_time": "2018-11-29T04:44:08.255616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4415454545454545\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "# Instantiate the classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB(alpha=alpha)\n",
    "# Fit to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "# Predict the labels: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "# Compute accuracy: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T04:44:15.616071Z",
     "start_time": "2018-11-29T04:44:11.002262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [(-14.29733286657654, '00 00'), (-14.29733286657654, '00 11'), (-14.29733286657654, '00 150'), (-14.29733286657654, '00 2000'), (-14.29733286657654, '00 25'), (-14.29733286657654, '00 28'), (-14.29733286657654, '00 3k'), (-14.29733286657654, '00 40'), (-14.29733286657654, '00 8x'), (-14.29733286657654, '00 advertised'), (-14.29733286657654, '00 afford'), (-14.29733286657654, '00 alll'), (-14.29733286657654, '00 amazon'), (-14.29733286657654, '00 arrived'), (-14.29733286657654, '00 avoid'), (-14.29733286657654, '00 bad'), (-14.29733286657654, '00 bargain'), (-14.29733286657654, '00 begining'), (-14.29733286657654, '00 best'), (-14.29733286657654, '00 better')]\n",
      "2 [(-9.995747193616925, 'worst'), (-9.991951277482904, 'better'), (-9.980429434025897, 'work'), (-9.938638749341585, 'really'), (-9.92515176688316, 'bought'), (-9.902001689445786, 'dvd'), (-9.854287803334739, 'did'), (-9.755961062851906, 'good'), (-9.750407948471391, 'product'), (-9.690334997035244, 'waste'), (-9.681157758146565, 'bad'), (-9.629599750688817, 'buy'), (-9.568453454235952, 'time'), (-9.54967746362356, 'read'), (-9.478330467175978, 'don'), (-9.470335897226862, 'money'), (-9.39392202554866, 'like'), (-9.360565428409668, 'just'), (-9.066934364025014, 'movie'), (-8.771254718620126, 'book')]\n"
     ]
    }
   ],
   "source": [
    "# inspect model\n",
    "\n",
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other considerations\n",
    "Sentiment Analysis\n",
    "- complex problems sarcasm\n",
    "- difficulty with negation\n",
    "    - ie. I liked it, but it could have been better\n",
    "- separate communities may use the same words differently\n",
    "Language biases\n",
    "- prejudices in text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
