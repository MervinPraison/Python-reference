{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting with XGBoost\n",
    "Gradient boosting is currently one of the most popular techniques for efficient modeling of tabular datasets of all sizes. \n",
    "\n",
    "XGboost is a very fast, scalable implementation of gradient boosting\n",
    "\n",
    "Summary\n",
    "1. Classification with XGBoost\n",
    "2. Regression with XGBoost\n",
    "3. Fine-tuning XGBoost model\n",
    "4. XGBoost in Pipelines\n",
    "\n",
    "Reference: Sergey Fogelson, VP Analytics, Viacom, DataCamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note on adding image\n",
    "<img src=\"image.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification with XGBoost\n",
    "Understand the basics of:\n",
    "- supervised classification\n",
    "- decision trees\n",
    "- boosting\n",
    "\n",
    "Supervised learning\n",
    "- relies on labeled data - some understanding on past behavior\n",
    "- 2 kinds of problems: regression and classificaiton\n",
    "- Classification problems\n",
    "    - outcomes are binary or multi-class\n",
    "    - AUC - metric for binary classification models\n",
    "        - larger area under the ROC curve = better model, more sensitive\n",
    "    - Accuracy score and confusion matrix - metric for multiclass\n",
    "    - common algorithms: logistic regression and decision trees\n",
    "\n",
    "Other supervised learning considerations\n",
    "- require a table of feature vectors\n",
    "- Features can be either numeric or categorical\n",
    "- Numeric features should be scaled (Z-scored)\n",
    "- Categorical features should be encoded (one-hot)\n",
    "- Other problems\n",
    "    - Ranking - predicting an ordering on a set of choices\n",
    "        - ie. Google search\n",
    "    - Recommendation\n",
    "        - Recommending an item to a user\n",
    "        - based on consumption history and profile\n",
    "        - ie. Netflix\n",
    "\n",
    "XGBoost introduction\n",
    "\n",
    "What is XGBoost?\n",
    "- Optimized gradient-boosting ML library\n",
    "- orginally written in C++ command line application\n",
    "- Has APIs in several languages:\n",
    "    - Python, R, Scala, Julia, Java\n",
    "    \n",
    "What makes XGBoost so popular?\n",
    "- speed and performance\n",
    "- core algorithm is parallelizable - GPUs and networks of computers\n",
    "    - feasible to scale to 100s of millions of training examples\n",
    "- the real draw is: consistently outperforms single-algorithm methods\n",
    "    - state of the art performance in many ML tasks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 XGBoost: Classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data\n",
    "class_data = pd.read_csv(\"classification_data.csv\")\n",
    "X, y = class_data.iloc[:,:-1], class_data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                   random_state=123)\n",
    "\n",
    "# instantiate XGBoost classifier\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                          n_estimators=10, seed=123)\n",
    "# fit and predict\n",
    "xg_cl.fit(X_train, y_train)\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# evaluate accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))\n",
    "# 0.743300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 XGBoost: Fit/Predict\n",
    "It's time to create your first XGBoost model! As Sergey showed you in the video, you can use the scikit-learn .fit() / .predict() paradigm that you are already familiar to build your XGBoost models, as the xgboost library has a scikit-learn compatible API!\n",
    "\n",
    "Here, you'll be working with churn data. This dataset contains imaginary data from a ride-sharing app with user behaviors over their first month of app usage in a set of imaginary cities as well as whether they used the service 5 months after sign-up. It has been pre-loaded for you into a DataFrame called churn_data - explore it in the Shell!\n",
    "\n",
    "Your goal is to use the first month's worth of data to predict whether the app's users will remain users of the service at the 5 month mark. This is a typical setup for a churn prediction problem. To do this, you'll split the data into training and test sets, fit a small xgboost model on the training set, and evaluate its performance on the test set by computing its accuracy.\n",
    "\n",
    "pandas and numpy have been imported as pd and np, and train_test_split has been imported from sklearn.model_selection. Additionally, the arrays for the features and the target have been created as X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))\n",
    "\n",
    "# accuracy: 0.743300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 What is a decision tree?\n",
    "Decision trees as base learners\n",
    "- Base learner - individual learning algorithm in an ensemble algorithm\n",
    "    - note: XGB is an ensemble learning method that uses outputs of many models for a final prediction\n",
    "- Composed of a series of binary questions/decisions - y/n, T/F\n",
    "- Predictions happen at the \"leaves\" of the tree\n",
    "\n",
    "Decision trees and CART (=classification and regression trees for ML)\n",
    "- Constructed iteratively (one decision at a time)\n",
    "    - Until a stopping criterion is met\n",
    "- Individual decision trees tend to overfit\n",
    "    - Low Bias and High Variance learning models\n",
    "        - good at learning relationships but tend to overfit, so generalize poorly\n",
    "- CART: Classification and Regression Trees\n",
    "    - XGB uses this\n",
    "    - Each leaf ALWAYS contains a real-valued score for classification or regression\n",
    "    - Can later be converted into categories\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Decision trees\n",
    "Your task in this exercise is to make a simple decision tree using scikit-learn's DecisionTreeClassifier on the breast cancer dataset that comes pre-loaded with scikit-learn.\n",
    "\n",
    "This dataset contains numeric measurements of various dimensions of individual tumors (such as perimeter and texture) from breast biopsies and a single outcome value (the tumor is either malignant, or benign).\n",
    "\n",
    "We've preloaded the dataset of samples (measurements) into X and the target values per tumor into y. Now, you have to split the complete dataset into training and testing sets, and then train a DecisionTreeClassifier. You'll specify a parameter called max_depth. Many other parameters can be modified within this model, and you can check all of them out here (http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "# :param max_depth of 4. This parameter specifies the maximum number \n",
    "#  of successive split points you can have before reaching a leaf node.\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 What is Boosting?\n",
    "Boosting overview\n",
    "- Not a specific machine learning algorithm\n",
    "- concept that can be applied to a set of ML models\n",
    "    - \"Meta-algorithm\"\n",
    "- Ensemble meta-algorithm used to convert many weak learners into a strong learner\n",
    "\n",
    "Weak learners and strong learners\n",
    "- Weak learner = ML algorithm that is slightly better than chance\n",
    "    - Example: decision tree whose predictions slightly better than 50%\n",
    "- Boosting converts a collection of weak learners into a strong learner\n",
    "- Strong learner = any algorithm that can be tuned to achieve good performance\n",
    "\n",
    "How boosting is accomplished?\n",
    "- Iteratively learning a set of weak models on subsets of the data\n",
    "- Weighting each weak prediction according to each weak learner's performance\n",
    "    - final prediction is much better than any individual predictions\n",
    "\n",
    "Model evaluation through cross-validation\n",
    "- Cross-validation = robust method for estimating the performance of a model on unseen data by...\n",
    "- Generating many non-overlapping train/test splits on training data\n",
    "- Reports the average test set performance across all data splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.a Cross-validation in XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "class_data = pd.read_csv(\"classification_data.csv\")\n",
    "\n",
    "# DMatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=churn_data.iloc[:,:-1],\n",
    "                            label=churn_data.month_5_still_here])\n",
    "\n",
    "# parameter dictionary to pass into cross validation\n",
    "params={\"objective\":\"binary:logistic\",\"max_depth\":4}\n",
    "\n",
    "# use cv method and pass required dmatrix\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=4,\n",
    "                   num_boost_round=10, metrics=\"error\", as_pandas=True)\n",
    "\n",
    "# output accuracy\n",
    "print(\"Accuracy: %f\" %((1-cv_results[\"test-error-mean\"]).iloc[-1]))\n",
    "# Accuracy: 0.88315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Measuring accuracy\n",
    "You'll now practice using XGBoost's learning API through its baked in cross-validation capabilities. As Sergey discussed in the previous video, XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a DMatrix.\n",
    "\n",
    "In the previous exercise, the input datasets were converted into DMatrix data on the fly, but when you use the xgboost cv object, you have to first explicitly convert your data into a DMatrix. So, that's what you will do here before running cross-validation on churn_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: churn_dmatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "# \"error\" metrics will be converted to accuracy\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, \n",
    "                    num_boost_round=5, metrics=\"error\", as_pandas=True, \n",
    "                    seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))\n",
    "\n",
    "# output\n",
    "       test-error-mean  test-error-std  train-error-mean  train-error-std\n",
    "    0          0.28378        0.001932           0.28232         0.002366\n",
    "    1          0.27190        0.001932           0.26951         0.001855\n",
    "    2          0.25798        0.003963           0.25605         0.003213\n",
    "    3          0.25434        0.003827           0.25090         0.001845\n",
    "    4          0.24852        0.000934           0.24654         0.001981\n",
    "    0.75148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv_results stores the training and test mean and standard deviation of the error per boosting round (tree built) as a DataFrame. From cv_results, the final round 'test-error-mean' is extracted and converted into an accuracy, where accuracy is 1-error. The final accuracy of around 75% is an improvement from earlier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Measuring AUC\n",
    "Now that you've used cross-validation to compute average out-of-sample accuracy (after converting from an error), it's very easy to compute any other metric you might be interested in. All you have to do is pass it (or a list of metrics) in as an argument to the metrics parameter of xgb.cv().\n",
    "\n",
    "Your job in this exercise is to compute another common metric used in binary classification - the area under the curve (\"auc\"). As before, churn_data is available in your workspace, along with the DMatrix churn_dmatrix and parameter dictionary params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross_validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, \n",
    "                    num_boost_round=5, metrics=\"auc\", as_pandas=True, \n",
    "                    seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])\n",
    "\n",
    "<script.py> output:\n",
    "       test-auc-mean  test-auc-std  train-auc-mean  train-auc-std\n",
    "    0       0.767863      0.002820        0.768893       0.001544\n",
    "    1       0.789157      0.006846        0.790864       0.006758\n",
    "    2       0.814476      0.005997        0.815872       0.003900\n",
    "    3       0.821682      0.003912        0.822959       0.002018\n",
    "    4       0.826191      0.001937        0.827528       0.000769\n",
    "    0.826191\n",
    "\n",
    "# An AUC of 0.84 is quite strong. As you have seen, XGBoost's \n",
    "# learning API makes it very easy to compute any metric you may be \n",
    "# interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 When should I use XGBoost?\n",
    "When to use XGBoost? Criteria:\n",
    "- large number of training samples\n",
    "    - > 1000 training samples and < 100 features\n",
    "    - should be ok when number of features < number of training samples\n",
    "- you have mixture of categorical and numeric features\n",
    "- Or just numeric features\n",
    "\n",
    "When to NOT use XGBoost? Criteria:\n",
    "- had success with other algorithms\n",
    "    - Not suited for: (Deep learning is better)\n",
    "        - image recognition\n",
    "        - computer vision\n",
    "        - NLP and NL understanding problems\n",
    "- dataset size issues\n",
    "    - < 100 training samples\n",
    "    - when number of training samples is significantly smaller than the number of features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Using XGBoost\n",
    "XGBoost is a powerful library that scales very well to many samples and works for a variety of supervised learning problems. That said, as Sergey described in the video, you shouldn't always pick it as your default machine learning library when starting a new project, since there are some situations in which it is not the best option. In this exercise, your job is to consider the below examples and select the one which would be the best use of XGBoost.\n",
    "\n",
    "Possible Answers\n",
    "- Visualizing the similarity between stocks by comparing the time series of their historical prices relative to each other.\n",
    "- Predicting whether a person will develop cancer using genetic data with millions of genes, 23 examples of genomes of people that didn't develop cancer, 3 genomes of people who wound up getting cancer.\n",
    "- Clustering documents into topics based on the terms used in them.\n",
    "- Predicting the likelihood that a given user will click an ad from a very large clickstream log with millions of users and their web interactions.\n",
    "\n",
    "Answer:\n",
    "- D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression with XGBoost\n",
    "Regression review\n",
    "- Outcome is real-valued - ie. height\n",
    "\n",
    "Common regression metrics\n",
    "- Root mean squared error (RMSE)\n",
    "    - Total Squared Error = compute difference b/n Actual and Predicted \n",
    "    - Mean squared error - take mean\n",
    "    - Root Mean Squared error - take square root\n",
    "        - allows us treat negative and positive errors equally\n",
    "        - but tends to punish larger differences b/n Actual and Predicted\n",
    "- Mean absolute error (MAE)\n",
    "    - Total Absolute Error\n",
    "    - Mean Absolute Error\n",
    "    - isn't affected by large differences like RMSE\n",
    "    - but not frequently used b/c it lacks some mathematical properties\n",
    "\n",
    "Common regression algorithms\n",
    "- linear regression\n",
    "- Decision trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which of these is a regression problem?\n",
    "Here are 4 potential machine learning problems you might encounter in the wild. Pick the one that is a clear example of a regression problem.\n",
    "\n",
    "Possible Answers\n",
    "- Recommending a restaurant to a user given their past history of restaurant visits and reviews for a dining aggregator app.\n",
    "- Predicting which of several thousand diseases a given person is most likely to have given their symptoms.\n",
    "- Tagging an email as spam/not spam based on its content and metadata (sender, time sent, etc.).\n",
    "- Predicting the expected payout of an auto insurance claim given claim properties (car, accident type, driver prior history, etc.).\n",
    "\n",
    "Answer: D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Objective (loss) functions and base learners\n",
    "Objective functions and why we use them\n",
    "- Loss functions quantifies how far off a prediction is from the actual result\n",
    "- Measures the difference b/n estimated and true values for some collection of data\n",
    "- Goal: Find the model that yields the minimum value of the loss function\n",
    "\n",
    "Common Loss Functions in XGBoost\n",
    "- reg:linear - use for regression problems\n",
    "- reg:logistic - use for classification problems when you want just decision, not probability\n",
    "- binary:logistic - use when you want probability rather than just decision\n",
    "\n",
    "Base Learners and why we need them\n",
    "- XGBoost involves creating a meta-model (ensemble learning method) that is composed of many individual models that combine to give a final prediction\n",
    "- Individual models = Base Learners\n",
    "- Want base learners that when combined create final prediction that is non-linear\n",
    "- Each base learner should be good at distinguishing or predicting different parts of the dataset\n",
    "    - goal of XGB is to have base learners that are slightly better than random guessing on certain subsets of training examples and uniformly bad on the remainder\n",
    "    - when predictions are all combined, the uniformly bad predictions cancel out, and those slightly better than chance predictions combine into a very good prediction\n",
    "- 2 kinds of base learners: tree and linear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.a Trees as Base Learners example: Scikit-learn API\n",
    "- Bosting housing dataset from UIC repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "X, y, = boston_data.iloc[:,:-1],boston_data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=123)\n",
    "# regression, note objective parameter\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10,\n",
    "                          seed=123)\n",
    "xg_reg.fit(X_train, y_train)\n",
    "preds = xg_reg.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.b Linear Base Learners example: Learning API Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "boston_data = pd.read_csv(\"boston_housing.csv\")\n",
    "X, y, = boston_data.iloc[:,:-1],boston_data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=123)\n",
    "# note: difference here\n",
    "# Learning API requires DMatrix\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "DM_test = xgb.DMatrix(data=X_test, label=y_test)\n",
    "params = {\"booster\":\"gblinear\",\"objective\":\"reg:linear\"}\n",
    "xg_reg = xgb.train(params=params, dtrain=DM_train, num_boost_round=10)\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# same here\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.a Decision trees as base learners\n",
    "It's now time to build an XGBoost model to predict house prices - not in Boston, Massachusetts, as you saw in the video, but in Ames, Iowa! This dataset of housing prices has been pre-loaded into a DataFrame called df. If you explore it in the Shell, you'll see that there are a variety of features about the house and its location in the city.\n",
    "\n",
    "In this exercise, your goal is to use trees as base learners. By default, XGBoost uses trees as base learners, so you don't have to specify that you want to use trees here with booster=\"gbtree\".\n",
    "\n",
    "xgboost has been imported as xgb and the arrays for the features and the target are available in X and y, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=123)\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "# :param n_estimators = 10, specifies 10 trees\n",
    "# Note: You don't have to specify booster=\"gbtree\" as this is the default.\n",
    "xg_reg = xgb.XGBRegressor(objective=\"reg:linear\", n_estimators=10)\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "\n",
    "# RMSE: 78847.401758"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.b Linear base learners\n",
    "Now that you've used trees as base models in XGBoost, let's use the other kind of base model that can be used with XGBoost - a linear learner. This model, although not as commonly used in XGBoost, allows you to create a regularized linear regression using XGBoost's powerful learning API. However, because it's uncommon, you have to use XGBoost's own non-scikit-learn compatible functions to build the model, such as xgb.train().\n",
    "\n",
    "In order to do this you must create the parameter dictionary that describes the kind of booster you want to use (similarly to how you created the dictionary in Chapter 1 (https://campus.datacamp.com/courses/extreme-gradient-boosting-with-xgboost/classification-with-xgboost?ex=9) when you used xgb.cv()). The key-value pair that defines the booster type (base model) you need is \"booster\":\"gblinear\".\n",
    "\n",
    "Once you've created the model, you can use the .train() and .predict() methods of the model just like you've done in the past.\n",
    "\n",
    "Here, the data has already been split into training and testing sets, so you can dive right into creating the DMatrix objects required by the XGBoost learning API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "DM_test =  xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params = params, \n",
    "                   dtrain=DM_train, \n",
    "                   num_boost_round =5)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "\n",
    "# RMSE: 43965.314324\n",
    "# Looks like the linear base learners performed better than the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluating model quality\n",
    "It's now time to begin evaluating model quality.\n",
    "\n",
    "Here, you will compare the RMSE and MAE of a cross-validated XGBoost model on the Ames housing data. As in previous exercises, all necessary modules have been pre-loaded and the data is available in the DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, \n",
    "                    num_boost_round=5, metrics=\"rmse\", as_pandas=True, \n",
    "                    seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"RMSE\"]).tail(1))\n",
    "\n",
    "   test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std\n",
    "0   142980.433594    1193.791602    141767.531250      429.454591\n",
    "1   104891.394532    1223.158855    102832.544922      322.469930\n",
    "2    79478.937500    1601.344539     75872.615235      266.475960\n",
    "3    62411.920899    2220.150028     57245.652343      273.625086\n",
    "4    51348.279297    2963.377719     44401.298828      316.423666\n",
    "4    51348.279297\n",
    "Name: test-rmse-mean, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now adapt code to compute the \"mae\" instead of the \"rmse\"\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, \n",
    "                    num_boost_round=5, metrics=\"mae\", as_pandas=True, \n",
    "                    seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-mae-mean\"]).tail(1))\n",
    "\n",
    "   test-mae-mean  test-mae-std  train-mae-mean  train-mae-std\n",
    "0  127634.000000   2404.009898   127343.482421     668.308109\n",
    "1   90122.501953   2107.912810    89770.056641     456.965267\n",
    "2   64278.558594   1887.567576    63580.791016     263.404950\n",
    "3   46819.168945   1459.818607    45633.155274     151.883420\n",
    "4   35670.646484   1140.607452    33587.090820      86.999396\n",
    "4    35670.646484\n",
    "Name: test-mae-mean, dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Regularization and base learners in XGBoost\n",
    "- Regularization is a control on model complexity\n",
    "    - penalize models as they get more complex\n",
    "- Want models that are both accurate and as simple as possible\n",
    "- Regularization parameters in XGBoost:\n",
    "    - gamma - minimum loss reduction allowed for a split to occur\n",
    "        - higher values lead to fewer splits\n",
    "    - alpha - l1 regularization on leaf weights, larger values mean more regularization (which means many leaf weights to go to 0)\n",
    "    - lambda - l2 regularization on leaf weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.a L1 Regularization in XGBoost example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "boston_data = pd.read_csv(\"boston_data.csv\")\n",
    "X,y = boston_data.iloc[:,:-1],boston_data.iloc[:,-1]\n",
    "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "params={\"objective\":\"reg:linear\",\"max_depth\"=4}\n",
    "l1_params = [1,10,1000]\n",
    "# empty list to store rmse values\n",
    "rmses_l1 = []\n",
    "\n",
    "for reg in l1_params:\n",
    "    params[\"alpha\"] = reg\n",
    "    cv_results = xgb.cv(dtrain=boston_dmatrix, params=params,nfold=4,\n",
    "                       num_boost_round=10,metrics=\"rmse\",\n",
    "                       as_pandas=True, seeds=123)\n",
    "    rmses_l1.append(cv_results[\"test-rmse-mean\"] \\\n",
    "                    .tail(1).values[0])\n",
    "    \n",
    "print(\"Best rmse as a function of l1:\")\n",
    "print(pd.DataFrame(list(zip(11_params,rmses_11)), \n",
    "                   columns=[\"l1\", \"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.b Base Learners in XGBoost - 2 types\n",
    "- Linear Base Learner:\n",
    "    - Sum of linear terms\n",
    "        - Like linear/logistic regression model\n",
    "    - Boosted model is weighted sum of linear models (thus is itself linear)\n",
    "    - Rarely used - since you don't get any non-linear combo of features\n",
    "        - can get identical performance from a regularized linear model\n",
    "- Tree Base Learner:\n",
    "    - Decision Tree\n",
    "    - Boosted model is weighted sum of decision trees (nonlinear)\n",
    "    - Almost exclusively used in XGBoost\n",
    "    \n",
    "Creating DataFrames from muiltiple equal-length lists\n",
    "- pd.DataFrame(list(zip(list1,list2)),columns=[\"list1\",\"list2\"]))\n",
    "- zip creates a generator of parallel values:\n",
    "    - zip([1,2,3],['a','b','c']) = [1,'a'],[2,'b'],[3,'c']\n",
    "    - generators need to be completely instantiated before they can be used in DataFrame objects\n",
    "- list() instantiates the full generator and passing that into the DataFrame converts the whole expression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrames from muiltiple equal-length lists\n",
    "pd.DataFrame(list(zip(list1,list2)),columns=[\"list1\",\"list2\"]))\n",
    "\n",
    "# example of zip\n",
    "zip([1,2,3],['a','b','c']) = [1,'a'],[2,'b'],[3,'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.a Using regularization in XGBoost\n",
    "Having seen an example of l1 regularization in the video, you'll now vary the l2 regularization penalty - also known as \"lambda\" - and see its effect on overall model performance on the Ames housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "reg_params = [1, 10, 100]\n",
    "\n",
    "# Create the initial parameter dictionary for varying l2 strength: params\n",
    "params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create an empty list for storing rmses as a function of l2 complexity\n",
    "rmses_l2 = []\n",
    "\n",
    "# Iterate over reg_params\n",
    "for reg in reg_params:\n",
    "\n",
    "    # Update l2 strength\n",
    "    params[\"lambda\"] = reg\n",
    "    \n",
    "    # Pass this updated param dictionary into cv\n",
    "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, \n",
    "                             params=params, \n",
    "                             nfold=2, \n",
    "                             num_boost_round=5, \n",
    "                             metrics=\"rmse\", \n",
    "                             as_pandas=True, \n",
    "                             seed=123)\n",
    "    \n",
    "    # Append best rmse (final round) to rmses_l2\n",
    "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
    "\n",
    "# Look at best rmse per l2 param\n",
    "print(\"Best rmse as a function of l2:\")\n",
    "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\", \"rmse\"]))\n",
    "\n",
    "<script.py> output:\n",
    "    Best rmse as a function of l2:\n",
    "        l2          rmse\n",
    "    0    1  52275.357421\n",
    "    1   10  57746.064453\n",
    "    2  100  76624.625000\n",
    "    \n",
    "# looks like as lambda increases, rmse increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Visualizing individual XGBoost trees - plot_tree()\n",
    "Now that you've used XGBoost to both build and evaluate regression as well as classification models, you should get a handle on how to visually explore your models. Here, you will visualize individual trees from the fully boosted model that XGBoost creates using the entire housing dataset.\n",
    "\n",
    "XGBoost has a plot_tree() function that makes this type of visualization easy. Once you train a model using the XGBoost learning API, you can pass it to the plot_tree() function along with the number of trees you want to plot using the num_trees argument.\n",
    "\n",
    "Plot the first tree using xgb.plot_tree(). It takes in two arguments - the model (in this case, xg_reg, which is trained), and num_trees, which is 0-indexed. So to plot the first tree, specify num_trees=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":2}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the first tree\n",
    "xgb.plot_tree(xg_reg, num_trees=0)\n",
    "plt.show()\n",
    "\n",
    "# Plot the fifth tree\n",
    "xgb.plot_tree(xg_reg, num_trees=4)\n",
    "plt.show()\n",
    "\n",
    "# Plot the last tree sideways\n",
    "xgb.plot_tree(xg_reg, num_trees=4, rankdir=\"LR\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at each of the plots. They provide insight into how the model arrived at its final decisions and what splits it made to arrive at those decisions. This allows us to identify which features are the most important in determining house price. In the next exercise, you'll learn another way of visualizing feature importances.\n",
    "\n",
    "<img src=\"images/xgb1.png\" width=\"500\" />\n",
    "<img src=\"images/xgb2.png\" width=\"500\" />\n",
    "<img src=\"images/xgb3.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Visualizing feature importances: What features are most important in my dataset - plot_importance()\n",
    "Another way to visualize your XGBoost models is to examine the importance of each feature column in the original dataset within the model.\n",
    "\n",
    "One simple way of doing this involves counting the number of times each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost has a plot_importance() function that allows you to do exactly this, and you'll get a chance to use it in this exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the feature importances\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like GrLivArea is the most important feature.\n",
    "\n",
    "<img src=\"images/xgb4.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning your XGBoost model\n",
    "Why tune your model?\n",
    "- Untuned Model vs Tuned Model Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untuned Model Example\n",
    "import pandas as pd\n",
    "housing_data = pd.read_csv(\"ames_housing_trimmed_processed.csv\")\n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]],\n",
    "      housing_data[housing_data.columns.tolist()[-1]]\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# most basic parameters for regression\n",
    "untuned_params={\"objective\":\"reg:linear\"}\n",
    "\n",
    "untuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, \n",
    "                                 params=untuned_params,nfold=4,\n",
    "                                 metrics=\"rmse\",\n",
    "                                 as_pandas=True, seeds=123)\n",
    "    \n",
    "print(\"Untuned rmse: %f\" %((untuned_cv_results_rmse[\"test-rmse-mean\"]) \\\n",
    "                          .tail(1)))\n",
    "# code got cut off on the end of the line\n",
    "\n",
    "# Untuned rmse: 34624.229980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned Model Example\n",
    "import pandas as pd\n",
    "housing_data = pd.read_csv(\"ames_housing_trimmed_processed.csv\")\n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]],\n",
    "      housing_data[housing_data.columns.tolist()[-1]]\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# tuned parameters - few xgb parameters that are important\n",
    "tuned_params={\"objective\":\"reg:linear\", 'colsample_bytree':0.3,\n",
    "             'learning_rate':0.1, 'max_depth':5}\n",
    "\n",
    "# cross validation with 200 constructed trees\n",
    "tuned_cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, \n",
    "                               params=tuned_params,nfold=4,\n",
    "                               num_boost_round=200,\n",
    "                               metrics=\"rmse\",\n",
    "                               as_pandas=True, seeds=123)\n",
    "    \n",
    "print(\"Tuned rmse: %f\" %((tuned_cv_results_rmse[\"test-rmse-mean\"]) \\\n",
    "                         .tail(1)))\n",
    "\n",
    "# Tuned rmse: 29812.683594\n",
    "# about 14% reduction in rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 When is tuning your model a bad idea?\n",
    "Now that you've seen the effect that tuning has on the overall performance of your XGBoost model, let's turn the question on its head and see if you can figure out when tuning your model might not be the best idea. Given that model tuning can be time-intensive and complicated, which of the following scenarios would NOT call for careful tuning of your model?\n",
    "\n",
    "Possible Answers\n",
    "- You have lots of examples from some dataset and very many features at your disposal.\n",
    "- You are very short on time before you must push an initial model to production and have little data to train your model on.\n",
    "- You have access to a multi-core (64 cores) server with lots of memory (200GB RAM) and no time constraints.\n",
    "- You must squeeze out every last bit of performance out of your xgboost model.\n",
    "\n",
    "Answer: B - short on time, need initial model, little data\n",
    "\n",
    "Need time to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tuning the number of boosting rounds - # of trees\n",
    "- this example attempts to cherry pick the best possible number of boosting rounds\n",
    "\n",
    "Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You'll use xgb.cv() inside a for loop and build one model per num_boost_round parameter.\n",
    "\n",
    "Here, you'll continue working with the Ames housing dataset. The features are available in the array X, and the target vector is contained in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, \n",
    "                        num_boost_round=curr_num_rounds, metrics=\"rmse\", \n",
    "                        as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail() \\\n",
    "                                .values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,\n",
    "                   columns=[\"num_boosting_rounds\", \"rmse\"]))\n",
    "\n",
    "# output\n",
    "   num_boosting_rounds          rmse\n",
    "0                    5  50903.299479\n",
    "1                   10  34774.194010\n",
    "2                   15  32895.098958\n",
    "\n",
    "# increasing the number of boosting rounds, decreases the RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Automated boosting round selection using early_stopping\n",
    "Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within xgb.cv(). This is done using a technique called early stopping.\n",
    "\n",
    "Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds. Here you will use the early_stopping_rounds parameter in xgb.cv() with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when num_boosting_rounds is reached, then early stopping does not occur.\n",
    "\n",
    "Here, the DMatrix and parameter dictionary have been created for you. Your task is to use cross-validation with early stopping. Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix,params=params,\n",
    "                    early_stopping_rounds=10, num_boost_round=50, \n",
    "                    metrics=\"rmse\",as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std\n",
    "0    142640.656250     705.559400    141871.630208      403.632626\n",
    "1    104907.664063     111.113862    103057.036458       73.769561\n",
    "2     79262.059895     563.766991     75975.966146      253.726099\n",
    "3     61620.136719    1087.694282     57420.529948      521.658354\n",
    "4     50437.562500    1846.448017     44552.955729      544.170190\n",
    "5     43035.658854    2034.471024     35763.949219      681.798925\n",
    "6     38600.880208    2169.796232     29861.464844      769.571318\n",
    "7     36071.817708    2109.795430     25994.675781      756.521419\n",
    "8     34383.184896    1934.546688     23306.836588      759.238254\n",
    "9     33509.139974    1887.375633     21459.770833      745.624404\n",
    "10    32916.805990    1850.893363     20148.721354      749.612769\n",
    "11    32197.832682    1734.456935     19215.382813      641.387376\n",
    "12    31770.852865    1802.155484     18627.389323      716.256596\n",
    "13    31482.782552    1779.123767     17960.695312      557.043568\n",
    "14    31389.990234    1892.319927     17559.736979      631.412969\n",
    "15    31302.883464    1955.166046     17205.712891      590.171393\n",
    "16    31234.058594    1880.705796     16876.571940      703.631755\n",
    "17    31318.347656    1828.860164     16597.662110      703.677609\n",
    "18    31323.634766    1775.909567     16330.460937      607.274494\n",
    "19    31204.135417    1739.076156     16005.972982      520.470911\n",
    "20    31089.863932    1756.022575     15814.300781      518.604760\n",
    "21    31047.998047    1624.672407     15493.405924      505.616658\n",
    "22    31056.916667    1668.043013     15270.734375      502.018453\n",
    "23    31024.983724    1548.985354     15086.382162      503.913199\n",
    "24    30983.685547    1663.130510     14917.608399      486.206187\n",
    "25    30989.477214    1686.668050     14709.589518      449.668010\n",
    "26    30952.113932    1613.172643     14457.286133      376.787666\n",
    "27    31066.902344    1648.534310     14185.567057      383.102691\n",
    "28    31095.642578    1709.225327     13934.066732      473.465449\n",
    "29    31103.887370    1778.880069     13749.644857      473.670886\n",
    "30    30976.084635    1744.514164     13549.836589      454.898834\n",
    "31    30938.469401    1746.052597     13413.484700      399.603323\n",
    "32    30931.000000    1772.469510     13275.915364      415.408340\n",
    "33    30929.056641    1765.541578     13085.878255      493.792778\n",
    "34    30890.629557    1786.510976     12947.181315      517.790039\n",
    "35    30884.493490    1769.729143     12846.027344      547.732372\n",
    "36    30833.542318    1691.001567     12702.378581      505.523315\n",
    "37    30856.688151    1771.445059     12532.243815      508.298162\n",
    "38    30818.016927    1782.784630     12384.055013      536.225042\n",
    "39    30839.392578    1847.327022     12198.443359      545.165562\n",
    "40    30776.964844    1912.781000     12054.583659      508.841772\n",
    "41    30794.702474    1919.674832     11897.036458      477.177568\n",
    "42    30780.956380    1906.820987     11756.221354      502.992395\n",
    "43    30783.754557    1951.259784     11618.846680      519.837469\n",
    "44    30776.731120    1953.447693     11484.080404      578.428621\n",
    "45    30758.543620    1947.454953     11356.552734      565.368794\n",
    "46    30729.971354    1985.698867     11193.557943      552.299272\n",
    "47    30732.662760    1966.997355     11071.315755      604.090310\n",
    "48    30712.241536    1957.751573     10950.778320      574.862779\n",
    "49    30720.854167    1950.511057     10824.865560      576.665674"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Overview of XGBoost's hyperparameters\n",
    "Tunable parameters in XGBoost\n",
    "- Note significantly different for each base learner\n",
    "\n",
    "Common Tree tunable parameters (most frequently used)\n",
    "- learning rate: learning rate/eta\n",
    "- gamma: min loss reduction to cretae new tree split\n",
    "- lambda: L2 reg on leaf weights\n",
    "- alpha: L1 reg on leaf weights\n",
    "- max_depth: max depth per tree, must be positive integer\n",
    "- subsample: % samples used per tree, value b/n 0 and 1\n",
    "    - low value >> fraction of training data used per boosting round would be low >> possible underfitting\n",
    "    - high value >> may lead to overfitting\n",
    "- colsample_bytree: % features used per tree, value b/n 0 and 1\n",
    "    - large value >> almost all features used\n",
    "    - small value >> small subset of features\n",
    "    - generally, smaller value can provide additional regularization\n",
    "    - large value may overfit\n",
    "\n",
    "Linear tunable parameters\n",
    "- lambda: L2 reg on weights\n",
    "- alpha: L1 reg on weights\n",
    "- lambda_bias: L2 reg term on bias\n",
    "\n",
    "Note: You can also tune the number of estimators used for both base model types (Tree and Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.a Tuning eta - aka 'learning rate'\n",
    "It's time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You'll begin by tuning the \"eta\", also known as the learning rate.\n",
    "\n",
    "The learning rate in XGBoost is a parameter that can range between 0 and 1, \n",
    "- with higher values of \"eta\" penalizing feature weights more strongly, causing much stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse \n",
    "# per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix,params=params,\n",
    "    nfold=3, early_stopping_rounds=5, num_boost_round=10, \n",
    "    metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), \n",
    "                   columns=[\"eta\",\"best_rmse\"]))\n",
    "\n",
    "# output\n",
    "     eta      best_rmse\n",
    "0  0.001  195736.406250\n",
    "1  0.010  179932.182292\n",
    "2  0.100   79759.411458"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.b Tuning max_depth\n",
    "In this exercise, your job is to tune max_depth, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:linear\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2,5,10,20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix,params=params,nfold=2,\n",
    "                        early_stopping_rounds=5,num_boost_round=10,\n",
    "                        metrics=\"rmse\",seed=123)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),\n",
    "                   columns=[\"max_depth\",\"best_rmse\"]))\n",
    "\n",
    "# output\n",
    "   max_depth     best_rmse\n",
    "0          2  37957.468750\n",
    "1          5  35596.599610\n",
    "2         10  36065.546875\n",
    "3         20  36739.576172\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.c Tuning colsample_bytree - fraction of features, value 0-1\n",
    "Now, it's time to tune \"colsample_bytree\". You've already seen this if you've ever worked with scikit-learn's RandomForestClassifier or RandomForestRegressor, where it just was called max_features. In both xgboost and sklearn, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, colsample_bytree must be specified as a float between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values: colsample_bytree_vals\n",
    "colsample_bytree_vals = [0.1,0.5,0.8,1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params['colsample_bytree'] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), \n",
    "                   columns=[\"colsample_bytree\",\"best_rmse\"]))\n",
    "\n",
    "# output\n",
    "   colsample_bytree     best_rmse\n",
    "0               0.1  45017.404296\n",
    "1               0.5  36050.654297\n",
    "2               0.8  35372.572266\n",
    "3               1.0  35836.046875\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other parameters to tune...\n",
    "There are several other individual parameters that you can tune, such as \"subsample\", which dictates the fraction of the training data that is used during any given boosting round. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Review of Grid Search and Random Search\n",
    "- find optimal hyperparameters simultaneously to get lowest loss possible\n",
    "\n",
    "2 strategies: Grid Search and Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.a Grid Search - Review\n",
    "- Search exhaustively over a given set of hyperparameters, once per set of hyperparameters\n",
    "- Number of models = number of distinct values per hyperparameter multiplied across each hyperparameter\n",
    "    - ie. 2 hyperparameters and 4 values per each hyperparameter, would try 16 combos\n",
    "- Pick final model hyperparameter values that give best cross-validated evaluation metric value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.b Grid Search Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [1]: import pandas as pd\n",
    "In [2]: import xgboost as xgb\n",
    "In [3]: import numpy as np\n",
    "In [4]: from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "In [5]: housing_data = pd.read_csv(\"ames_housing_trimmed_processed.csv\")\n",
    "In [6]: X, y = housing_data[housing_data.columns.tolist()[:-1]],\n",
    "   ...: housing_data[housing_data.columns.tolist()[-1]\n",
    "In [7]: housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "                     \n",
    "# 4 learning rate/eta values, 1 number of trees, 3 subsamples = 12 models\n",
    "In [8]: gbm_param_grid = {\n",
    "   ...: 'learning_rate': [0.01,0.1,0.5,0.9],\n",
    "   ...: 'n_estimators': [200],\n",
    "   ...: 'subsample': [0.3, 0.5, 0.9]}\n",
    "                     \n",
    "In [9]: gbm = xgb.XGBRegressor()\n",
    "In [10]: grid_mse = GridSearchCV(estimator=gbm,\n",
    "    ...: param_grid=gbm_param_grid, \n",
    "    ...: scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "In [11]: grid_mse.fit(X, y)\n",
    "                     \n",
    "In [12]: print(\"Best parameters found: \",grid_mse.best_params_)\n",
    "Best parameters found: {'learning_rate': 0.1, \n",
    "'n_estimators': 200, 'subsample': 0.5}\n",
    "In [13]: print(\"Lowest RMSE found: \", \n",
    "               np.sqrt(np.abs(grid_mse.best_score_)))\n",
    "Lowest RMSE found:  28530.1829341"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.c Random Search - Review\n",
    "- Create a (possibly infinite) range of hyperparameter values per hyperparameter that you would like to search over\n",
    "- Set the number of iterations you would like for the random search to continue\n",
    "- During each iteration, randomly draw a value in the range of specified values for each hyperparameter searched over and train/evaluate a model with those hyperparameters\n",
    "- After you've reached the maximum number of iterations, select the hyperparameter configuration with the best evaluated score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.d Random Search Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [1]: import pandas as pd\n",
    "In [2]: import xgboost as xgb\n",
    "In [3]: import numpy as np\n",
    "In [4]: from sklearn.model_selection import RandomizedSearchCV\n",
    "In [5]: housing_data = pd.read_csv(\"ames_housing_trimmed_processed.csv\")\n",
    "In [6]: X,y = housing_data[housing_data.columns.tolist()[:-1]],\n",
    "   ...: housing_data[housing_data.columns.tolist()[-1]]\n",
    "In [7]: housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "    \n",
    "# 20 eta/learning rate, 20 values for subsample = 400 models to try\n",
    "In [8]: gbm_param_grid = {\n",
    "   ...: 'learning_rate': np.arange(0.05,1.05,.05),\n",
    "   ...: 'n_estimators': [200],\n",
    "   ...: 'subsample': np.arange(0.05,1.05,.05)}\n",
    "    \n",
    "# try 25 random combos\n",
    "In [9]: gbm = xgb.XGBRegressor()\n",
    "In [10]: randomized_mse = RandomizedSearchCV(estimator=gbm,\n",
    "    ...: param_distributions=gbm_param_grid, n_iter=25,\n",
    "    ...: scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "In [11]: randomized_mse.fit(X, y)\n",
    "\n",
    "In [12]: print(\"Best parameters found: \",randomized_mse.best_params_)\n",
    "Best parameters found: {'subsample': 0.60000000000000009,\n",
    "'n_estimators': 200, 'learning_rate': 0.20000000000000001}\n",
    "In [13]: print(\"Lowest RMSE found: \",\n",
    "    ...: np.sqrt(np.abs(randomized_mse.best_score_)))\n",
    "Lowest RMSE found: 28300.2374291\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.d Practice - Grid Search with XGBoost\n",
    "Now that you've learned how to tune parameters individually with XGBoost, let's take your parameter tuning to the next level by using scikit-learn's GridSearch and RandomizedSearch capabilities with internal cross-validation using the GridSearchCV and RandomizedSearchCV functions. You will use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously. Let's get to work, starting with GridSearchCV! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
    "                        scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n",
    "\n",
    "# output\n",
    "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n",
    "Best parameters found:  {'n_estimators': 50, 'max_depth': 5,\n",
    "                         'colsample_bytree': 0.7}\n",
    "Lowest RMSE found:  30342.16964561695"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.e Practice - Random Search with XGBoost\n",
    "Often, GridSearchCV can be really time consuming, so in practice, you may want to use RandomizedSearchCV instead, as you will do in this exercise. The good news is you only have to make a few modifications to your GridSearchCV code to do RandomizedSearchCV. The key difference is you have to specify a param_distributions parameter instead of a param_grid parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid: gbm_param_grid \n",
    "# range(2,12) - give values b/n 2 and 11\n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(param_distributions=gbm_param_grid, \n",
    "                                    estimator=gbm, \n",
    "                                    scoring='neg_mean_squared_error', \n",
    "                                    n_iter=5, cv=4, verbose=1)\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))\n",
    "\n",
    "# output\n",
    "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
    "Best parameters found:  {'max_depth': 5, 'n_estimators': 25}\n",
    "Lowest RMSE found:  36636.35808132903"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Limits of Grid Search and Random Search\n",
    "Grid Search\n",
    "    - if hyperparameter grid is small, you'll get an answer in a reasonable amount of time\n",
    "    - Number of models you must build with every additional new parameter grows very quickly\n",
    "    \n",
    "Random Search\n",
    "- problem: as you add new hyperparameters, hyperparameter space to explore can be massive\n",
    "- Randomly jumping throughout the space looking for a \"best\" result becomes a waiting game\n",
    "\n",
    "Both approaches have significant limitations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 When Should you Use Grid Search and Random Search?\n",
    "Now that you've seen some of the drawbacks of grid search and random search, which of the following most accurately describes why both random search and grid search are non-ideal search hyperparameter tuning strategies in all scenarios?\n",
    "\n",
    "Possible Answers\n",
    "- Grid Search and Random Search both take a very long time to perform, regardless of the number of parameters you want to tune.\n",
    "- Grid Search and Random Search both scale exponentially in the number of hyperparameters you want to tune.\n",
    "- The search space size can be massive for Grid Search in certain cases, whereas for Random Search the number of hyperparameters has a significant effect on how long it takes to run.\n",
    "- Grid Search and Random Search require that you have some idea of where the ideal values for hyperparameters reside.\n",
    "\n",
    "Answer: C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost in Pipelines\n",
    "- incorporate models into 2 end-to-end ML pipelines\n",
    "- tune most important XGBoost hyperparameters in a pipeline\n",
    "- more advanced preprocessing techniques\n",
    "\n",
    "Pipelines Review using sklearn\n",
    "- Takes a list of named 2-tuples (name, pipeline_step) as input\n",
    "- Tuples can contain any arbitrary scikit-learn compatible estimator or transformer object\n",
    "- Pipeline implements fit/predict methods\n",
    "- Can be used as input estimator into grid/randomized search and cross_val_score methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn pipeline example\n",
    "In [1]: import pandas as pd\n",
    "   ...: from sklearn.ensemble import RandomForestRegressor\n",
    "   ...: import numpy as np\n",
    "   ...: from sklearn.preprocessing import StandardScaler\n",
    "   ...: from sklearn.pipeline import Pipeline\n",
    "   ...: from sklearn.model_selection import cross_val_score\n",
    "In [2]: names = [\"crime\",\"zone\",\"industry\",\"charles\",\n",
    "   ...: \"no\",\"rooms\",\"age\", \"distance\",\n",
    "   ...: \"radial\",\"tax\",\"pupil\",\"aam\",\"lower\",\"med_price\"]\n",
    "\n",
    "In [3]: data = pd.read_csv(\"boston_housing.csv\",names=names)\n",
    "\n",
    "In [4]: X, y = data.iloc[:,:-1], data.iloc[:,-1]\n",
    "\n",
    "# random forest pipeline\n",
    "In [5]: rf_pipeline = Pipeline[(\"st_scaler\", \n",
    "   ...: StandardScaler()),\n",
    "   ...: (\"rf_model\",RandomForestRegressor())]\n",
    "\n",
    "# neg_mean_squared_error is MSE in API compatible way\n",
    "In [6]: scores = cross_val_score(rf_pipeline,X,y,    \n",
    "   ...: scoring=\"neg_mean_squared_error\",cv=10)\n",
    "    \n",
    "In [7]: final_avg_rmse = np.mean(np.sqrt(np.abs(scores)))\n",
    "\n",
    "In [8]: print(\"Final RMSE:\", final_avg_rmse)\n",
    "Final RMSE: 4.54530686529"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing I: LabelEncoder and OneHotEncoder\n",
    "- LabelEncoder: Converts a categorical column of strings into integers\n",
    "- OneHotEncoder: Takes the column of integers and encodes them as dummy variables\n",
    "- Cannot be done within a pipeline - use DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing II: DictVectorizer\n",
    "- Traditionally used in text processing\n",
    "- Converts lists of feature mappings into vectors\n",
    "- Need to convert DataFrame into a list of dictionary entries\n",
    "- Explore the scikit-learn documentation (http://scikit-learn.org/stable/documentation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Exploratory data analysis\n",
    "Before diving into the nitty gritty of pipelines and preprocessing, let's do some exploratory analysis of the original, unprocessed Ames housing dataset (https://www.kaggle.com/c/house-prices-advanced-regression-techniques). When you worked with this data in previous chapters, we preprocessed it for you so you could focus on the core XGBoost concepts. In this chapter, you'll do the preprocessing yourself!\n",
    "\n",
    "A smaller version of this original, unprocessed dataset has been pre-loaded into a pandas DataFrame called df. Your task is to explore df in the Shell and pick the option that is incorrect. The larger purpose of this exercise is to understand the kinds of transformations you will need to perform in order to be able to use XGBoost.\n",
    "\n",
    "Possible Answers\n",
    "- The DataFrame has 21 columns and 1460 rows.\n",
    "- The mean of the LotArea column is 10516.828082.\n",
    "- The DataFrame has missing values.\n",
    "- The LotFrontage column has no missing values and its entries are of type float64.\n",
    "- The standard deviation of SalePrice is 79442.502883\n",
    "\n",
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/xgb4.png\" width=\"300\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
