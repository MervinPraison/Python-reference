{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Gradient Boosting with XGBoost\n",
    "Gradient boosting is currently one of the most popular techniques for efficient modeling of tabular datasets of all sizes. \n",
    "\n",
    "XGboost is a very fast, scalable implementation of gradient boosting\n",
    "\n",
    "Summary\n",
    "1. Classification with XGBoost\n",
    "2. Regression with XGBoost\n",
    "3. Fine-tuning XGBoost model\n",
    "4. XGBoost in Pipelines\n",
    "\n",
    "Reference: Sergey Fogelson, VP Analytics, Viacom, DataCamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note on adding image\n",
    "<img src=\"image.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification with XGBoost\n",
    "Understand the basics of:\n",
    "- supervised classification\n",
    "- decision trees\n",
    "- boosting\n",
    "\n",
    "Supervised learning\n",
    "- relies on labeled data - some understanding on past behavior\n",
    "- 2 kinds of problems: regression and classificaiton\n",
    "- Classification problems\n",
    "    - outcomes are binary or multi-class\n",
    "    - AUC - metric for binary classification models\n",
    "        - larger area under the ROC curve = better model, more sensitive\n",
    "    - Accuracy score and confusion matrix - metric for multiclass\n",
    "    - common algorithms: logistic regression and decision trees\n",
    "\n",
    "Other supervised learning considerations\n",
    "- require a table of feature vectors\n",
    "- Features can be either numeric or categorical\n",
    "- Numeric features should be scaled (Z-scored)\n",
    "- Categorical features should be encoded (one-hot)\n",
    "- Other problems\n",
    "    - Ranking - predicting an ordering on a set of choices\n",
    "        - ie. Google search\n",
    "    - Recommendation\n",
    "        - Recommending an item to a user\n",
    "        - based on consumption history and profile\n",
    "        - ie. Netflix\n",
    "\n",
    "XGBoost introduction\n",
    "\n",
    "What is XGBoost?\n",
    "- Optimized gradient-boosting ML library\n",
    "- orginally written in C++ command line application\n",
    "- Has APIs in several languages:\n",
    "    - Python, R, Scala, Julia, Java\n",
    "    \n",
    "What makes XGBoost so popular?\n",
    "- speed and performance\n",
    "- core algorithm is parallelizable - GPUs and networks of computers\n",
    "    - feasible to scale to 100s of millions of training examples\n",
    "- the real draw is: consistently outperforms single-algorithm methods\n",
    "    - state of the art performance in many ML tasks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 XGBoost: Classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data\n",
    "class_data = pd.read_csv(\"classification_data.csv\")\n",
    "X, y = class_data.iloc[:,:-1], class_data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                   random_state=123)\n",
    "\n",
    "# instantiate XGBoost classifier\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                          n_estimators=10, seed=123)\n",
    "# fit and predict\n",
    "xg_cl.fit(X_train, y_train)\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# evaluate accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))\n",
    "# 0.743300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 XGBoost: Fit/Predict\n",
    "It's time to create your first XGBoost model! As Sergey showed you in the video, you can use the scikit-learn .fit() / .predict() paradigm that you are already familiar to build your XGBoost models, as the xgboost library has a scikit-learn compatible API!\n",
    "\n",
    "Here, you'll be working with churn data. This dataset contains imaginary data from a ride-sharing app with user behaviors over their first month of app usage in a set of imaginary cities as well as whether they used the service 5 months after sign-up. It has been pre-loaded for you into a DataFrame called churn_data - explore it in the Shell!\n",
    "\n",
    "Your goal is to use the first month's worth of data to predict whether the app's users will remain users of the service at the 5 month mark. This is a typical setup for a churn prediction problem. To do this, you'll split the data into training and test sets, fit a small xgboost model on the training set, and evaluate its performance on the test set by computing its accuracy.\n",
    "\n",
    "pandas and numpy have been imported as pd and np, and train_test_split has been imported from sklearn.model_selection. Additionally, the arrays for the features and the target have been created as X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))\n",
    "\n",
    "# accuracy: 0.743300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 What is a decision tree?\n",
    "Decision trees as base learners\n",
    "- Base learner - individual learning algorithm in an ensemble algorithm\n",
    "    - note: XGB is an ensemble learning method that uses outputs of many models for a final prediction\n",
    "- Composed of a series of binary questions/decisions - y/n, T/F\n",
    "- Predictions happen at the \"leaves\" of the tree\n",
    "\n",
    "Decision trees and CART (=classification and regression trees for ML)\n",
    "- Constructed iteratively (one decision at a time)\n",
    "    - Until a stopping criterion is met\n",
    "- Individual decision trees tend to overfit\n",
    "    - Low Bias and High Variance learning models\n",
    "        - good at learning relationships but tend to overfit, so generalize poorly\n",
    "- CART: Classification and Regression Trees\n",
    "    - XGB uses this\n",
    "    - Each leaf ALWAYS contains a real-valued score for classification or regression\n",
    "    - Can later be converted into categories\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Decision trees\n",
    "Your task in this exercise is to make a simple decision tree using scikit-learn's DecisionTreeClassifier on the breast cancer dataset that comes pre-loaded with scikit-learn.\n",
    "\n",
    "This dataset contains numeric measurements of various dimensions of individual tumors (such as perimeter and texture) from breast biopsies and a single outcome value (the tumor is either malignant, or benign).\n",
    "\n",
    "We've preloaded the dataset of samples (measurements) into X and the target values per tumor into y. Now, you have to split the complete dataset into training and testing sets, and then train a DecisionTreeClassifier. You'll specify a parameter called max_depth. Many other parameters can be modified within this model, and you can check all of them out here (http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "# :param max_depth of 4. This parameter specifies the maximum number \n",
    "#  of successive split points you can have before reaching a leaf node.\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 What is Boosting?\n",
    "Boosting overview\n",
    "- Not a specific machine learning algorithm\n",
    "- concept that can be applied to a set of ML models\n",
    "    - \"Meta-algorithm\"\n",
    "- Ensemble meta-algorithm used to convert many weak learners into a strong learner\n",
    "\n",
    "Weak learners and strong learners\n",
    "- Weak learner = ML algorithm that is slightly better than chance\n",
    "    - Example: decision tree whose predictions slightly better than 50%\n",
    "- Boosting converts a collection of weak learners into a strong learner\n",
    "- Strong learner = any algorithm that can be tuned to achieve good performance\n",
    "\n",
    "How boosting is accomplished?\n",
    "- Iteratively learning a set of weak models on subsets of the data\n",
    "- Weighting each weak prediction according to each weak learner's performance\n",
    "    - final prediction is much better than any individual predictions\n",
    "\n",
    "Model evaluation through cross-validation\n",
    "- Cross-validation = robust method for estimating the performance of a model on unseen data by...\n",
    "- Generating many non-overlapping train/test splits on training data\n",
    "- Reports the average test set performance across all data splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.a Cross-validation in XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "class_data = pd.read_csv(\"classification_data.csv\")\n",
    "\n",
    "# DMatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=churn_data.iloc[:,:-1],\n",
    "                            label=churn_data.month_5_still_here])\n",
    "\n",
    "# parameter dictionary to pass into cross validation\n",
    "params={\"objective\":\"binary:logistic\",\"max_depth\":4}\n",
    "\n",
    "# use cv method and pass required dmatrix\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=4,\n",
    "                   num_boost_round=10, metrics=\"error\", as_pandas=True)\n",
    "\n",
    "# output accuracy\n",
    "print(\"Accuracy: %f\" %((1-cv_results[\"test-error-mean\"]).iloc[-1]))\n",
    "# Accuracy: 0.88315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Measuring accuracy\n",
    "You'll now practice using XGBoost's learning API through its baked in cross-validation capabilities. As Sergey discussed in the previous video, XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a DMatrix.\n",
    "\n",
    "In the previous exercise, the input datasets were converted into DMatrix data on the fly, but when you use the xgboost cv object, you have to first explicitly convert your data into a DMatrix. So, that's what you will do here before running cross-validation on churn_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: churn_dmatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "# \"error\" metrics will be converted to accuracy\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, \n",
    "                    num_boost_round=5, metrics=\"error\", as_pandas=True, \n",
    "                    seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))\n",
    "\n",
    "# output\n",
    "       test-error-mean  test-error-std  train-error-mean  train-error-std\n",
    "    0          0.28378        0.001932           0.28232         0.002366\n",
    "    1          0.27190        0.001932           0.26951         0.001855\n",
    "    2          0.25798        0.003963           0.25605         0.003213\n",
    "    3          0.25434        0.003827           0.25090         0.001845\n",
    "    4          0.24852        0.000934           0.24654         0.001981\n",
    "    0.75148"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv_results stores the training and test mean and standard deviation of the error per boosting round (tree built) as a DataFrame. From cv_results, the final round 'test-error-mean' is extracted and converted into an accuracy, where accuracy is 1-error. The final accuracy of around 75% is an improvement from earlier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Measuring AUC\n",
    "Now that you've used cross-validation to compute average out-of-sample accuracy (after converting from an error), it's very easy to compute any other metric you might be interested in. All you have to do is pass it (or a list of metrics) in as an argument to the metrics parameter of xgb.cv().\n",
    "\n",
    "Your job in this exercise is to compute another common metric used in binary classification - the area under the curve (\"auc\"). As before, churn_data is available in your workspace, along with the DMatrix churn_dmatrix and parameter dictionary params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross_validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, \n",
    "                    num_boost_round=5, metrics=\"auc\", as_pandas=True, \n",
    "                    seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])\n",
    "\n",
    "<script.py> output:\n",
    "       test-auc-mean  test-auc-std  train-auc-mean  train-auc-std\n",
    "    0       0.767863      0.002820        0.768893       0.001544\n",
    "    1       0.789157      0.006846        0.790864       0.006758\n",
    "    2       0.814476      0.005997        0.815872       0.003900\n",
    "    3       0.821682      0.003912        0.822959       0.002018\n",
    "    4       0.826191      0.001937        0.827528       0.000769\n",
    "    0.826191\n",
    "\n",
    "# An AUC of 0.84 is quite strong. As you have seen, XGBoost's \n",
    "# learning API makes it very easy to compute any metric you may be \n",
    "# interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 When should I use XGBoost?\n",
    "When to use XGBoost? Criteria:\n",
    "- large number of training samples\n",
    "    - > 1000 training samples and < 100 features\n",
    "    - should be ok when number of features < number of training samples\n",
    "- you have mixture of categorical and numeric features\n",
    "- Or just numeric features\n",
    "\n",
    "When to NOT use XGBoost? Criteria:\n",
    "- had success with other algorithms\n",
    "    - Not suited for: (Deep learning is better)\n",
    "        - image recognition\n",
    "        - computer vision\n",
    "        - NLP and NL understanding problems\n",
    "- dataset size issues\n",
    "    - < 100 training samples\n",
    "    - when number of training samples is significantly smaller than the number of features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Using XGBoost\n",
    "XGBoost is a powerful library that scales very well to many samples and works for a variety of supervised learning problems. That said, as Sergey described in the video, you shouldn't always pick it as your default machine learning library when starting a new project, since there are some situations in which it is not the best option. In this exercise, your job is to consider the below examples and select the one which would be the best use of XGBoost.\n",
    "\n",
    "Possible Answers\n",
    "- Visualizing the similarity between stocks by comparing the time series of their historical prices relative to each other.\n",
    "- Predicting whether a person will develop cancer using genetic data with millions of genes, 23 examples of genomes of people that didn't develop cancer, 3 genomes of people who wound up getting cancer.\n",
    "- Clustering documents into topics based on the terms used in them.\n",
    "- Predicting the likelihood that a given user will click an ad from a very large clickstream log with millions of users and their web interactions.\n",
    "\n",
    "Answer:\n",
    "- D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression with XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning your XGBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost in Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
