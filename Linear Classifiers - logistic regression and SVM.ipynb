{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear classifiers with scikit-learn\n",
    "- logistic regression\n",
    "- SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Applying logistic regression and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review KNN classification\n",
    "- using Large Movie Review Dataset\n",
    "# The variables X_train, X_test, y_train, and y_test are already \n",
    "# loaded into the environment. The X variables contain features \n",
    "# based on the words in the movie reviews, and the y variables \n",
    "# contain labels for whether the review sentiment is positive (+1) \n",
    "# or negative (-1).\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create and fit the model with default hyperparameters\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test features, print the results\n",
    "pred = knn.predict(X_test)[0]\n",
    "print(\"Prediction for test example 0:\", pred)\n",
    "# Prediction for test example 0: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing models\n",
    "# Compare k nearest neighbors classifiers with k=1 and k=5 on the\n",
    "# handwritten digits data set, which is already loaded into the \n",
    "# variables X_train, y_train, X_test, and y_test. \n",
    "# You can set k with the n_neighbors parameter when creating the \n",
    "# KNeighborsClassifier object, which is also already imported into \n",
    "# the environment.\n",
    "\n",
    "# Which model has a higher test accuracy?\n",
    "In [2]: knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "In [3]: knn.fit(X_train, y_train)\n",
    "Out[3]: \n",
    "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
    "           weights='uniform')\n",
    "\n",
    "In [4]: knn.score(X_test, y_test)\n",
    "Out[4]: 0.9888888888888889\n",
    "\n",
    "In [5]: knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "In [6]: knn.fit(X_train, y_train)\n",
    "Out[6]: \n",
    "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
    "           weights='uniform')\n",
    "\n",
    "In [7]: knn.score(X_test, y_test)\n",
    "Out[7]: 0.9933333333333333\n",
    "\n",
    "# n_neighbors = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.predict(X_test)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogReg example 2\n",
    "import sklearn.datasets\n",
    "wine = sklearn.datasets.load_wine()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.ft(wine.data, wine.target)\n",
    "lr.score(wine.data, wine.target)\n",
    "# 0.972\n",
    "\n",
    "# confidence intervals\n",
    "lr.predict_proba(wine.data[:1])\n",
    "array([[ 9.951e-01, 4.357e-03, 5.339e-04]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC\n",
    "import sklearn.datasets\n",
    "wine = sklearn.datasets.load_wine()\n",
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC()\n",
    "svm.ft(wine.data, wine.target)\n",
    "svm.score(wine.data, wine.target)\n",
    "# 0.893"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC - uses nonlinear SVM by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "import sklearn.datasets\n",
    "wine = sklearn.datasets.load_wine()\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.ft(wine.data, wine.target)\n",
    "svm.score(wine.data, wine.target)\n",
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples: logistic regression and SVM\n",
    "#For each classifier, print out the training and validation accuracy.\n",
    "\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target)\n",
    "\n",
    "# Apply logistic regression and print scores\n",
    "lr = LogisticRegression()\n",
    "lr.fit(Xtrain, ytrain)\n",
    "print(lr.score(Xtrain,ytrain))\n",
    "print(lr.score(Xtest,ytest))\n",
    "\n",
    "# Apply SVM and print scores\n",
    "svm = SVC()\n",
    "svm.fit(Xtrain, ytrain)\n",
    "print(svm.score(Xtrain,ytrain))\n",
    "print(svm.score(Xtest,ytest))\n",
    "\n",
    "# 0.9977728285077951\n",
    "# 0.9444444444444444\n",
    "# 1.0\n",
    "# 0.26666666666666666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis for movie reviews\n",
    "# In this exercise you'll explore the probabilities outputted by \n",
    "# logistic regression on a subset of the Large Movie Review Dataset. \n",
    "# The variables X and y are already loaded into the environment. \n",
    "# X contains features based on the number of times words appear in \n",
    "# the movie reviews, and y contains labels for whether the review \n",
    "# sentiment is positive (+1) or negative (-1).\n",
    "\n",
    "# Instantiate logistic regression and train\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Predict sentiment for a glowing review\n",
    "review1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\n",
    "review1_features = get_features(review1)\n",
    "print(\"Review:\", review1)\n",
    "print(\"Probability of positive review:\", \n",
    "      lr.predict_proba(review1_features)[0,1])\n",
    "\n",
    "# Predict sentiment for a poor review\n",
    "review2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\n",
    "review2_features = get_features(review2)\n",
    "print(\"Review:\", review2)\n",
    "print(\"Probability of positive review:\", \n",
    "      lr.predict_proba(review2_features)[0,1])\n",
    "\n",
    "# Review: LOVED IT! This movie was amazing. Top 10 this year.\n",
    "# Probability of positive review: 0.8079007873616059\n",
    "# Review: Total junk! I'll never watch a film by that director again, no matter how good the reviews.\n",
    "# Probability of positive review: 0.5855117402793947\n",
    "\n",
    "# note: \"good\" in the second review throws it off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear classifiers\n",
    "- decision boundaries: linear, nonlinear\n",
    "- linearly separable data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing decision boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision boundaries of various classifier types. \n",
    "# A subset of scikit-learn's built-in wine dataset is already \n",
    "# loaded into X along with binary labels in y.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [LogisticRegression(),\n",
    "LinearSVC(),\n",
    "SVC(),\n",
    "KNeighborsClassifier()]\n",
    "\n",
    "# Fit the classifiers\n",
    "for c in classifiers:\n",
    "    c.fit(X, y)\n",
    "\n",
    "# Plot the classifiers\n",
    "plot_4_classifiers(X, y, classifiers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifiers: the coefficients\n",
    "- prediction equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot products\n",
    "x = np.arange(3)\n",
    "x\n",
    "# out: array([0,1,2])\n",
    "    \n",
    "y = np.arange(3,6)\n",
    "y\n",
    "# out: array([3,4,5])\n",
    "    \n",
    "x*y\n",
    "# out: array([0,4,10])\n",
    "\n",
    "# dot product\n",
    "np.sum(x*y)\n",
    "# out: 14\n",
    "# convenient notation: dot product using @\n",
    "x@y\n",
    "# out: 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear classifier prediction\n",
    "- raw model output = coefficients * features + intercept\n",
    "- Linear classifier prediction:\n",
    "    - compute raw model output\n",
    "    - check the sign (which side of decision boundary)\n",
    "        - if positive, predict one class\n",
    "        - if negative, predict the other class\n",
    "- This is the same for logistic regression and linear SVM\n",
    "    - 'fit' is different but 'predict' is the same\n",
    "    - difference in 'fit' in loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How Logistic Regression makes predictions\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,y)\n",
    "lr.predict(X)[10]\n",
    "# out: 0\n",
    "lr.predict(X)[20]\n",
    "# out: 1\n",
    "\n",
    "# get coefficients\n",
    "lr.coef_ & X[10] + lr.intercept_ # raw model output for example 10\n",
    "array([-33.78572166])\n",
    "# it's negative so predict other class\n",
    "# for example 20, positive, so predict one class\n",
    "lr.coef_ & X[20] + lr.intercept_ # raw model output for example 20\n",
    "array([0.08050621])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the model coefficients\n",
    "# coefficients determine slope of boundary\n",
    "# intercept shifts the boundary\n",
    "\n",
    "# Observe the effects of changing the coefficients of a linear\n",
    "# classifer. A 2D dataset is already loaded into the environment\n",
    "# as X and y, along with a linear classifier object model.\n",
    "\n",
    "# Set the coefficients\n",
    "# changed coefficients from 0,1 to -1,1\n",
    "model.coef_ = np.array([[0,1]])\n",
    "# changed intercept from 0 to -3\n",
    "model.intercept_ = np.array([0])\n",
    "\n",
    "# Plot the data and decision boundary\n",
    "plot_classifier(X,y,model)\n",
    "\n",
    "# Print the number of errors\n",
    "num_err = np.sum(y != model.predict(X))\n",
    "print(\"Number of errors:\", num_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a loss function?\n",
    "- sklearn's LinearRegression minimizes a loss\n",
    "- minimization is with respect to coefficients or parameters of model\n",
    "- note: model.score() isn't necessarily the loss function\n",
    "- The loss is used to fit the model on the data, while the score is used to see how we're doing\n",
    "\n",
    "Classification errors: the 0-1 loss\n",
    "- The squared error/loss isn't appropriate for Classification problems since y are categories (not numbers)\n",
    "- a natural loss for classification problem is the number of errors\n",
    "- This is the '0-1 loss': 0 for correct prediction, 1 for incorrect prediction\n",
    "- By summing the results, you get the total number of incorrect\n",
    "- But this loss is hard to minimize, so logreg and SVM don't use this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing a loss function\n",
    "- using scipy.optimize.minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "minimize(np.square, 0).x\n",
    "#out: array([0.])\n",
    "\n",
    "minimize(np.square, 2).x\n",
    "#out: array([-1.88846401e-08])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model on the Boston housing price data set, which is \n",
    "# already loaded into the variables X and y. For simplicity, we \n",
    "# won't include an intercept in our regression model.\n",
    "\n",
    "# The squared error, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        # Get the true and predicted target values for example 'i'\n",
    "        y_i_true = y[i]\n",
    "        y_i_pred = w@X[i]\n",
    "        s = s + (y_i_pred - y_i_true)**2\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x\n",
    "print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LinearRegression coefficients\n",
    "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
    "print(lr.coef_)\n",
    "\n",
    "# [-9.16299112e-02  4.86754828e-02 -3.77698794e-03  2.85635998e+00\n",
    "#  -2.88057050e+00  5.92521269e+00 -7.22470732e-03 -9.67992974e-01\n",
    "#   1.70448714e-01 -9.38971600e-03 -3.92421893e-01  1.49830571e-02\n",
    "#  -4.16973012e-01]\n",
    "# [-9.16297843e-02  4.86751203e-02 -3.77930006e-03  2.85636751e+00\n",
    "#  -2.88077933e+00  5.92521432e+00 -7.22447929e-03 -9.67995240e-01\n",
    "#   1.70443393e-01 -9.38925373e-03 -3.92425680e-01  1.49832102e-02\n",
    "#  -4.16972624e-01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Support Vector Machines - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
