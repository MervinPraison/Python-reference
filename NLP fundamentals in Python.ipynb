{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP fundamentals in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataCamp with Katharine Jarmul - founder of kjamistan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP (natural language processing)\n",
    "\n",
    "NLP basics:\n",
    "- topic identification\n",
    "- text classification\n",
    "\n",
    "NLP applications:\n",
    "- chatbots\n",
    "- translation\n",
    "- sentiment analysis\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues\n",
    "- problem installing pycld2 for polyglot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regular expressions & word tokenization\n",
    "- parsing text\n",
    "- handle non-English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# match pattern on string\n",
    "re.match('abc', 'abcdef')\n",
    "\n",
    "# match word\n",
    "word_regex = '\\w+'\n",
    "re.match(word_regex, 'hi there!')\n",
    "\n",
    "# split on spaces for tokenization\n",
    "re.split('\\s+', 'Split on spaces.')\n",
    "# ['Split', 'on', 'spaces.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Regex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.a Common Regex patterns\n",
    "- \\w+ matches word\n",
    "- \\d matches digit\n",
    "- \\s matches space\n",
    "- .* wildcard\n",
    "- # + or * greedy match\n",
    "- \\S matches 'not space'\n",
    "- [a-z] lowercase group using brackets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.b re package/module\n",
    "- split - split a string on regex\n",
    "- findall - find all patterns in a string\n",
    "- search - search for a pattern\n",
    "- match - match an entire string or substring based on a pattern\n",
    "\n",
    "Notes:\n",
    "- pass the pattern first and string second\n",
    "- may return an iterator, string, or match object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.b Which pattern?\n",
    "Which of the following Regex patterns results in the following text?\n",
    "- my_string = \"Let's write RegEx!\"\n",
    "- re.findall(PATTERN, my_string)\n",
    "- ['Let', 's', 'write', 'RegEx']\n",
    "\n",
    "In the IPython Shell, try replacing PATTERN with one of the below options and observe the resulting output. The re module has been pre-imported for you and my_string is available in your namespace. \n",
    "\n",
    "Answer:\n",
    "- re.findall(r\"\\w+\", my_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.c Practicing regular expressions: re.split() and re.findall()\n",
    "Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.\n",
    "\n",
    "Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, \"\\n\" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string \"\\n\" - that is, the character \"\\\" followed by the character \"n\" - and not as a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regex module\n",
    "import re\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "# match sentence endings (., ?, and !)\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tokenization\n",
    "What is tokenization?\n",
    "- turning a string or document into tokens (smaller chunks)\n",
    "- one step in preparing a text for NLP\n",
    "- many theories and rules\n",
    "- you can create your own rules using regex\n",
    "- Examples:\n",
    "    - breaking out words or sentences\n",
    "    - separating punctuation\n",
    "    - separating all hashtags in a tweet\n",
    "    \n",
    "nltk library = natural language toolkit\n",
    "\n",
    "Reasons to tokenize?\n",
    "- easier to map part of speech\n",
    "- matching common words\n",
    "- removing unwanted tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.a nltk library\n",
    "some nltk tokenizers\n",
    "- sent_tokenize: tokenize a document into sentences\n",
    "- regexp_tokenize: tokenize a string or document based on a regex pattern\n",
    "- TweetTokenizer: special class just for tweet tokenization, allowing you to separate hashtags, mentions, and lots of exclamation points!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# separate words and punctuation\n",
    "word_tokenize(\"Hi there!\")\n",
    "# output\n",
    "# ['Hi', 'there', '!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.b Difference b/n re.search() and re.match()\n",
    "- match starts from beginning of string\n",
    "\n",
    "Follow this rule:\n",
    "- if you look for a pattern that might not be at the beginning of a string, use search()\n",
    "- if you want the entire string or just the beginning, use match()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# same results\n",
    "rematch('abc', 'abcde')\n",
    "re.search('abc', 'abcde')\n",
    "\n",
    "# different results\n",
    "re.match('cd', 'abcde')\n",
    "# no result\n",
    "re.search('cd', 'abcde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.c Word tokenization with NLTK\n",
    "Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as scene_one. Feel free to check it out in the IPython Shell!\n",
    "\n",
    "Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:07:35.232289Z",
     "start_time": "2018-11-19T20:07:33.742167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.d More regex with re.search()\n",
    "In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.\n",
    "\n",
    "You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in \n",
    "# square brackets: pattern1\n",
    "pattern1 = r\"\\[.*]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence \n",
    "# and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Advanced Tokenization with Regex\n",
    "Regex groups using or \"|\" character\n",
    "- OR is represented using |\n",
    "- you can define a group using ()\n",
    "- define explicit character ranges using []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: all digitis or words\n",
    "import re\n",
    "match_digits_and_words = ('\\d+|\\w+')\n",
    "re.findall(match_digits_and_words, 'He has 11 cats.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex ranges and groups\n",
    "- Pattern, matches, example\n",
    "- [A-Za-z]+, upper and lowercase English, 'ABCDEFghijk'\n",
    "- [0-9], numbers 0-9, 9\n",
    "- [A-Za-z\\.\\]+m, upper and lowercase English alphabet,\n",
    "- (a-z), a, - and z, 'a-z'\n",
    "- (\\s+|,), spaces or comma, ',\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character range with re.match()\n",
    "import re\n",
    "my_str = 'match lowercase spaces nums like 12, but no comma'\n",
    "re.match('[a-z0-9 ]+', my_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.a Choosing a tokenizer\n",
    "Given the following string, which of the below patterns is the best tokenizer? If possible, you want to retain sentence punctuation as separate tokens, but have '#1' remain a single token.\n",
    "- my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "\n",
    "The string is available in your workspace as my_string, and the patterns have been pre-loaded as pattern1, pattern2, pattern3, and pattern4, respectively.\n",
    "- pattern1 = r\"\\w+(\\?!)\"\n",
    "- pattern2 = r\"(\\w+|#\\d|\\?|!)\"\n",
    "- pattern3 = r\"(#\\d\\w+\\?!)\"\n",
    "- pattern4 = r\"\\s+\"\n",
    "\n",
    "Additionally, regexp_tokenize has been imported from nltk.tokenize. You can use regexp_tokenize() with my_string and one of the patterns as arguments to experiment for yourself and see which is the best tokenizer.\n",
    "- regexp_tokenize(string, pattern)\n",
    "\n",
    "Answer: pattern 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "In [11]: regexp_tokenize(pattern1, my_string)\n",
    "Out[11]: []\n",
    "\n",
    "In [12]: regexp_tokenize(my_string, pattern1)\n",
    "Out[12]: []\n",
    "\n",
    "In [13]: regexp_tokenize(my_string, pattern2)\n",
    "Out[13]: \n",
    "['SOLDIER',\n",
    " '#1',\n",
    " 'Found',\n",
    " 'them',\n",
    " '?',\n",
    " 'In',\n",
    " 'Mercea',\n",
    " '?',\n",
    " 'The',\n",
    " 'coconut',\n",
    " 's',\n",
    " 'tropical',\n",
    " '!']\n",
    "\n",
    "In [14]: regexp_tokenize(my_string, pattern3)\n",
    "Out[14]: []\n",
    "\n",
    "In [15]: regexp_tokenize(my_string, pattern4)\n",
    "Out[15]: [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.b Regex with NLTK tokenization\n",
    "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets.\n",
    "\n",
    "Here, you're given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "regexp_tokenize(tweets[0], pattern1)\n",
    "\n",
    "# Write a pattern that matches both mentions and hashtags\n",
    "# mention example: @example\n",
    "pattern2 = r\"([@|#]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[-1], pattern2)\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.c Non-ascii tokenization\n",
    "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
    "\n",
    "Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji and the German characters!\n",
    "\n",
    "The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.\n",
    "\n",
    "Unicode ranges for emoji are:\n",
    "- ('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-Z|Ü]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'| \\\n",
    "'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕\n",
    "\n",
    "In [1]: all_words = word_tokenize(german_text)\n",
    "        print(all_words)\n",
    "- ['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n",
    "\n",
    "\n",
    "In [2]: capital_words = r\"[A-Z|Ü]\\w+\"\n",
    "        print(regexp_tokenize(german_text, capital_words))\n",
    "- ['Wann', 'Pizza', 'Und', 'Über']\n",
    "\n",
    "In [3]: emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "        print(regexp_tokenize(german_text, emoji))\n",
    "- ['🍕', '🚕']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Charting word length with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T00:24:28.219699Z",
     "start_time": "2018-11-20T00:24:27.968283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a13b65240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example histogram with matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# load arry and plot histogram\n",
    "plt.hist([1,5,5,7,7,7,9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.a Combining NLP data extraction with plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T00:49:50.240529Z",
     "start_time": "2018-11-20T00:35:39.094268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T00:53:26.504814Z",
     "start_time": "2018-11-20T00:53:26.384677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAADhdJREFUeJzt3V2MXPV9h/HnG9t5KSFBileNZWw2VVClJCovXVEQUoSStIKAIFKJZKSSBKWyFEELaqQKuACFK7ghVUIU5AINpBSIeInc4DSlggi4gLB2zauDZCEqVlDZgQRw84Kc/nqx52K7jJmzuzM7+O/nI418Zua/c34jy4+Pj8/spqqQJLXlPZMeQJI0esZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQWsnteP169fX9PT0pHYvSYelnTt3/qKqpoatm1jcp6enmZ2dndTuJemwlOS/+qzztIwkNci4S1KDjLskNci4S1KDjLskNWho3JO8P8nPkjyZ5Nkk3xiw5n1J7kqyN8njSabHMawkqZ8+R+6/Az5TVScAJwJnJjl10ZqvAr+sqo8D3wSuG+2YkqSlGBr3mnegu7uuuy3+2XznAbd223cDn02SkU0pSVqSXufck6xJshvYBzxQVY8vWrIReAmgqg4CrwMfGeWgkqT+en1Ctap+D5yY5BjgviSfqqpnFiwZdJT+tp+8nWQrsBVg8+bNyxhXatv05fdPZL8vXnv2RPar8VnS1TJV9Svgp8CZi56aAzYBJFkLfBh4bcDXb6uqmaqamZoa+q0RJEnL1OdqmanuiJ0kHwA+B/x80bLtwJe77fOBB6vqbUfukqTV0ee0zAbg1iRrmP/L4AdV9aMk1wCzVbUduBn4fpK9zB+xbxnbxJKkoYbGvaqeAk4a8PhVC7Z/C3xxtKNJkpbLT6hKUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1aGjck2xK8lCSPUmeTXLpgDVnJHk9ye7udtV4xpUk9bG2x5qDwNeraleSo4GdSR6oqucWrXukqs4Z/YiSpKUaeuReVa9U1a5u+01gD7Bx3INJkpZvSefck0wDJwGPD3j6tCRPJvlxkk8e4uu3JplNMrt///4lDytJ6qd33JN8ELgHuKyq3lj09C7guKo6Afg28MNBr1FV26pqpqpmpqamljuzJGmIXnFPso75sN9eVfcufr6q3qiqA932DmBdkvUjnVSS1Fufq2UC3AzsqarrD7Hmo906kpzSve6roxxUktRfn6tlTgcuBJ5Osrt77EpgM0BV3QicD3wtyUHgN8CWqqoxzCtJ6mFo3KvqUSBD1twA3DCqoSRJK+MnVCWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQUPjnmRTkoeS7EnybJJLB6xJkm8l2ZvkqSQnj2dcSVIfa3usOQh8vap2JTka2Jnkgap6bsGas4Dju9ufAd/tfpUkTcDQI/eqeqWqdnXbbwJ7gI2Llp0H3FbzHgOOSbJh5NNKknpZ0jn3JNPAScDji57aCLy04P4cb/8LQJK0SvqclgEgyQeBe4DLquqNxU8P+JIa8Bpbga0AmzdvXsKY/9/05fcv+2tX6sVrz57YviWpr15H7knWMR/226vq3gFL5oBNC+4fC7y8eFFVbauqmaqamZqaWs68kqQe+lwtE+BmYE9VXX+IZduBL3VXzZwKvF5Vr4xwTknSEvQ5LXM6cCHwdJLd3WNXApsBqupGYAfweWAv8GvgotGPKknqa2jcq+pRBp9TX7imgItHNZQkaWX8hKokNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDjLskNci4S1KDhsY9yS1J9iV55hDPn5Hk9SS7u9tVox9TkrQUa3us+R5wA3DbO6x5pKrOGclEkqQVG3rkXlUPA6+twiySpBEZ1Tn305I8meTHST55qEVJtiaZTTK7f//+Ee1akrTYKOK+Cziuqk4Avg388FALq2pbVc1U1czU1NQIdi1JGmTFca+qN6rqQLe9A1iXZP2KJ5MkLduK457ko0nSbZ/SvearK31dSdLyDb1aJskdwBnA+iRzwNXAOoCquhE4H/hakoPAb4AtVVVjm1iSNNTQuFfVBUOev4H5SyUlSe8SfkJVkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkho0NO5JbkmyL8kzh3g+Sb6VZG+Sp5KcPPoxJUlL0efI/XvAme/w/FnA8d1tK/DdlY8lSVqJoXGvqoeB195hyXnAbTXvMeCYJBtGNaAkaelGcc59I/DSgvtz3WOSpAlZO4LXyIDHauDCZCvzp27YvHnzCHZ95Ji+/P6J7fvFa8+e2L6lcWn9z9QojtzngE0L7h8LvDxoYVVtq6qZqpqZmpoawa4lSYOMIu7bgS91V82cCrxeVa+M4HUlScs09LRMkjuAM4D1SeaAq4F1AFV1I7AD+DywF/g1cNG4hpUk9TM07lV1wZDnC7h4ZBNJklbMT6hKUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1qFfck5yZ5Pkke5NcPuD5ryTZn2R3d/vr0Y8qSepr7bAFSdYA3wH+HJgDnkiyvaqeW7T0rqq6ZAwzSpKWqM+R+ynA3qp6oareAu4EzhvvWJKklegT943ASwvuz3WPLfaXSZ5KcneSTYNeKMnWJLNJZvfv37+McSVJffSJewY8Vovu/yswXVV/AvwHcOugF6qqbVU1U1UzU1NTS5tUktRbn7jPAQuPxI8FXl64oKperarfdXf/EfjT0YwnSVqOPnF/Ajg+yceSvBfYAmxfuCDJhgV3zwX2jG5ESdJSDb1apqoOJrkE+AmwBrilqp5Ncg0wW1Xbgb9Nci5wEHgN+MoYZ5YkDTE07gBVtQPYseixqxZsXwFcMdrRJEnL5SdUJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBveKe5MwkzyfZm+TyAc+/L8ld3fOPJ5ke9aCSpP6Gxj3JGuA7wFnAJ4ALknxi0bKvAr+sqo8D3wSuG/WgkqT++hy5nwLsraoXquot4E7gvEVrzgNu7bbvBj6bJKMbU5K0FH3ivhF4acH9ue6xgWuq6iDwOvCRUQwoSVq6tT3WDDoCr2WsIclWYGt390CS53vsf5D1wC+W+bUrksmdcPI9Hxkm8p4n+HsMR+Dvc65b0Xs+rs+iPnGfAzYtuH8s8PIh1swlWQt8GHht8QtV1TZgW5/B3kmS2aqaWenrHE58z0cG3/ORYTXec5/TMk8Axyf5WJL3AluA7YvWbAe+3G2fDzxYVW87cpckrY6hR+5VdTDJJcBPgDXALVX1bJJrgNmq2g7cDHw/yV7mj9i3jHNoSdI763NahqraAexY9NhVC7Z/C3xxtKO9oxWf2jkM+Z6PDL7nI8PY33M8eyJJ7fHbD0hSgw6ruCe5Jcm+JM9MepbVkmRTkoeS7EnybJJLJz3TuCV5f5KfJXmye8/fmPRMqyHJmiT/meRHk55ltSR5McnTSXYnmZ30POOW5Jgkdyf5efdn+rSx7etwOi2T5NPAAeC2qvrUpOdZDUk2ABuqaleSo4GdwBeq6rkJjzY23aebj6qqA0nWAY8Cl1bVYxMebayS/B0wA3yoqs6Z9DyrIcmLwExVHRHXuSe5FXikqm7qrj78g6r61Tj2dVgduVfVwwy4fr5lVfVKVe3qtt8E9vD2Twg3peYd6O6u626Hz1HIMiQ5FjgbuGnSs2g8knwI+DTzVxdSVW+NK+xwmMX9SNd9t82TgMcnO8n4dacodgP7gAeqqvX3/A/A3wP/O+lBVlkB/55kZ/cJ9pb9EbAf+Kfu9NtNSY4a186M+2EiyQeBe4DLquqNSc8zblX1+6o6kflPRJ+SpNnTcEnOAfZV1c5JzzIBp1fVycx/19mLu1OvrVoLnAx8t6pOAv4HeNu3UB8V434Y6M473wPcXlX3Tnqe1dT9s/WnwJkTHmWcTgfO7c4/3wl8Jsk/T3ak1VFVL3e/7gPuY/670LZqDphb8K/Qu5mP/VgY93e57j8Xbwb2VNX1k55nNSSZSnJMt/0B4HPAzyc71fhU1RVVdWxVTTP/6e4Hq+qvJjzW2CU5qrtIgO70xF8AzV4JV1X/DbyU5I+7hz4LjO3CiF6fUH23SHIHcAawPskccHVV3TzZqcbudOBC4OnuHDTAld2nhlu1Abi1+0Ex7wF+UFVHzOWBR5A/BO7rfvTDWuBfqurfJjvS2P0NcHt3pcwLwEXj2tFhdSmkJKkfT8tIUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ16P8AUSKJo7DsGIIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1778afd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(\"This is a pretty cool tool!\")\n",
    "# make a list of word lengths\n",
    "word_lengths = [len(w) for w in words]\n",
    "\n",
    "plt.hist(word_lengths)\n",
    "plt.show()\n",
    "\n",
    "# see 4 letter words is the most common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.a Charting practice\n",
    "Try using your new skills to find and chart the number of words per line in the script using matplotlib. The Holy Grail script is loaded for you, and you need to use regex to find the words per line.\n",
    "\n",
    "Using list comprehensions here will speed up your computations. For example: my_lines = [tokenize(l) for l in lines] will call a function tokenize on each line in the list lines. The new transformed list will be saved in the my_lines variable.\n",
    "\n",
    "You have access to the entire script in the variable holy_grail. Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T00:53:30.896938Z",
     "start_time": "2018-11-20T00:53:30.869581Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker (ie. remove ARTHUR:)\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "# Keep only words in each line\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot (see below)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T00:53:43.635598Z",
     "start_time": "2018-11-20T00:53:43.607540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"428pt\" version=\"1.1\" viewBox=\"0 0 270 428\" width=\"270pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 428  L 270 428  L 270 0  L 0 0  z \" style=\"fill:#ffffff;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 36.8875 400.521875  L 255.7 400.521875  L 255.7 14.3  L 36.8875 14.3  z \" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 46.833523 400.521875  L 66.725568 400.521875  L 66.725568 32.691518  L 46.833523 32.691518  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 66.725568 400.521875  L 86.617614 400.521875  L 86.617614 329.445485  L 66.725568 329.445485  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 86.617614 400.521875  L 106.509659 400.521875  L 106.509659 379.640676  L 86.617614 379.640676  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 106.509659 400.521875  L 126.401705 400.521875  L 126.401705 391.687521  L 106.509659 391.687521  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 126.401705 400.521875  L 146.29375 400.521875  L 146.29375 396.907821  L 126.401705 396.907821  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 146.29375 400.521875  L 166.185795 400.521875  L 166.185795 398.915629  L 146.29375 398.915629  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 166.185795 400.521875  L 186.077841 400.521875  L 186.077841 398.915629  L 166.185795 398.915629  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 186.077841 400.521875  L 205.969886 400.521875  L 205.969886 398.514067  L 186.077841 398.514067  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 205.969886 400.521875  L 225.861932 400.521875  L 225.861932 400.120313  L 205.969886 400.120313  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 225.861932 400.521875  L 245.753977 400.521875  L 245.753977 399.718752  L 225.861932 399.718752  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0  L 0 3.5  \" id=\"m22b2467620\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.833523\" xlink:href=\"#m22b2467620\" y=\"400.521875\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625  Q 24.171875 66.40625 20.328125 58.90625  Q 16.5 51.421875 16.5 36.375  Q 16.5 21.390625 20.328125 13.890625  Q 24.171875 6.390625 31.78125 6.390625  Q 39.453125 6.390625 43.28125 13.890625  Q 47.125 21.390625 47.125 36.375  Q 47.125 51.421875 43.28125 58.90625  Q 39.453125 66.40625 31.78125 66.40625  z M 31.78125 74.21875  Q 44.046875 74.21875 50.515625 64.515625  Q 56.984375 54.828125 56.984375 36.375  Q 56.984375 17.96875 50.515625 8.265625  Q 44.046875 -1.421875 31.78125 -1.421875  Q 19.53125 -1.421875 13.0625 8.265625  Q 6.59375 17.96875 6.59375 36.375  Q 6.59375 54.828125 13.0625 64.515625  Q 19.53125 74.21875 31.78125 74.21875  z \" id=\"DejaVuSans-30\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(43.652273 415.120313)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"85.458854\" xlink:href=\"#m22b2467620\" y=\"400.521875\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 20 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875  L 53.609375 8.296875  L 53.609375 0  L 7.328125 0  L 7.328125 8.296875  Q 12.9375 14.109375 22.625 23.890625  Q 32.328125 33.6875 34.8125 36.53125  Q 39.546875 41.84375 41.421875 45.53125  Q 43.3125 49.21875 43.3125 52.78125  Q 43.3125 58.59375 39.234375 62.25  Q 35.15625 65.921875 28.609375 65.921875  Q 23.96875 65.921875 18.8125 64.3125  Q 13.671875 62.703125 7.8125 59.421875  L 7.8125 69.390625  Q 13.765625 71.78125 18.9375 73  Q 24.125 74.21875 28.421875 74.21875  Q 39.75 74.21875 46.484375 68.546875  Q 53.21875 62.890625 53.21875 53.421875  Q 53.21875 48.921875 51.53125 44.890625  Q 49.859375 40.875 45.40625 35.40625  Q 44.1875 33.984375 37.640625 27.21875  Q 31.109375 20.453125 19.1875 8.296875  z \" id=\"DejaVuSans-32\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(79.096354 415.120313)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"124.084185\" xlink:href=\"#m22b2467620\" y=\"400.521875\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 40 -->\n",
       "      <defs>\n",
       "       <path d=\"M 37.796875 64.3125  L 12.890625 25.390625  L 37.796875 25.390625  z M 35.203125 72.90625  L 47.609375 72.90625  L 47.609375 25.390625  L 58.015625 25.390625  L 58.015625 17.1875  L 47.609375 17.1875  L 47.609375 0  L 37.796875 0  L 37.796875 17.1875  L 4.890625 17.1875  L 4.890625 26.703125  z \" id=\"DejaVuSans-34\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(117.721685 415.120313)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"162.709516\" xlink:href=\"#m22b2467620\" y=\"400.521875\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 60 -->\n",
       "      <defs>\n",
       "       <path d=\"M 33.015625 40.375  Q 26.375 40.375 22.484375 35.828125  Q 18.609375 31.296875 18.609375 23.390625  Q 18.609375 15.53125 22.484375 10.953125  Q 26.375 6.390625 33.015625 6.390625  Q 39.65625 6.390625 43.53125 10.953125  Q 47.40625 15.53125 47.40625 23.390625  Q 47.40625 31.296875 43.53125 35.828125  Q 39.65625 40.375 33.015625 40.375  z M 52.59375 71.296875  L 52.59375 62.3125  Q 48.875 64.0625 45.09375 64.984375  Q 41.3125 65.921875 37.59375 65.921875  Q 27.828125 65.921875 22.671875 59.328125  Q 17.53125 52.734375 16.796875 39.40625  Q 19.671875 43.65625 24.015625 45.921875  Q 28.375 48.1875 33.59375 48.1875  Q 44.578125 48.1875 50.953125 41.515625  Q 57.328125 34.859375 57.328125 23.390625  Q 57.328125 12.15625 50.6875 5.359375  Q 44.046875 -1.421875 33.015625 -1.421875  Q 20.359375 -1.421875 13.671875 8.265625  Q 6.984375 17.96875 6.984375 36.375  Q 6.984375 53.65625 15.1875 63.9375  Q 23.390625 74.21875 37.203125 74.21875  Q 40.921875 74.21875 44.703125 73.484375  Q 48.484375 72.75 52.59375 71.296875  z \" id=\"DejaVuSans-36\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(156.347016 415.120313)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"201.334847\" xlink:href=\"#m22b2467620\" y=\"400.521875\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 80 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 34.625  Q 24.75 34.625 20.71875 30.859375  Q 16.703125 27.09375 16.703125 20.515625  Q 16.703125 13.921875 20.71875 10.15625  Q 24.75 6.390625 31.78125 6.390625  Q 38.8125 6.390625 42.859375 10.171875  Q 46.921875 13.96875 46.921875 20.515625  Q 46.921875 27.09375 42.890625 30.859375  Q 38.875 34.625 31.78125 34.625  z M 21.921875 38.8125  Q 15.578125 40.375 12.03125 44.71875  Q 8.5 49.078125 8.5 55.328125  Q 8.5 64.0625 14.71875 69.140625  Q 20.953125 74.21875 31.78125 74.21875  Q 42.671875 74.21875 48.875 69.140625  Q 55.078125 64.0625 55.078125 55.328125  Q 55.078125 49.078125 51.53125 44.71875  Q 48 40.375 41.703125 38.8125  Q 48.828125 37.15625 52.796875 32.3125  Q 56.78125 27.484375 56.78125 20.515625  Q 56.78125 9.90625 50.3125 4.234375  Q 43.84375 -1.421875 31.78125 -1.421875  Q 19.734375 -1.421875 13.25 4.234375  Q 6.78125 9.90625 6.78125 20.515625  Q 6.78125 27.484375 10.78125 32.3125  Q 14.796875 37.15625 21.921875 38.8125  z M 18.3125 54.390625  Q 18.3125 48.734375 21.84375 45.5625  Q 25.390625 42.390625 31.78125 42.390625  Q 38.140625 42.390625 41.71875 45.5625  Q 45.3125 48.734375 45.3125 54.390625  Q 45.3125 60.0625 41.71875 63.234375  Q 38.140625 66.40625 31.78125 66.40625  Q 25.390625 66.40625 21.84375 63.234375  Q 18.3125 60.0625 18.3125 54.390625  z \" id=\"DejaVuSans-38\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(194.972347 415.120313)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"239.960178\" xlink:href=\"#m22b2467620\" y=\"400.521875\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 100 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875  L 28.515625 8.296875  L 28.515625 63.921875  L 10.984375 60.40625  L 10.984375 69.390625  L 28.421875 72.90625  L 38.28125 72.90625  L 38.28125 8.296875  L 54.390625 8.296875  L 54.390625 0  L 12.40625 0  z \" id=\"DejaVuSans-31\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(230.416428 415.120313)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0  L -3.5 0  \" id=\"ma0e477f1b2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.8875\" xlink:href=\"#ma0e477f1b2\" y=\"400.521875\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(23.525 404.321094)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.8875\" xlink:href=\"#ma0e477f1b2\" y=\"320.20957\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(10.8 324.008789)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.8875\" xlink:href=\"#ma0e477f1b2\" y=\"239.897265\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(10.8 243.696484)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.8875\" xlink:href=\"#ma0e477f1b2\" y=\"159.58496\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 600 -->\n",
       "      <g transform=\"translate(10.8 163.384179)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.8875\" xlink:href=\"#ma0e477f1b2\" y=\"79.272655\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 800 -->\n",
       "      <g transform=\"translate(10.8 83.071874)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 36.8875 400.521875  L 36.8875 14.3  \" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 255.7 400.521875  L 255.7 14.3  \" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 36.8875 400.521875  L 255.7 400.521875  \" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 36.8875 14.3  L 255.7 14.3  \" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p231c1a4d4a\">\n",
       "   <rect height=\"386.221875\" width=\"218.8125\" x=\"36.8875\" y=\"14.3\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image, SVG\n",
    "SVG(filename='Images/nlp_linelength.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simple topic identification\n",
    "- objective: identify topics from texts based on term frequencies\n",
    "- methods:\n",
    "    1. bag-of-words\n",
    "    2. Tf-idf\n",
    "- libraries: NLTK, Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Bag-of-words\n",
    "- basic method for finding topics in a text\n",
    "- need to first create tokens using tokenization\n",
    "    - then count up all tokens\n",
    "- concept: word frequency related to importance\n",
    "- preprocessing steps:\n",
    "    - usually may include lower case for all letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T02:52:22.129902Z",
     "start_time": "2018-11-20T02:52:22.122936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'The': 3, 'cat': 3, 'the': 3, 'box': 3, '.': 3, 'is': 2, 'in': 1, 'likes': 1, 'over': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 3), ('cat', 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(word_tokenize(\n",
    "    \"\"\"The cat is in the box. The cat likes the box.\n",
    "    The box is over the cat.\"\"\"))\n",
    "print(counter)\n",
    "# counter object like a dictionary\n",
    "\n",
    "# return top 2 tokens\n",
    "counter.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.a Building a Counter with bag-of-words\n",
    "In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as article. Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you'd like to peek at the title at the end, we've included it as article_title. Note that this article text has had very little preprocessing from the raw Wikipedia database entry.\n",
    "\n",
    "word_tokenize has been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))\n",
    "\n",
    "# [(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 68), \n",
    "#  ('to', 63), ('a', 60), ('in', 44), ('and', 41), ('(', 40)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Simple text preprocessing\n",
    "Preprocessing:\n",
    "- tokenization to create a bag of words\n",
    "- lowercasing words\n",
    "- Lemmatization/Stemming = shorten words to their root systems\n",
    "    - make plural words singular\n",
    "- Remove\n",
    "    - stop words - ie. and, the\n",
    "    - punctuation\n",
    "    - unwanted tokens\n",
    "    \n",
    "Try some different methods and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T03:14:43.434009Z",
     "start_time": "2018-11-20T03:14:43.414449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('box', 3)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text preprocessing with Python\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = \"\"\"The cat is in the box. The cat likes the box.\n",
    "    The box is over the cat.\"\"\"\n",
    "\n",
    "# list comprehension to tokenize sentences, \n",
    "# only use alphabet characters\n",
    "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
    "\n",
    "# remove stop words\n",
    "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
    "\n",
    "# print 2 most common words after preprocessing\n",
    "Counter(no_stops).most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.a Text preprocessing practice\n",
    "Now, it's your turn to apply the techniques you've learned to help clean up text for better NLP results. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.\n",
    "\n",
    "You start with the same tokens you created in the last exercise: lower_tokens. You also have the Counter class imported.\n",
    "- Create another list called no_stops in which you remove all stop words, which are held in a list called english_stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T03:22:38.263249Z",
     "start_time": "2018-11-20T03:22:38.154547Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# list of english stop words\n",
    "english_stops = ['i',\n",
    " 'me',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'we',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'you',\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves',\n",
    " 'he',\n",
    " 'him',\n",
    " 'his',\n",
    " 'himself',\n",
    " 'she',\n",
    " 'her',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'it',\n",
    " 'its',\n",
    " 'itself',\n",
    " 'they',\n",
    " 'them',\n",
    " 'their',\n",
    " 'theirs',\n",
    " 'themselves',\n",
    " 'what',\n",
    " 'which',\n",
    " 'who',\n",
    " 'whom',\n",
    " 'this',\n",
    " 'that',\n",
    " 'these',\n",
    " 'those',\n",
    " 'am',\n",
    " 'is',\n",
    " 'are',\n",
    " 'was',\n",
    " 'were',\n",
    " 'be',\n",
    " 'been',\n",
    " 'being',\n",
    " 'have',\n",
    " 'has',\n",
    " 'had',\n",
    " 'having',\n",
    " 'do',\n",
    " 'does',\n",
    " 'did',\n",
    " 'doing',\n",
    " 'a',\n",
    " 'an',\n",
    " 'the',\n",
    " 'and',\n",
    " 'but',\n",
    " 'if',\n",
    " 'or',\n",
    " 'because',\n",
    " 'as',\n",
    " 'until',\n",
    " 'while',\n",
    " 'of',\n",
    " 'at',\n",
    " 'by',\n",
    " 'for',\n",
    " 'with',\n",
    " 'about',\n",
    " 'against',\n",
    " 'between',\n",
    " 'into',\n",
    " 'through',\n",
    " 'during',\n",
    " 'before',\n",
    " 'after',\n",
    " 'above',\n",
    " 'below',\n",
    " 'to',\n",
    " 'from',\n",
    " 'up',\n",
    " 'down',\n",
    " 'in',\n",
    " 'out',\n",
    " 'on',\n",
    " 'off',\n",
    " 'over',\n",
    " 'under',\n",
    " 'again',\n",
    " 'further',\n",
    " 'then',\n",
    " 'once',\n",
    " 'here',\n",
    " 'there',\n",
    " 'when',\n",
    " 'where',\n",
    " 'why',\n",
    " 'how',\n",
    " 'all',\n",
    " 'any',\n",
    " 'both',\n",
    " 'each',\n",
    " 'few',\n",
    " 'more',\n",
    " 'most',\n",
    " 'other',\n",
    " 'some',\n",
    " 'such',\n",
    " 'no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'only',\n",
    " 'own',\n",
    " 'same',\n",
    " 'so',\n",
    " 'than',\n",
    " 'too',\n",
    " 'very',\n",
    " 's',\n",
    " 't',\n",
    " 'can',\n",
    " 'will',\n",
    " 'just',\n",
    " 'don',\n",
    " 'should',\n",
    " 'now',\n",
    " 'd',\n",
    " 'll',\n",
    " 'm',\n",
    " 'o',\n",
    " 're',\n",
    " 've',\n",
    " 'y',\n",
    " 'ain',\n",
    " 'aren',\n",
    " 'couldn',\n",
    " 'didn',\n",
    " 'doesn',\n",
    " 'hadn',\n",
    " 'hasn',\n",
    " 'haven',\n",
    " 'isn',\n",
    " 'ma',\n",
    " 'mightn',\n",
    " 'mustn',\n",
    " 'needn',\n",
    " 'shan',\n",
    " 'shouldn',\n",
    " 'wasn',\n",
    " 'weren',\n",
    " 'won',\n",
    " 'wouldn',\n",
    " '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing: remove stop words, non-alpha characters,\n",
    "# lemmatize, and perform bag-of-words\n",
    "\n",
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))\n",
    "\n",
    "# output\n",
    "# [('debugging', 40), ('system', 25), ('software', 16), \n",
    "#  ('bug', 16), ('problem', 15), ('tool', 15), ('computer', 14), \n",
    "#  ('term', 13), ('process', 13), ('used', 12)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 gensim\n",
    "What is gensim?\n",
    "- popular open-source NLP library\n",
    "- uses top academic models to perform complex tasks\n",
    "    - building document or word vectors\n",
    "    - performing topic identification and document comparison\n",
    "\n",
    "Other notes:\n",
    "- LDA used for topic analysis and modeling\n",
    "\n",
    "Examples of word vector (visuals work better)\n",
    "- male-female\n",
    "- verb tense: walking-walked\n",
    "- country-capital: Spain-Madrid\n",
    "\n",
    "corpus/corpora = set of texts used to perform NLP tasks\n",
    "\n",
    "Note:\n",
    "- gensim models can be easily saved, updated, and reused\n",
    "    - AWESOME!!!\n",
    "- our dictionary can also be updated\n",
    "    - with new texts\n",
    "    - words that meet certain thresholds\n",
    "    - then use for feature exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.a Create a gensim dictionary and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# documents contain movie reviews\n",
    "my_documents = ['The movie was about a spaceship and aliens.',\n",
    "                'I really liked the movie!',\n",
    "                'Awesome action scenes, but boring characters.',\n",
    "                'The movie was awful! I hate alien films.',\n",
    "                'Space is cool! I liked the movie.',\n",
    "                'More space films, please!'\n",
    "               ]\n",
    "\n",
    "# simple brief example tokenizing and lowercase\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "# should also remove punctuation and stop words\n",
    "\n",
    "# start corpus: use Dictionary class to map id to each token\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "\n",
    "# look at token and id\n",
    "dictionary.token2id\n",
    "\n",
    "# create a gensim corpus\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# view corpus - a list of lists, each list item is 1 document\n",
    "# form: (id, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.b Example: Creating and querying a corpus with gensim\n",
    "It's time to apply the methods you learned in the previous video to create your first gensim dictionary and corpus!\n",
    "\n",
    "You'll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called articles. You'll need to do some light preprocessing and then generate the gensim dictionary and corpus.\n",
    "\n",
    "Notes:\n",
    "- Obtain the id for \"computer\" from dictionary. To do this, use its .token2id method which returns ids from text, and then chain .get() which returns tokens from ids. Pass in \"computer\" as an argument to .get().\n",
    "- Use a list comprehension in which you iterate over articles to create a gensim MmCorpus from dictionary.\n",
    "    - In the output expression, use the .doc2bow() method on dictionary with article as the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from \n",
    "# the fifth document\n",
    "print(corpus[4][:10])\n",
    "\n",
    "# output\n",
    "# computer\n",
    "# [(0, 88), (23, 11), (24, 2), (39, 1), (41, 2), (55, 22), (56, 1), \n",
    "#  (57, 1), (58, 1), (59, 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.c Gensim bag-of-words\n",
    "Now, you'll use your new gensim corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are and feel free to explore more documents in the IPython Shell!\n",
    "\n",
    "You have access to the dictionary and corpus objects you created in the previous exercise, as well as the Python defaultdict and itertools to help with the creation of intermediate data structures for analysis.\n",
    "\n",
    "The fifth document from corpus is stored in the variable doc, which has been sorted in descending order.\n",
    "\n",
    "Notes:\n",
    "- Print the top five words of bow_doc using each word_id with the dictionary alongside word_count. The word_id can be accessed using the .get() method of dictionary.\n",
    "- Create a defaultdict called total_word_count in which the keys are all the token ids (word_id) and the values are the sum of their occurrence across all documents (word_count). Remember to specify int when creating the defaultdict, and inside the for loop, increment each word_id of total_word_count by word_count.\n",
    "- Create a sorted list from the defaultdict, using words across the entire corpus. To achieve this, use the .items() method on total_word_count inside sorted().\n",
    "- Similar to how you printed the top five words of bow_doc earlier, print the top five words of sorted_word_count as well as the number of occurrences of each word across all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), \n",
    "                           key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "\n",
    "# output\n",
    "# engineering 91\n",
    "# '' 88\n",
    "# reverse 71\n",
    "# software 51\n",
    "# cite 26\n",
    "\n",
    "# '' 1042\n",
    "# computer 594\n",
    "# software 450\n",
    "# `` 345\n",
    "# cite 322"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Tf-idf with gensim\n",
    "What is tf-idf?\n",
    "- tf-idf = Term frequency - inverse document frequency\n",
    "- determines the most important words in each document in corpus\n",
    "- Each corpus may have shared word beyond just stopwords\n",
    "    - These word should be down-weighted in importance\n",
    "    - ie. example from astronomy: downgrade the word \"Sky\"\n",
    "- Ensures most common words don't show up as key words\n",
    "- keeps document specific frequent word weighted high\n",
    "\n",
    "Tf-idf formula\n",
    "- weight will be low if the term is not frequent in the document\n",
    "- weight will also be low if logarithm is close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidModel\n",
    "\n",
    "# use bag of words corpus and translate to TfidfModel\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# reference each document like a dictionary\n",
    "# displays [(token_id, token_weights)]\n",
    "tfidf[corpus[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.a What is tf-idf?\n",
    "You want to calculate the tf-idf weight for the word \"computer\", which appears five times in a document containing 100 words. Given a corpus containing 200 documents, with 20 documents mentioning the word \"computer\", tf-idf can be calculated by multiplying term frequency with inverse document frequency.\n",
    "\n",
    "- Term frequency = percentage share of the word compared to all tokens in the document \n",
    "- Inverse document frequency = logarithm of the total number of documents in a corpora divided by the number of documents containing the term\n",
    "\n",
    "Which of the below options is correct?\n",
    "- Answer: (5 / 100) * log(200 / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.b Tf-idf with Wikipedia\n",
    "Now it's your turn to determine new significant terms for your corpus by applying gensim's tf-idf. You will again have access to the same corpus and dictionary objects you created in the previous exercises - dictionary, corpus, and doc. Will tf-idf make for more interesting results on the document level?\n",
    "- Use doc to calculate the weights. You can do this by passing [doc] to tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfModel\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# output\n",
    "# [(24, 0.0022836332291091273), (39, 0.0043409401554717324), \n",
    "#  (41, 0.008681880310943465), (55, 0.011988285029371418), \n",
    "#  (56, 0.005482756770026296)]\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, \n",
    "                              key=lambda w: w[1], \n",
    "                              reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)\n",
    "    \n",
    "# output\n",
    "# reverse 0.4884961428651127\n",
    "# infringement 0.18674529210288995\n",
    "# engineering 0.16395041814479536\n",
    "# interoperability 0.12449686140192663\n",
    "# reverse-engineered 0.12449686140192663"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Named-entity recognition (NER)\n",
    "Objective:\n",
    "- learn how to identify the who, what and where of your texts using pre-trained models on English and non-English text\n",
    "\n",
    "Libraries: polyglot, spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Named Entity Recognition?\n",
    "- NLP task to identify important named entities in the text\n",
    "    - people, places, organizations\n",
    "    - dates, states, works of art\n",
    "    - ... other categories\n",
    "- Can be used alongside topic identification\n",
    "    - or on its own\n",
    "- Who? What? When? Where?\n",
    "\n",
    "nltk and the Stanford CoreNLP library\n",
    "- The Stanford CoreNLP library:\n",
    "    - integrated into Python via nltk\n",
    "    - Java based\n",
    "    - support for NER as well as conference and dependency trees\n",
    "- When using the Stanford library with NLTK, what is needed to get started?\n",
    "    - NLTK, the Stanford Java Libraries and some environment variables to help with integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Using nltk for Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T12:50:20.541289Z",
     "start_time": "2018-11-20T12:50:20.525086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "view tags for parts of speech [('In', 'IN'), ('New', 'NNP'), ('York', 'NNP')] \n",
      "\n",
      "(S\n",
      "  In/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  like/VBP\n",
      "  to/TO\n",
      "  reide/VB\n",
      "  the/DT\n",
      "  (ORGANIZATION Metro/NNP)\n",
      "  to/TO\n",
      "  visit/VB\n",
      "  (ORGANIZATION MOMA/NNP)\n",
      "  and/CC\n",
      "  some/DT\n",
      "  restaurants/NNS\n",
      "  rated/VBN\n",
      "  well/RB\n",
      "  by/IN\n",
      "  (PERSON Ruth/NNP Reichl/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = '''In New York, I like to reide the Metro to visit MOMA\n",
    "and some restaurants rated well by Ruth Reichl.'''\n",
    "\n",
    "# preprocessing: tokenization\n",
    "tokenized_sent = nltk.word_tokenize(sentence)\n",
    "# tag for parts of speech\n",
    "tagged_sent = nltk.pos_tag(tokenized_sent)\n",
    "# ie. NNP = proper noun singular\n",
    "print('view tags for parts of speech', tagged_sent[:3], '\\n')\n",
    "\n",
    "# nltk's ne_chunk() (aka named entity chunk)\n",
    "# Returns sentence as a tree representing grammar\n",
    "print(nltk.ne_chunk(tagged_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.a NER with NLTK\n",
    "You're now going to have some fun with named-entity recognition! A scraped news article has been pre-loaded into your workspace. Your task is to use nltk to find the named entities in this article.\n",
    "\n",
    "What might the article be about, given the names you found?\n",
    "\n",
    "Along with nltk, sent_tokenize and word_tokenize from nltk.tokenize have been pre-imported.\n",
    "\n",
    "Notes\n",
    "- Chunk each tagged sentence into named-entity chunks using nltk.ne_chunk_sents(). Along with pos_sentences, specify the additional keyword argument binary=True.\n",
    "- Loop over each sentence and each chunk, and test whether it is a named-entity chunk by testing if it has the attribute label, and if the chunk.label() is equal to \"NE\". If so, print that chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the article into sentences: sentences\n",
    "sentences = nltk.sent_tokenize(article)\n",
    "\n",
    "# Tokenize each sentence into words: token_sentences\n",
    "token_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "# Create the named entity chunks: chunked_sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "\n",
    "# Test for stems of the tree with 'NE' tags\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)\n",
    "\n",
    "# output\n",
    "# (NE Uber/NNP)\n",
    "# (NE Beyond/NN)\n",
    "# (NE Apple/NNP)\n",
    "# (NE Uber/NNP)\n",
    "# (NE Uber/NNP)\n",
    "# (NE Travis/NNP Kalanick/NNP)\n",
    "# (NE Tim/NNP Cook/NNP)\n",
    "# (NE Apple/NNP)\n",
    "# (NE Silicon/NNP Valley/NNP)\n",
    "# (NE CEO/NNP)\n",
    "# (NE Yahoo/NNP)\n",
    "# (NE Marissa/NNP Mayer/NNP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.b Charting practice\n",
    "In this exercise, you'll use some extracted named entities and their groupings from a series of newspaper articles to chart the diversity of named entity types in the articles.\n",
    "\n",
    "You'll use a defaultdict called ner_categories, with keys representing every named entity group type, and values to count the number of each different named entity type. You have a chunked sentence list called chunked_sentences similar to the last exercise, but this time with non-binary category names.\n",
    "\n",
    "You can use hasattr() to determine if each chunk has a 'label' and then simply use the chunk's .label() method as the dictionary key.\n",
    "\n",
    "Notes:\n",
    "- Fill up the dictionary with values for each of the keys. Remember, the keys will represent the label().\n",
    "    - In the outer for loop, iterate over chunked_sentences, using sent as your iterator variable.\n",
    "    - In the inner for loop, iterate over sent. If the condition is true, increment the value of each key by 1.\n",
    "- For the pie chart labels, create a list called labels from the keys of ner_categories, which can be accessed using .keys().\n",
    "- Use a list comprehension to create a list called values, using the .get() method on ner_categories to compute the values of each label l.\n",
    "- Use plt.pie() to create a pie chart for each of the NER categories. Along with values and labels=labels, pass the extra keyword arguments autopct='%1.1f%%' and startangle=140 to add percentages to the chart and rotate the initial start angle.\n",
    "- Display your pie chart. Was the distribution what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the defaultdict: ner_categories\n",
    "ner_categories = defaultdict(int)\n",
    "\n",
    "# Create the nested for loop\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_categories[chunk.label()] += 1\n",
    "            \n",
    "# Create a list from the dictionary keys for the chart labels: labels\n",
    "labels = list(ner_categories.keys())\n",
    "\n",
    "# Create a list of the values: values\n",
    "values = [ner_categories.get(l) for l in labels]\n",
    "\n",
    "# Create the pie chart\n",
    "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T13:13:48.447651Z",
     "start_time": "2018-11-20T13:13:48.435492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"399pt\" version=\"1.1\" viewBox=\"0 0 270 399\" width=\"270pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 399  L 270 399  L 270 0  L 0 0  z \" style=\"fill:#ffffff;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 61.030749 104.264588  C 60.735306 104.804835 60.442382 105.348312 60.151998 105.894978  C 59.861613 106.441644 59.573776 106.991483 59.288509 107.544456  L 135 199.5  L 61.030749 104.264588  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 59.288509 107.544456  C 49.831394 125.876462 43.33523 147.385556 40.332921 170.307274  C 37.330612 193.228992 37.906561 216.918931 42.013523 239.433726  L 135 199.5  L 59.288509 107.544456  z \" style=\"fill:#ff7f0e;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 42.013523 239.433726  C 42.137406 240.112869 42.264448 240.790646 42.39464 241.467006  C 42.524831 242.143365 42.658168 242.81829 42.794641 243.491728  L 135 199.5  L 42.013523 239.433726  z \" style=\"fill:#2ca02c;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 42.794641 243.491728  C 46.731933 262.92072 53.235711 280.898588 61.899553 296.301901  C 70.563396 311.705213 81.202998 324.206294 93.153492 333.023954  C 105.103985 341.841613 118.111148 346.788273 131.362165 347.554817  C 144.613183 348.321361 157.826164 344.891482 170.175434 337.479514  C 182.524704 330.067545 193.747556 318.831162 203.142675 304.472523  C 212.537795 290.113884 219.905319 272.938441 224.784854 254.019485  C 229.664389 235.100529 231.952131 214.840522 231.50512 194.505509  C 231.05811 174.170496 227.885855 154.193064 222.186591 135.821599  L 135 199.5  L 42.794641 243.491728  z \" style=\"fill:#d62728;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 222.186591 135.821599  C 215.236764 113.418971 204.729485 94.049777 191.632533 79.498026  C 178.535581 64.946275 163.263791 55.672882 147.223779 52.531977  C 131.183766 49.391072 114.883586 52.482142 99.824574 61.520481  C 84.765562 70.55882 71.424701 85.258147 61.030757 104.264572  L 135 199.5  L 222.186591 135.821599  z \" style=\"fill:#9467bd;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\"/>\n",
       "   <g id=\"matplotlib.axis_2\"/>\n",
       "   <g id=\"text_1\">\n",
       "    <!-- FACILITY -->\n",
       "    <defs>\n",
       "     <path d=\"M 9.8125 72.90625  L 51.703125 72.90625  L 51.703125 64.59375  L 19.671875 64.59375  L 19.671875 43.109375  L 48.578125 43.109375  L 48.578125 34.8125  L 19.671875 34.8125  L 19.671875 0  L 9.8125 0  z \" id=\"DejaVuSans-46\"/>\n",
       "     <path d=\"M 34.1875 63.1875  L 20.796875 26.90625  L 47.609375 26.90625  z M 28.609375 72.90625  L 39.796875 72.90625  L 67.578125 0  L 57.328125 0  L 50.6875 18.703125  L 17.828125 18.703125  L 11.1875 0  L 0.78125 0  z \" id=\"DejaVuSans-41\"/>\n",
       "     <path d=\"M 64.40625 67.28125  L 64.40625 56.890625  Q 59.421875 61.53125 53.78125 63.8125  Q 48.140625 66.109375 41.796875 66.109375  Q 29.296875 66.109375 22.65625 58.46875  Q 16.015625 50.828125 16.015625 36.375  Q 16.015625 21.96875 22.65625 14.328125  Q 29.296875 6.6875 41.796875 6.6875  Q 48.140625 6.6875 53.78125 8.984375  Q 59.421875 11.28125 64.40625 15.921875  L 64.40625 5.609375  Q 59.234375 2.09375 53.4375 0.328125  Q 47.65625 -1.421875 41.21875 -1.421875  Q 24.65625 -1.421875 15.125 8.703125  Q 5.609375 18.84375 5.609375 36.375  Q 5.609375 53.953125 15.125 64.078125  Q 24.65625 74.21875 41.21875 74.21875  Q 47.75 74.21875 53.53125 72.484375  Q 59.328125 70.75 64.40625 67.28125  z \" id=\"DejaVuSans-43\"/>\n",
       "     <path d=\"M 9.8125 72.90625  L 19.671875 72.90625  L 19.671875 0  L 9.8125 0  z \" id=\"DejaVuSans-49\"/>\n",
       "     <path d=\"M 9.8125 72.90625  L 19.671875 72.90625  L 19.671875 8.296875  L 55.171875 8.296875  L 55.171875 0  L 9.8125 0  z \" id=\"DejaVuSans-4c\"/>\n",
       "     <path d=\"M -0.296875 72.90625  L 61.375 72.90625  L 61.375 64.59375  L 35.5 64.59375  L 35.5 0  L 25.59375 0  L 25.59375 64.59375  L -0.296875 64.59375  z \" id=\"DejaVuSans-54\"/>\n",
       "     <path d=\"M -0.203125 72.90625  L 10.40625 72.90625  L 30.609375 42.921875  L 50.6875 72.90625  L 61.28125 72.90625  L 35.5 34.71875  L 35.5 0  L 25.59375 0  L 25.59375 34.71875  z \" id=\"DejaVuSans-59\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(9.420322 99.293851)scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-46\"/>\n",
       "     <use x=\"57.378906\" xlink:href=\"#DejaVuSans-41\"/>\n",
       "     <use x=\"125.771484\" xlink:href=\"#DejaVuSans-43\"/>\n",
       "     <use x=\"195.595703\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "     <use x=\"225.087891\" xlink:href=\"#DejaVuSans-4c\"/>\n",
       "     <use x=\"280.800781\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "     <use x=\"310.292969\" xlink:href=\"#DejaVuSans-54\"/>\n",
       "     <use x=\"371.376953\" xlink:href=\"#DejaVuSans-59\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_2\">\n",
       "    <!-- 0.5% -->\n",
       "    <defs>\n",
       "     <path d=\"M 31.78125 66.40625  Q 24.171875 66.40625 20.328125 58.90625  Q 16.5 51.421875 16.5 36.375  Q 16.5 21.390625 20.328125 13.890625  Q 24.171875 6.390625 31.78125 6.390625  Q 39.453125 6.390625 43.28125 13.890625  Q 47.125 21.390625 47.125 36.375  Q 47.125 51.421875 43.28125 58.90625  Q 39.453125 66.40625 31.78125 66.40625  z M 31.78125 74.21875  Q 44.046875 74.21875 50.515625 64.515625  Q 56.984375 54.828125 56.984375 36.375  Q 56.984375 17.96875 50.515625 8.265625  Q 44.046875 -1.421875 31.78125 -1.421875  Q 19.53125 -1.421875 13.0625 8.265625  Q 6.59375 17.96875 6.59375 36.375  Q 6.59375 54.828125 13.0625 64.515625  Q 19.53125 74.21875 31.78125 74.21875  z \" id=\"DejaVuSans-30\"/>\n",
       "     <path d=\"M 10.6875 12.40625  L 21 12.40625  L 21 0  L 10.6875 0  z \" id=\"DejaVuSans-2e\"/>\n",
       "     <path d=\"M 10.796875 72.90625  L 49.515625 72.90625  L 49.515625 64.59375  L 19.828125 64.59375  L 19.828125 46.734375  Q 21.96875 47.46875 24.109375 47.828125  Q 26.265625 48.1875 28.421875 48.1875  Q 40.625 48.1875 47.75 41.5  Q 54.890625 34.8125 54.890625 23.390625  Q 54.890625 11.625 47.5625 5.09375  Q 40.234375 -1.421875 26.90625 -1.421875  Q 22.3125 -1.421875 17.546875 -0.640625  Q 12.796875 0.140625 7.71875 1.703125  L 7.71875 11.625  Q 12.109375 9.234375 16.796875 8.0625  Q 21.484375 6.890625 26.703125 6.890625  Q 35.15625 6.890625 40.078125 11.328125  Q 45.015625 15.765625 45.015625 23.390625  Q 45.015625 31 40.078125 35.4375  Q 35.15625 39.890625 26.703125 39.890625  Q 22.75 39.890625 18.8125 39.015625  Q 14.890625 38.140625 10.796875 36.28125  z \" id=\"DejaVuSans-35\"/>\n",
       "     <path d=\"M 72.703125 32.078125  Q 68.453125 32.078125 66.03125 28.46875  Q 63.625 24.859375 63.625 18.40625  Q 63.625 12.0625 66.03125 8.421875  Q 68.453125 4.78125 72.703125 4.78125  Q 76.859375 4.78125 79.265625 8.421875  Q 81.6875 12.0625 81.6875 18.40625  Q 81.6875 24.8125 79.265625 28.4375  Q 76.859375 32.078125 72.703125 32.078125  z M 72.703125 38.28125  Q 80.421875 38.28125 84.953125 32.90625  Q 89.5 27.546875 89.5 18.40625  Q 89.5 9.28125 84.9375 3.921875  Q 80.375 -1.421875 72.703125 -1.421875  Q 64.890625 -1.421875 60.34375 3.921875  Q 55.8125 9.28125 55.8125 18.40625  Q 55.8125 27.59375 60.375 32.9375  Q 64.9375 38.28125 72.703125 38.28125  z M 22.3125 68.015625  Q 18.109375 68.015625 15.6875 64.375  Q 13.28125 60.75 13.28125 54.390625  Q 13.28125 47.953125 15.671875 44.328125  Q 18.0625 40.71875 22.3125 40.71875  Q 26.5625 40.71875 28.96875 44.328125  Q 31.390625 47.953125 31.390625 54.390625  Q 31.390625 60.6875 28.953125 64.34375  Q 26.515625 68.015625 22.3125 68.015625  z M 66.40625 74.21875  L 74.21875 74.21875  L 28.609375 -1.421875  L 20.796875 -1.421875  z M 22.3125 74.21875  Q 30.03125 74.21875 34.609375 68.875  Q 39.203125 63.53125 39.203125 54.390625  Q 39.203125 45.171875 34.640625 39.84375  Q 30.078125 34.515625 22.3125 34.515625  Q 14.546875 34.515625 10.03125 39.859375  Q 5.515625 45.21875 5.515625 54.390625  Q 5.515625 63.484375 10.046875 68.84375  Q 14.59375 74.21875 22.3125 74.21875  z \" id=\"DejaVuSans-25\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(77.388855 146.096362)scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n",
       "     <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n",
       "     <use x=\"159.033203\" xlink:href=\"#DejaVuSans-25\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_3\">\n",
       "    <!-- ORGANIZATION -->\n",
       "    <defs>\n",
       "     <path d=\"M 39.40625 66.21875  Q 28.65625 66.21875 22.328125 58.203125  Q 16.015625 50.203125 16.015625 36.375  Q 16.015625 22.609375 22.328125 14.59375  Q 28.65625 6.59375 39.40625 6.59375  Q 50.140625 6.59375 56.421875 14.59375  Q 62.703125 22.609375 62.703125 36.375  Q 62.703125 50.203125 56.421875 58.203125  Q 50.140625 66.21875 39.40625 66.21875  z M 39.40625 74.21875  Q 54.734375 74.21875 63.90625 63.9375  Q 73.09375 53.65625 73.09375 36.375  Q 73.09375 19.140625 63.90625 8.859375  Q 54.734375 -1.421875 39.40625 -1.421875  Q 24.03125 -1.421875 14.8125 8.828125  Q 5.609375 19.09375 5.609375 36.375  Q 5.609375 53.65625 14.8125 63.9375  Q 24.03125 74.21875 39.40625 74.21875  z \" id=\"DejaVuSans-4f\"/>\n",
       "     <path d=\"M 44.390625 34.1875  Q 47.5625 33.109375 50.5625 29.59375  Q 53.5625 26.078125 56.59375 19.921875  L 66.609375 0  L 56 0  L 46.6875 18.703125  Q 43.0625 26.03125 39.671875 28.421875  Q 36.28125 30.8125 30.421875 30.8125  L 19.671875 30.8125  L 19.671875 0  L 9.8125 0  L 9.8125 72.90625  L 32.078125 72.90625  Q 44.578125 72.90625 50.734375 67.671875  Q 56.890625 62.453125 56.890625 51.90625  Q 56.890625 45.015625 53.6875 40.46875  Q 50.484375 35.9375 44.390625 34.1875  z M 19.671875 64.796875  L 19.671875 38.921875  L 32.078125 38.921875  Q 39.203125 38.921875 42.84375 42.21875  Q 46.484375 45.515625 46.484375 51.90625  Q 46.484375 58.296875 42.84375 61.546875  Q 39.203125 64.796875 32.078125 64.796875  z \" id=\"DejaVuSans-52\"/>\n",
       "     <path d=\"M 59.515625 10.40625  L 59.515625 29.984375  L 43.40625 29.984375  L 43.40625 38.09375  L 69.28125 38.09375  L 69.28125 6.78125  Q 63.578125 2.734375 56.6875 0.65625  Q 49.8125 -1.421875 42 -1.421875  Q 24.90625 -1.421875 15.25 8.5625  Q 5.609375 18.5625 5.609375 36.375  Q 5.609375 54.25 15.25 64.234375  Q 24.90625 74.21875 42 74.21875  Q 49.125 74.21875 55.546875 72.453125  Q 61.96875 70.703125 67.390625 67.28125  L 67.390625 56.78125  Q 61.921875 61.421875 55.765625 63.765625  Q 49.609375 66.109375 42.828125 66.109375  Q 29.4375 66.109375 22.71875 58.640625  Q 16.015625 51.171875 16.015625 36.375  Q 16.015625 21.625 22.71875 14.15625  Q 29.4375 6.6875 42.828125 6.6875  Q 48.046875 6.6875 52.140625 7.59375  Q 56.25 8.5 59.515625 10.40625  z \" id=\"DejaVuSans-47\"/>\n",
       "     <path d=\"M 9.8125 72.90625  L 23.09375 72.90625  L 55.421875 11.921875  L 55.421875 72.90625  L 64.984375 72.90625  L 64.984375 0  L 51.703125 0  L 19.390625 60.984375  L 19.390625 0  L 9.8125 0  z \" id=\"DejaVuSans-4e\"/>\n",
       "     <path d=\"M 5.609375 72.90625  L 62.890625 72.90625  L 62.890625 65.375  L 16.796875 8.296875  L 64.015625 8.296875  L 64.015625 0  L 4.5 0  L 4.5 7.515625  L 50.59375 64.59375  L 5.609375 64.59375  z \" id=\"DejaVuSans-5a\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(-47.065037 170.147376)scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-4f\"/>\n",
       "     <use x=\"78.710938\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "     <use x=\"148.193359\" xlink:href=\"#DejaVuSans-47\"/>\n",
       "     <use x=\"225.683594\" xlink:href=\"#DejaVuSans-41\"/>\n",
       "     <use x=\"294.091797\" xlink:href=\"#DejaVuSans-4e\"/>\n",
       "     <use x=\"368.896484\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "     <use x=\"398.388672\" xlink:href=\"#DejaVuSans-5a\"/>\n",
       "     <use x=\"466.894531\" xlink:href=\"#DejaVuSans-41\"/>\n",
       "     <use x=\"535.193359\" xlink:href=\"#DejaVuSans-54\"/>\n",
       "     <use x=\"596.277344\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "     <use x=\"625.769531\" xlink:href=\"#DejaVuSans-4f\"/>\n",
       "     <use x=\"704.480469\" xlink:href=\"#DejaVuSans-4e\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_4\">\n",
       "    <!-- 15.0% -->\n",
       "    <defs>\n",
       "     <path d=\"M 12.40625 8.296875  L 28.515625 8.296875  L 28.515625 63.921875  L 10.984375 60.40625  L 10.984375 69.390625  L 28.421875 72.90625  L 38.28125 72.90625  L 38.28125 8.296875  L 54.390625 8.296875  L 54.390625 0  L 12.40625 0  z \" id=\"DejaVuSans-31\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(62.316159 184.743739)scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n",
       "     <use x=\"127.246094\" xlink:href=\"#DejaVuSans-2e\"/>\n",
       "     <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "     <use x=\"222.65625\" xlink:href=\"#DejaVuSans-25\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_5\">\n",
       "    <!-- LOCATION -->\n",
       "    <g transform=\"translate(-18.528396 248.423081)scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-4c\"/>\n",
       "     <use x=\"55.666016\" xlink:href=\"#DejaVuSans-4f\"/>\n",
       "     <use x=\"134.376953\" xlink:href=\"#DejaVuSans-43\"/>\n",
       "     <use x=\"204.201172\" xlink:href=\"#DejaVuSans-41\"/>\n",
       "     <use x=\"272.5\" xlink:href=\"#DejaVuSans-54\"/>\n",
       "     <use x=\"333.583984\" xlink:href=\"#DejaVuSans-49\"/>\n",
       "     <use x=\"363.076172\" xlink:href=\"#DejaVuSans-4f\"/>\n",
       "     <use x=\"441.787109\" xlink:href=\"#DejaVuSans-4e\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_6\">\n",
       "    <!-- 0.5% -->\n",
       "    <g transform=\"translate(66.73444 227.439578)scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n",
       "     <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n",
       "     <use x=\"159.033203\" xlink:href=\"#DejaVuSans-25\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_7\">\n",
       "    <!-- GPE -->\n",
       "    <defs>\n",
       "     <path d=\"M 19.671875 64.796875  L 19.671875 37.40625  L 32.078125 37.40625  Q 38.96875 37.40625 42.71875 40.96875  Q 46.484375 44.53125 46.484375 51.125  Q 46.484375 57.671875 42.71875 61.234375  Q 38.96875 64.796875 32.078125 64.796875  z M 9.8125 72.90625  L 32.078125 72.90625  Q 44.34375 72.90625 50.609375 67.359375  Q 56.890625 61.8125 56.890625 51.125  Q 56.890625 40.328125 50.609375 34.8125  Q 44.34375 29.296875 32.078125 29.296875  L 19.671875 29.296875  L 19.671875 0  L 9.8125 0  z \" id=\"DejaVuSans-50\"/>\n",
       "     <path d=\"M 9.8125 72.90625  L 55.90625 72.90625  L 55.90625 64.59375  L 19.671875 64.59375  L 19.671875 43.015625  L 54.390625 43.015625  L 54.390625 34.71875  L 19.671875 34.71875  L 19.671875 8.296875  L 56.78125 8.296875  L 56.78125 0  L 9.8125 0  z \" id=\"DejaVuSans-45\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(173.692978 354.03684)scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-47\"/>\n",
       "     <use x=\"77.490234\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "     <use x=\"137.792969\" xlink:href=\"#DejaVuSans-45\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_8\">\n",
       "    <!-- 52.3% -->\n",
       "    <defs>\n",
       "     <path d=\"M 19.1875 8.296875  L 53.609375 8.296875  L 53.609375 0  L 7.328125 0  L 7.328125 8.296875  Q 12.9375 14.109375 22.625 23.890625  Q 32.328125 33.6875 34.8125 36.53125  Q 39.546875 41.84375 41.421875 45.53125  Q 43.3125 49.21875 43.3125 52.78125  Q 43.3125 58.59375 39.234375 62.25  Q 35.15625 65.921875 28.609375 65.921875  Q 23.96875 65.921875 18.8125 64.3125  Q 13.671875 62.703125 7.8125 59.421875  L 7.8125 69.390625  Q 13.765625 71.78125 18.9375 73  Q 24.125 74.21875 28.421875 74.21875  Q 39.75 74.21875 46.484375 68.546875  Q 53.21875 62.890625 53.21875 53.421875  Q 53.21875 48.921875 51.53125 44.890625  Q 49.859375 40.875 45.40625 35.40625  Q 44.1875 33.984375 37.640625 27.21875  Q 31.109375 20.453125 19.1875 8.296875  z \" id=\"DejaVuSans-32\"/>\n",
       "     <path d=\"M 40.578125 39.3125  Q 47.65625 37.796875 51.625 33  Q 55.609375 28.21875 55.609375 21.1875  Q 55.609375 10.40625 48.1875 4.484375  Q 40.765625 -1.421875 27.09375 -1.421875  Q 22.515625 -1.421875 17.65625 -0.515625  Q 12.796875 0.390625 7.625 2.203125  L 7.625 11.71875  Q 11.71875 9.328125 16.59375 8.109375  Q 21.484375 6.890625 26.8125 6.890625  Q 36.078125 6.890625 40.9375 10.546875  Q 45.796875 14.203125 45.796875 21.1875  Q 45.796875 27.640625 41.28125 31.265625  Q 36.765625 34.90625 28.71875 34.90625  L 20.21875 34.90625  L 20.21875 43.015625  L 29.109375 43.015625  Q 36.375 43.015625 40.234375 45.921875  Q 44.09375 48.828125 44.09375 54.296875  Q 44.09375 59.90625 40.109375 62.90625  Q 36.140625 65.921875 28.71875 65.921875  Q 24.65625 65.921875 20.015625 65.03125  Q 15.375 64.15625 9.8125 62.3125  L 9.8125 71.09375  Q 15.4375 72.65625 20.34375 73.4375  Q 25.25 74.21875 29.59375 74.21875  Q 40.828125 74.21875 47.359375 69.109375  Q 53.90625 64.015625 53.90625 55.328125  Q 53.90625 49.265625 50.4375 45.09375  Q 46.96875 40.921875 40.578125 39.3125  z \" id=\"DejaVuSans-33\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(140.221667 285.047083)scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "     <use x=\"127.246094\" xlink:href=\"#DejaVuSans-2e\"/>\n",
       "     <use x=\"159.033203\" xlink:href=\"#DejaVuSans-33\"/>\n",
       "     <use x=\"222.65625\" xlink:href=\"#DejaVuSans-25\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_9\">\n",
       "    <!-- PERSON -->\n",
       "    <defs>\n",
       "     <path d=\"M 53.515625 70.515625  L 53.515625 60.890625  Q 47.90625 63.578125 42.921875 64.890625  Q 37.9375 66.21875 33.296875 66.21875  Q 25.25 66.21875 20.875 63.09375  Q 16.5 59.96875 16.5 54.203125  Q 16.5 49.359375 19.40625 46.890625  Q 22.3125 44.4375 30.421875 42.921875  L 36.375 41.703125  Q 47.40625 39.59375 52.65625 34.296875  Q 57.90625 29 57.90625 20.125  Q 57.90625 9.515625 50.796875 4.046875  Q 43.703125 -1.421875 29.984375 -1.421875  Q 24.8125 -1.421875 18.96875 -0.25  Q 13.140625 0.921875 6.890625 3.21875  L 6.890625 13.375  Q 12.890625 10.015625 18.65625 8.296875  Q 24.421875 6.59375 29.984375 6.59375  Q 38.421875 6.59375 43.015625 9.90625  Q 47.609375 13.234375 47.609375 19.390625  Q 47.609375 24.75 44.3125 27.78125  Q 41.015625 30.8125 33.5 32.328125  L 27.484375 33.5  Q 16.453125 35.6875 11.515625 40.375  Q 6.59375 45.0625 6.59375 53.421875  Q 6.59375 63.09375 13.40625 68.65625  Q 20.21875 74.21875 32.171875 74.21875  Q 37.3125 74.21875 42.625 73.28125  Q 47.953125 72.359375 53.515625 70.515625  z \" id=\"DejaVuSans-53\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(148.446156 40.59455)scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "     <use x=\"60.302734\" xlink:href=\"#DejaVuSans-45\"/>\n",
       "     <use x=\"123.486328\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "     <use x=\"192.96875\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "     <use x=\"256.445312\" xlink:href=\"#DejaVuSans-4f\"/>\n",
       "     <use x=\"335.15625\" xlink:href=\"#DejaVuSans-4e\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_10\">\n",
       "    <!-- 31.8% -->\n",
       "    <defs>\n",
       "     <path d=\"M 31.78125 34.625  Q 24.75 34.625 20.71875 30.859375  Q 16.703125 27.09375 16.703125 20.515625  Q 16.703125 13.921875 20.71875 10.15625  Q 24.75 6.390625 31.78125 6.390625  Q 38.8125 6.390625 42.859375 10.171875  Q 46.921875 13.96875 46.921875 20.515625  Q 46.921875 27.09375 42.890625 30.859375  Q 38.875 34.625 31.78125 34.625  z M 21.921875 38.8125  Q 15.578125 40.375 12.03125 44.71875  Q 8.5 49.078125 8.5 55.328125  Q 8.5 64.0625 14.71875 69.140625  Q 20.953125 74.21875 31.78125 74.21875  Q 42.671875 74.21875 48.875 69.140625  Q 55.078125 64.0625 55.078125 55.328125  Q 55.078125 49.078125 51.53125 44.71875  Q 48 40.375 41.703125 38.8125  Q 48.828125 37.15625 52.796875 32.3125  Q 56.78125 27.484375 56.78125 20.515625  Q 56.78125 9.90625 50.3125 4.234375  Q 43.84375 -1.421875 31.78125 -1.421875  Q 19.734375 -1.421875 13.25 4.234375  Q 6.78125 9.90625 6.78125 20.515625  Q 6.78125 27.484375 10.78125 32.3125  Q 14.796875 37.15625 21.921875 38.8125  z M 18.3125 54.390625  Q 18.3125 48.734375 21.84375 45.5625  Q 25.390625 42.390625 31.78125 42.390625  Q 38.140625 42.390625 41.71875 45.5625  Q 45.3125 48.734375 45.3125 54.390625  Q 45.3125 60.0625 41.71875 63.234375  Q 38.140625 66.40625 31.78125 66.40625  Q 25.390625 66.40625 21.84375 63.234375  Q 18.3125 60.0625 18.3125 54.390625  z \" id=\"DejaVuSans-38\"/>\n",
       "    </defs>\n",
       "    <g transform=\"translate(126.450673 114.078561)scale(0.1 -0.1)\">\n",
       "     <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-31\"/>\n",
       "     <use x=\"127.246094\" xlink:href=\"#DejaVuSans-2e\"/>\n",
       "     <use x=\"159.033203\" xlink:href=\"#DejaVuSans-38\"/>\n",
       "     <use x=\"222.65625\" xlink:href=\"#DejaVuSans-25\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(filename='Images/nlp_ne_piechart.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 ScaCy\n",
    "What is SpaCy?\n",
    "- NLP library similar to gensim, with different implementations\n",
    "- focus on creating NLP pipelines to generate model and corpora\n",
    "- open-source, with extra libraries and tools\n",
    "    - displaCy\n",
    "        - entity recognition visualization tool to view parse trees \n",
    "        - which uses Node.js to create interactive text\n",
    "- also support German and Chinese languages\n",
    "\n",
    "Why use SpaCy for NER?\n",
    "- easy pipeline creation\n",
    "- different entity types compared to nltk\n",
    "- informal language corpora\n",
    "    - easily find entities in Tweets and chat messages\n",
    "- Quickly growing - so support may increase\n",
    "\n",
    "Download notes in Bash for MacOS:\n",
    "- conda install spacy\n",
    "- sudo python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.a SpaCy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T23:34:17.353929Z",
     "start_time": "2018-11-20T23:34:16.593838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.pipeline.EntityRecognizer object at 0x1a1e095728>\n",
      "(Berlin, Germany, \n",
      ", Angela Merkel)\n",
      "Berlin GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# nlp has several linked objects including entity\n",
    "# for entity recognition in the text\n",
    "print(nlp.entity)\n",
    "\n",
    "# load new document\n",
    "doc = nlp(\"\"\"Berlin is the capital of Germany;\n",
    "and the residence of Chancellor Angela Merkel.\"\"\")\n",
    "\n",
    "# named entities are stored in .ents\n",
    "print(doc.ents)\n",
    "\n",
    "# check out each label (.label_) using indexing\n",
    "print(doc.ents[0], doc.ents[0].label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.b Comparing NLTK with spaCy NER\n",
    "Using the same text you used in the first exercise of this chapter, you'll now see the results using spaCy's NER annotator. How will they compare?\n",
    "\n",
    "The article has been pre-loaded as article. To minimize execution times, you'll be asked to specify the keyword arguments tagger=False, parser=False, matcher=False when loading the spaCy model, because you only care about the entity in this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "# Instantiate the English model: nlp\n",
    "# Additional args to improve execution time\n",
    "nlp = spacy.load('en', tagger=False, parser=False, \n",
    "                 matcher=False)\n",
    "\n",
    "# Create a new spacy document object: doc\n",
    "doc = nlp(article)\n",
    "\n",
    "# Print all of the found entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "    ORG Uber\n",
    "    ORG Uber\n",
    "    ORG Apple\n",
    "    ORG Uber\n",
    "    ORG Uber\n",
    "    PERSON Travis Kalanick\n",
    "    ORG Uber\n",
    "    PERSON Tim Cook\n",
    "    ORG Apple\n",
    "    CARDINAL Millions\n",
    "    ORG Uber\n",
    "    GPE drivers’\n",
    "    LOC Silicon Valley’s\n",
    "    ORG Yahoo\n",
    "    PERSON Marissa Mayer\n",
    "    MONEY $186m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.c spaCy NER Categories\n",
    "Which are the extra categories that spacy uses compared to nltk in its named-entity recognition?\n",
    "\n",
    "Answer: NORP, CARDINAL, MONEY, WORKOFART, LANGUAGE, EVENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Multilingual NER with polyglot\n",
    "What is polyglot?\n",
    "- NLP library which uses word vectors for simple tasks (ie NER)\n",
    "- Why polyglot?\n",
    "    - vectors for many different languages\n",
    "    - 130+ languages\n",
    "    - transliteration = translate text by swapping characters\n",
    "- other features\n",
    "    - uses language detection, so don't need to specify language\n",
    "    \n",
    "Installation - polyglot\n",
    "- pip install polyglot\n",
    "    - if issues, click [here](https://hackernoon.com/install-polyglot-on-mac-3c90445abc1f)\n",
    "    - install icu with directions [here](https://anaconda.org/conda-forge/icu)\n",
    "    - conda install -c conda-forge pyicu\n",
    "- can't get pycld2 running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.a Spanish NER with polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-25T17:10:32.300558Z",
     "start_time": "2018-11-25T17:10:32.180332Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycld2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e6229a3d5150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m text = \"\"\"El presidente de la Generalitat de Cataluña,\n\u001b[1;32m      4\u001b[0m \u001b[0mCarles\u001b[0m \u001b[0mPuigdemont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mha\u001b[0m \u001b[0mafirmado\u001b[0m \u001b[0mhoy\u001b[0m \u001b[0ma\u001b[0m \u001b[0mla\u001b[0m \u001b[0malcaldesa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mde\u001b[0m \u001b[0mMadrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mManuela\u001b[0m \u001b[0mCarmena\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mque\u001b[0m \u001b[0men\u001b[0m \u001b[0msu\u001b[0m \u001b[0metapa\u001b[0m \u001b[0mde\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/polyglot/text.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcached_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/polyglot/detect/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Detector'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Language'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/polyglot/detect/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0micu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLocale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpycld2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcld2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycld2'"
     ]
    }
   ],
   "source": [
    "from polyglot.text import Text\n",
    "\n",
    "text = \"\"\"El presidente de la Generalitat de Cataluña,\n",
    "Carles Puigdemont, ha afirmado hoy a la alcaldesa\n",
    "de Madrid, Manuela Carmena, que en su etapa de\n",
    "alcalde de Girona (de julio de 2011 a enero de 2016\n",
    "hizo una gran promoción de Madrid.\"\"\"\n",
    "\n",
    "ptext = Text(text)\n",
    "\n",
    "# view entity chunks\n",
    "# I-ORG = organization, I-LOC = location, I-PER = person\n",
    "ptext.entities\n",
    "\n",
    "# may need to clean up entities when they don't match \n",
    "# expectations or you don't want to track them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.b French NER with polyglot I\n",
    "In this exercise and the next, you'll use the polyglot library to identify French entities. The library functions slightly differently than spacy, so you'll use a few of the new things you learned in the last video to display the named entity text and category.\n",
    "\n",
    "You have access to the full article string in article. Additionally, the Text class of polyglot has been imported from polyglot.text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new text object using Polyglot's Text class: txt\n",
    "txt = Text(article)\n",
    "\n",
    "# Print each of the entities found\n",
    "for ent in txt.entities:\n",
    "    print(ent)\n",
    "    \n",
    "# Print the type of ent\n",
    "print(type(ent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "    ['Charles', 'Cuvelliez']\n",
    "    ['Charles', 'Cuvelliez']\n",
    "    ['Bruxelles']\n",
    "    ['l’IA']\n",
    "    ['Julien', 'Maldonato']\n",
    "    ['Deloitte']\n",
    "    ['Ethiquement']\n",
    "    ['l’IA']\n",
    "    ['.']\n",
    "    <class 'polyglot.text.Chunk'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.c French NER with polyglot II\n",
    "Here, you'll complete the work you began in the previous exercise.\n",
    "\n",
    "Your task is to use a list comprehension to create a list of tuples, in which the first element is the entity tag, and the second element is the full string of the entity text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of tuples: entities\n",
    "entities = [(ent.tag, ' '.join(ent)) for ent in txt.entities]\n",
    "\n",
    "# Print entities\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.d Spanish NER with polyglot - using blog-like texts\n",
    "You'll continue your exploration of polyglot now with some Spanish annotation. This article is not written by a newspaper, so it is your first example of a more blog-like text. How do you think that might compare when finding entities?\n",
    "\n",
    "The Text object has been created as txt, and each entity has been printed, as you can see in the IPython Shell.\n",
    "\n",
    "Your specific task is to determine how many of the entities contain the words \"Márquez\" or \"Gabo\" - these refer to the same person in different ways!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the count variable: count\n",
    "count = 0\n",
    "\n",
    "# Iterate over all the entities\n",
    "for ent in txt.entities:\n",
    "    # Check whether the entity contains 'Márquez' or 'Gabo'\n",
    "    if \"Márquez\" in ent or \"Gabo\" in ent:\n",
    "        # Increment count\n",
    "        count += 1\n",
    "\n",
    "# Print count\n",
    "print(count)\n",
    "\n",
    "# Calculate the percentage of entities that refer to \"Gabo\": percentage\n",
    "percentage = count / len(txt.entities)\n",
    "print(percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "    29\n",
    "    0.29591836734693877"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Building a \"fake news\" classifier\n",
    "- apply lessons to build a supervised ML model to detect \"fake news\"\n",
    "    - choose a few important features and test ideas to identify and classify \"fake news\" articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Supervised learning with NLP\n",
    "How to create supervised learning data from text\n",
    "- use bag-of-words models or tf-idf as features\n",
    "\n",
    "Example\n",
    "- IMDB Movie Dataset\n",
    "- Goal: Predict movie genre based on plot summary (text)\n",
    "- Categorical features generated using preprocessing\n",
    "- Target label: Sci-Fi and Action are 0 or 1, opposite genres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Predicting movie genre\n",
    "- Goal: Create bag-of-word vectors for the movie plots\n",
    "    - can we predict genre based on the words used in the plot summary?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Count Vectorizer with Python\n",
    "- bag-of-words vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-21T23:22:52.883012Z",
     "start_time": "2018-11-21T23:22:52.037617Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# load data into a dataframe\n",
    "df = ...\n",
    "# set target label\n",
    "y = df['Sci-Fi']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "df['plot'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# create a count vectorizer and remove stop_words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "# create bag-of-words vectors on train/test sets\n",
    "# fit_transform will create a bag-of-words dictionary \n",
    "#  and vectors for each document\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "# note: if you have unknown words in test set only, may\n",
    "#  need more data or remove those words from the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 CountVectorizer for text classification\n",
    "- simple supervised model using a sparse text vectorizer\n",
    "\n",
    "It's time to begin building your text classifier! The [data](https://s3.amazonaws.com/assets.datacamp.com/production/course_3629/fake_or_real_news.csv) has been loaded into a DataFrame called df. Explore it in the IPython Shell to investigate what columns you can use. The .head() method is particularly informative.\n",
    "\n",
    "In this exercise, you'll use pandas alongside scikit-learn to create a sparse text vectorizer you can use to train and test a simple supervised model. To begin, you'll set up a CountVectorizer and investigate some of its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df['label']\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the training data using only the 'text' column \n",
    "#  values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data using only the 'text' column values:\n",
    "#  count_test \n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "       Unnamed: 0                                              title  \\\n",
    "    0        8476                       You Can Smell Hillary’s Fear   \n",
    "    1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
    "    2        3608        Kerry to go to Paris in gesture of sympathy   \n",
    "    3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
    "    4         875   The Battle of New York: Why This Primary Matters   \n",
    "    \n",
    "                                                    text label  \n",
    "    0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
    "    1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
    "    2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
    "    3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
    "    4  It's primary day in New York and front-runners...  REAL  \n",
    "    ['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 TfidfVectorizer for text classification\n",
    "Similar to the sparse CountVectorizer created in the previous exercise, you'll work on creating tf-idf vectors for your documents. You'll set up a TfidfVectorizer and investigate some of its features.\n",
    "\n",
    "In this exercise, you'll use pandas and sklearn along with the same X_train, y_train and X_test, y_test DataFrames and Series you created in the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                                   max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "- ['00', '000', '0000', '000billion', '000ft', '004', '006s', '0099', '00am', '01']\n",
    "-     [[0. 0. 0. ... 0. 0. 0.]\n",
    "-     [0. 0. 0. ... 0. 0. 0.]\n",
    "-     [0. 0. 0. ... 0. 0. 0.]\n",
    "-     [0. 0. 0. ... 0. 0. 0.]\n",
    "-     [0. 0. 0. ... 0. 0. 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.a Inspecting the vectors\n",
    "To get a better idea of how the vectors work, you'll investigate them by converting them into pandas DataFrames.\n",
    "\n",
    "Here, you'll use the same data structures you created in the previous two exercises (count_train, count_vectorizer, tfidf_train, tfidf_vectorizer) as well as pandas, which is imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, \n",
    "                        columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, \n",
    "                        columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "       00  000  02  03  06  10  100  1040s  11  11pm   ...    yves  zabadani  \\\n",
    "    0   0    0   0   0   0   0    0      0   0     0   ...       0         0   \n",
    "    1   0    0   0   0   0   0    0      0   0     0   ...       0         0   \n",
    "    2   0    0   0   0   0   0    0      0   0     0   ...       0         0   \n",
    "    3   0    0   0   0   0   0    0      0   0     0   ...       0         0   \n",
    "    4   0    0   0   0   0   0    1      0   0     0   ...       0         0   \n",
    "    \n",
    "       zaniest  zany  zelaya  zero  zone  zoom  zooming  zuesse  \n",
    "    0        0     0       0     0     0     0        0       0  \n",
    "    1        0     0       0     0     0     0        0       0  \n",
    "    2        0     0       0     0     0     0        0       0  \n",
    "    3        0     0       0     0     0     0        0       1  \n",
    "    4        0     0       0     3     0     0        0       0  \n",
    "    \n",
    "    [5 rows x 6446 columns]\n",
    "        00  000   02   03   06   10       100  1040s   11  11pm    ...     yves  \\\n",
    "    0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000    0.0  0.0   0.0    ...      0.0   \n",
    "    1  0.0  0.0  0.0  0.0  0.0  0.0  0.000000    0.0  0.0   0.0    ...      0.0   \n",
    "    2  0.0  0.0  0.0  0.0  0.0  0.0  0.000000    0.0  0.0   0.0    ...      0.0   \n",
    "    3  0.0  0.0  0.0  0.0  0.0  0.0  0.000000    0.0  0.0   0.0    ...      0.0   \n",
    "    4  0.0  0.0  0.0  0.0  0.0  0.0  0.039568    0.0  0.0   0.0    ...      0.0   \n",
    "    \n",
    "       zabadani  zaniest  zany  zelaya      zero  zone  zoom  zooming    zuesse  \n",
    "    0       0.0      0.0   0.0     0.0  0.000000   0.0   0.0      0.0  0.000000  \n",
    "    1       0.0      0.0   0.0     0.0  0.000000   0.0   0.0      0.0  0.000000  \n",
    "    2       0.0      0.0   0.0     0.0  0.000000   0.0   0.0      0.0  0.000000  \n",
    "    3       0.0      0.0   0.0     0.0  0.000000   0.0   0.0      0.0  0.046812  \n",
    "    4       0.0      0.0   0.0     0.0  0.137944   0.0   0.0      0.0  0.000000  \n",
    "    \n",
    "    [5 rows x 6445 columns]\n",
    "    {'time'}\n",
    "    False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Training and Testing a classification model with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.a Naive Bayes Classifier\n",
    "Naive Bayes Model\n",
    "- commonly used for testing NLP classification problems\n",
    "- basis in probability\n",
    "\n",
    "Question: Given a particular piece of data, how likely is a particular outcome?\n",
    "\n",
    "Examples:\n",
    "- If a movie plot has a spaceship, how likely is it sci-fi?\n",
    "- Given a spaceship AND an alien, how likely NOW is it sci-fi?\n",
    "\n",
    "Each word from CountVectorizer acts as a feature\n",
    "\n",
    "Naive Bayes - simple and effective\n",
    "- not always the best\n",
    "- sklearn's MultinomialNB works well with CountVectorizer\n",
    "    - this model works well with integers\n",
    "    - would NOT work well with float type like tf-idf weighted inputs - linear models and SVM may be better\n",
    "    \n",
    "Confusion matrix\n",
    "- if labels aren't passed, it will use Python ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T05:29:00.013077Z",
     "start_time": "2018-11-22T05:29:00.004846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Naive Bayes - MultinomialNB works well with CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "pred = nb_classifier.predict(count_test)\n",
    "metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "# confusion matrix\n",
    "metrics.confusion_matrix(y_test, pred, labels=[0,1])\n",
    "# output\n",
    "# array([[6410, 563],\n",
    "#        [864,  2242]])\n",
    "\n",
    "# notice data is skewed to Action movie genre\n",
    "# Maybe that's why Action movies are predicted more accurately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.b Training and testing the \"fake news\" model with CountVectorizer\n",
    "Now it's your turn to train the \"fake news\" model using the features you identified and extracted. In this first exercise you'll train and test a Naive Bayes model using the CountVectorizer data.\n",
    "\n",
    "The training and test sets have been created, and count_vectorizer, count_train, and count_test have been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, \n",
    "                              labels=['FAKE','REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output:\n",
    "#     0.893352462936394\n",
    "#     [[ 865  143]\n",
    "#      [  80 1003]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.c Training and testing the \"fake news\" model with TfidfVectorizer\n",
    "Now that you have evaluated the model using the CountVectorizer, you'll do the same using the TfidfVectorizer with a Naive Bayes model.\n",
    "\n",
    "The training and test sets have been created, and tfidf_vectorizer, tfidf_train, and tfidf_test have been computed. Additionally, MultinomialNB and metrics have been imported from, respectively, sklearn.naive_bayes and sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, \n",
    "                              labels=['FAKE','REAL'])\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output:\n",
    "#     0.8565279770444764\n",
    "#     [[ 739  269]\n",
    "#      [  31 1052]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Simple NLP, Complex Problems\n",
    "Translation\n",
    "- a work in progress\n",
    "- example: German has many related words to economics that in English the vectors point to 'economics'\n",
    "\n",
    "Sentiment Analysis\n",
    "- complex problems with snark, sarcasm\n",
    "- difficulty with negation\n",
    "    - ie. I liked it, but it could have been better\n",
    "- separate communities may use the same words differently\n",
    "    - ie. https://nlp.stanford.edu/projects/socialsent/\n",
    "    - 2 different reddit communities: TwoX for women and Sports\n",
    "\n",
    "Language Biases\n",
    "- prejudices in text\n",
    "- English to Turkish to English\n",
    "    - She and He can get switched\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Ways to improve the model\n",
    "1. Tweaking alpha levels\n",
    "2. Trying a new classification model\n",
    "3. Training on a larger dataset\n",
    "4. Improving text preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Improving your model - hyperparameter alpha\n",
    "Your job in this exercise is to test a few different alpha levels using the Tfidf vectors to determine if there is a better performing combination.\n",
    "\n",
    "The training and test sets have been created, and tfidf_vectorizer, tfidf_train, and tfidf_test have been computed.\n",
    "\n",
    "Uses a function to check accuracy for different alpha values\n",
    "- try GridSearch instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0,1,0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alpha:  0.0\n",
    "Score:  0.8813964610234337\n",
    "\n",
    "Alpha:  0.1\n",
    "Score:  0.8976566236250598\n",
    "\n",
    "Alpha:  0.2\n",
    "Score:  0.8938307030129125\n",
    "\n",
    "Alpha:  0.30000000000000004\n",
    "Score:  0.8900047824007652\n",
    "\n",
    "Alpha:  0.4\n",
    "Score:  0.8857006217120995\n",
    "\n",
    "Alpha:  0.5\n",
    "Score:  0.8842659014825442\n",
    "\n",
    "Alpha:  0.6000000000000001\n",
    "Score:  0.874701099952176\n",
    "\n",
    "Alpha:  0.7000000000000001\n",
    "Score:  0.8703969392635102\n",
    "\n",
    "Alpha:  0.8\n",
    "Score:  0.8660927785748446\n",
    "\n",
    "Alpha:  0.9\n",
    "Score:  0.8589191774270684"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Inspecting your model\n",
    "Now that you have built a \"fake news\" classifier, you'll investigate what it has learned. You can map the important vector weights back to actual words using some simple inspection techniques.\n",
    "\n",
    "You have your well performing tfidf Naive Bayes classifier available as nb_classifier, and the vectors as tfidf_vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAKE [(-12.641778440826338, '0000'), \n",
    "      (-12.641778440826338, '000035'), \n",
    "      (-12.641778440826338, '0001'), \n",
    "      (-12.641778440826338, '0001pt'), \n",
    "      (-12.641778440826338, '000km'), \n",
    "      (-12.641778440826338, '0011'), \n",
    "      (-12.641778440826338, '006s'), \n",
    "      (-12.641778440826338, '007'), \n",
    "      (-12.641778440826338, '007s'), \n",
    "      (-12.641778440826338, '008s'), \n",
    "      (-12.641778440826338, '0099'), \n",
    "      (-12.641778440826338, '00am'), \n",
    "      (-12.641778440826338, '00p'), \n",
    "      (-12.641778440826338, '00pm'), \n",
    "      (-12.641778440826338, '014'), \n",
    "      (-12.641778440826338, '015'), \n",
    "      (-12.641778440826338, '018'), \n",
    "      (-12.641778440826338, '01am'), \n",
    "      (-12.641778440826338, '020'), \n",
    "      (-12.641778440826338, '023')]\n",
    "REAL [(-6.790929954967984, 'states'), \n",
    "      (-6.765360557845786, 'rubio'), \n",
    "      (-6.751044290367751, 'voters'), \n",
    "      (-6.701050756752027, 'house'), \n",
    "      (-6.695547793099875, 'republicans'), \n",
    "      (-6.6701912490429685, 'bush'), \n",
    "      (-6.661945235816139, 'percent'), \n",
    "      (-6.589623788689862, 'people'), \n",
    "      (-6.559670340096453, 'new'), \n",
    "      (-6.489892292073901, 'party'), \n",
    "      (-6.452319082422527, 'cruz'), \n",
    "      (-6.452076515575875, 'state'), \n",
    "      (-6.397696648238072, 'republican'), \n",
    "      (-6.376343060363355, 'campaign'), \n",
    "      (-6.324397735392007, 'president'), \n",
    "      (-6.2546017970213645, 'sanders'), \n",
    "      (-6.144621899738043, 'obama'), \n",
    "      (-5.756817248152807, 'clinton'), \n",
    "      (-5.596085785733112, 'said'), \n",
    "      (-5.357523914504495, 'trump')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
