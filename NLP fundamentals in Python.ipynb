{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP fundamentals in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataCamp with Katharine Jarmul - founder of kjamistan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP (natural language processing)\n",
    "\n",
    "NLP basics:\n",
    "- topic identification\n",
    "- text classification\n",
    "\n",
    "NLP applications:\n",
    "- chatbots\n",
    "- translation\n",
    "- sentiment analysis\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images\\sqlalchemy1.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regular expressions & word tokenization\n",
    "- parsing text\n",
    "- handle non-English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# match pattern on string\n",
    "re.match('abc', 'abcdef')\n",
    "\n",
    "# match word\n",
    "word_regex = '\\w+'\n",
    "re.match(word_regex, 'hi there!')\n",
    "\n",
    "# split on spaces for tokenization\n",
    "re.split('\\s+', 'Split on spaces.')\n",
    "# ['Split', 'on', 'spaces.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Regex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.a Common Regex patterns\n",
    "- \\w+ matches word\n",
    "- \\d matches digit\n",
    "- \\s matches space\n",
    "- .* wildcard\n",
    "- # + or * greedy match\n",
    "- \\S matches 'not space'\n",
    "- [a-z] lowercase group using brackets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.b re package/module\n",
    "- split - split a string on regex\n",
    "- findall - find all patterns in a string\n",
    "- search - search for a pattern\n",
    "- match - match an entire string or substring based on a pattern\n",
    "\n",
    "Notes:\n",
    "- pass the pattern first and string second\n",
    "- may return an iterator, string, or match object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.b Which pattern?\n",
    "Which of the following Regex patterns results in the following text?\n",
    "- my_string = \"Let's write RegEx!\"\n",
    "- re.findall(PATTERN, my_string)\n",
    "- ['Let', 's', 'write', 'RegEx']\n",
    "\n",
    "In the IPython Shell, try replacing PATTERN with one of the below options and observe the resulting output. The re module has been pre-imported for you and my_string is available in your namespace. \n",
    "\n",
    "Answer:\n",
    "- re.findall(r\"\\w+\", my_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.c Practicing regular expressions: re.split() and re.findall()\n",
    "Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.\n",
    "\n",
    "Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, \"\\n\" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string \"\\n\" - that is, the character \"\\\" followed by the character \"n\" - and not as a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regex module\n",
    "import re\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "# match sentence endings (., ?, and !)\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tokenization\n",
    "What is tokenization?\n",
    "- turning a string or document into tokens (smaller chunks)\n",
    "- one step in preparing a text for NLP\n",
    "- many theories and rules\n",
    "- you can create your own rules using regex\n",
    "- Examples:\n",
    "    - breaking out words or sentences\n",
    "    - separating punctuation\n",
    "    - separating all hashtags in a tweet\n",
    "    \n",
    "nltk library = natural language toolkit\n",
    "\n",
    "Reasons to tokenize?\n",
    "- easier to map part of speech\n",
    "- matching common words\n",
    "- removing unwanted tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.a nltk library\n",
    "some nltk tokenizers\n",
    "- sent_tokenize: tokenize a document into sentences\n",
    "- regexp_tokenize: tokenize a string or document based on a regex pattern\n",
    "- TweetTokenizer: special class just for tweet tokenization, allowing you to separate hashtags, mentions, and lots of exclamation points!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# separate words and punctuation\n",
    "work_tokenize(\"Hi there!\")\n",
    "# output\n",
    "# ['Hi', 'there', '!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.b Difference b/n re.search() and re.match()\n",
    "- match starts from beginning of string\n",
    "\n",
    "Follow this rule:\n",
    "- if you look for a pattern that might not be at the beginning of a string, use search()\n",
    "- if you want the entire string or just the beginning, use match()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# same results\n",
    "rematch('abc', 'abcde')\n",
    "re.search('abc', 'abcde')\n",
    "\n",
    "# different results\n",
    "re.match('cd', 'abcde')\n",
    "# no result\n",
    "re.search('cd', 'abcde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.c Word tokenization with NLTK\n",
    "Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as scene_one. Feel free to check it out in the IPython Shell!\n",
    "\n",
    "Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T20:07:35.232289Z",
     "start_time": "2018-11-19T20:07:33.742167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "\n",
    "# Print the unique tokens result\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.d More regex with re.search()\n",
    "In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.\n",
    "\n",
    "You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Advanced Tokenization with Regex\n",
    "Regex groups using or \"|\" character\n",
    "- OR is represented using |\n",
    "- you can define a group using ()\n",
    "- define explicit character ranges using []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: all digitis or words\n",
    "import re\n",
    "match_digits_and_words = ('\\d+|\\w+')\n",
    "re.findall(match_digits_and_words, 'He has 11 cats.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex ranges and groups\n",
    "- Pattern, matches, example\n",
    "- [A-Za-z]+, upper and lowercase English, 'ABCDEFghijk'\n",
    "- [0-9], numbers 0-9, 9\n",
    "- [A-Za-z\\.\\]+m, upper and lowercase English alphabet,\n",
    "- (a-z), a, - and z, 'a-z'\n",
    "- (\\s+|,), spaces or comma, ',\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character range with re.match()\n",
    "import re\n",
    "my_str = 'match lowercase spaces nums like 12, but no comma'\n",
    "re.match('[a-z0-9 ]+', my_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.a Choosing a tokenizer\n",
    "Given the following string, which of the below patterns is the best tokenizer? If possible, you want to retain sentence punctuation as separate tokens, but have '#1' remain a single token.\n",
    "- my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\"\n",
    "\n",
    "The string is available in your workspace as my_string, and the patterns have been pre-loaded as pattern1, pattern2, pattern3, and pattern4, respectively.\n",
    "- pattern1 = r\"\\w+(\\?!)\"\n",
    "- pattern2 = r\"(\\w+|#\\d|\\?|!)\"\n",
    "- pattern3 = r\"(#\\d\\w+\\?!)\"\n",
    "- pattern4 = r\"\\s+\"\n",
    "\n",
    "Additionally, regexp_tokenize has been imported from nltk.tokenize. You can use regexp_tokenize() with my_string and one of the patterns as arguments to experiment for yourself and see which is the best tokenizer.\n",
    "- regexp_tokenize(string, pattern)\n",
    "\n",
    "Answer: pattern 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "In [11]: regexp_tokenize(pattern1, my_string)\n",
    "Out[11]: []\n",
    "\n",
    "In [12]: regexp_tokenize(my_string, pattern1)\n",
    "Out[12]: []\n",
    "\n",
    "In [13]: regexp_tokenize(my_string, pattern2)\n",
    "Out[13]: \n",
    "['SOLDIER',\n",
    " '#1',\n",
    " 'Found',\n",
    " 'them',\n",
    " '?',\n",
    " 'In',\n",
    " 'Mercea',\n",
    " '?',\n",
    " 'The',\n",
    " 'coconut',\n",
    " 's',\n",
    " 'tropical',\n",
    " '!']\n",
    "\n",
    "In [14]: regexp_tokenize(my_string, pattern3)\n",
    "Out[14]: []\n",
    "\n",
    "In [15]: regexp_tokenize(my_string, pattern4)\n",
    "Out[15]: [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.b Regex with NLTK tokenization\n",
    "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets.\n",
    "\n",
    "Here, you're given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "regexp_tokenize(tweets[0], pattern1)\n",
    "\n",
    "# Write a pattern that matches both mentions and hashtags\n",
    "# mention example: @example\n",
    "pattern2 = r\"([@|#]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[-1], pattern2)\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.c Non-ascii tokenization\n",
    "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
    "\n",
    "Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji and the German characters!\n",
    "\n",
    "The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.\n",
    "\n",
    "Unicode ranges for emoji are:\n",
    "- ('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-Z|Ü]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'| \\\n",
    "'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕\n",
    "\n",
    "In [1]: all_words = word_tokenize(german_text)\n",
    "        print(all_words)\n",
    "- ['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n",
    "\n",
    "\n",
    "In [2]: capital_words = r\"[A-Z|Ü]\\w+\"\n",
    "        print(regexp_tokenize(german_text, capital_words))\n",
    "- ['Wann', 'Pizza', 'Und', 'Über']\n",
    "\n",
    "In [3]: emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "        print(regexp_tokenize(german_text, emoji))\n",
    "- ['🍕', '🚕']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Charting word length with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T00:24:28.219699Z",
     "start_time": "2018-11-20T00:24:27.968283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAD2RJREFUeJzt3VuMXWd9hvHnxTaHhEOqeFpcx2aoiFABFZKOQmikKCJQJSRKegiSI5WTqFyh0CYtUhW4CIIrkCqoIIjIJSkJTQM0CcgFc0gFFLiIYWycg2NQXRrIkLQ2BBxcDsH034u9XE3H2957ZvZkjT+en7TldfhmrTejyTtrvllrT6oKSVJbntR3AEnS5FnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAat7evE69evr+np6b5OL0knpV27dn2/qqZGjeut3Kenp5mdne3r9JJ0UkrynXHGOS0jSQ2y3CWpQZa7JDXIcpekBlnuktSgkeWe5KlJvpbkniR7k7xjyJinJPlYkv1JdiaZXomwkqTxjHPl/nPg5VX1YuAlwEVJzl0w5o3AD6vqecB7gXdPNqYkaTFGlnsNHO5W13WvhX+b73Lg5m75duDCJJlYSknSoow1555kTZI9wAHgrqrauWDIRuAhgKo6AhwCTp9kUEnS+MZ6QrWqfgm8JMlpwCeSvKiq7p83ZNhV+jF/eTvJVmArwObNm5cQV9JKmL72072d+8F3XdLbuVu2qLtlqupHwJeAixbsmgM2ASRZCzwLeHTIx2+rqpmqmpmaGvnWCJKkJRrnbpmp7oqdJE8DXgF8c8Gw7cDruuUrgC9U1TFX7pKkJ8Y40zIbgJuTrGHwzeDjVfWpJO8EZqtqO3Aj8JEk+xlcsW9ZscSSpJFGlntV3QucNWT7dfOWfwa8erLRJElL5ROqktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGjSy3JNsSvLFJPuS7E1y9ZAxFyQ5lGRP97puZeJKksaxdowxR4C3VNXuJM8AdiW5q6oeWDDuK1V16eQjSpIWa+SVe1U9UlW7u+UfA/uAjSsdTJK0dIuac08yDZwF7Byy+2VJ7knymSQvPM7Hb00ym2T24MGDiw4rSRrP2OWe5OnAHcA1VfXYgt27gedU1YuB9wOfHHaMqtpWVTNVNTM1NbXUzJKkEcYq9yTrGBT7rVV158L9VfVYVR3ulncA65Ksn2hSSdLYxrlbJsCNwL6qes9xxjy7G0eSc7rj/mCSQSVJ4xvnbpnzgNcA9yXZ0217G7AZoKpuAK4A3pTkCPBTYEtV1QrklSSNYWS5V9VXgYwYcz1w/aRCSZKWxydUJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBI8s9yaYkX0yyL8neJFcPGZMk70uyP8m9Sc5embiSpHGsHWPMEeAtVbU7yTOAXUnuqqoH5o25GDize70U+GD3rySpByOv3Kvqkara3S3/GNgHbFww7HLglhq4GzgtyYaJp5UkjWVRc+5JpoGzgJ0Ldm0EHpq3Psex3wAkSU+QcaZlAEjydOAO4Jqqemzh7iEfUkOOsRXYCrB58+ZFxJSeWNPXfrqX8z74rkt6Oa/aM9aVe5J1DIr91qq6c8iQOWDTvPUzgIcXDqqqbVU1U1UzU1NTS8krSRrDOHfLBLgR2FdV7znOsO3Aa7u7Zs4FDlXVIxPMKUlahHGmZc4DXgPcl2RPt+1twGaAqroB2AG8CtgP/AR4w+SjSpLGNbLcq+qrDJ9Tnz+mgKsmFUqStDw+oSpJDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBI8s9yU1JDiS5/zj7L0hyKMme7nXd5GNKkhZj7RhjPgxcD9xygjFfqapLJ5JIkrRsI6/cq+rLwKNPQBZJ0oRMas79ZUnuSfKZJC883qAkW5PMJpk9ePDghE4tSVpoEuW+G3hOVb0YeD/wyeMNrKptVTVTVTNTU1MTOLUkaZhll3tVPVZVh7vlHcC6JOuXnUyStGTLLvckz06Sbvmc7pg/WO5xJUlLN/JumSS3ARcA65PMAW8H1gFU1Q3AFcCbkhwBfgpsqapascSSpJFGlntVXTli//UMbpWUJK0SPqEqSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ0aWe5JbkpyIMn9x9mfJO9Lsj/JvUnOnnxMSdJijHPl/mHgohPsvxg4s3ttBT64/FiSpOUYWe5V9WXg0RMMuRy4pQbuBk5LsmFSASVJizeJOfeNwEPz1ue6bZKknqydwDEyZFsNHZhsZTB1w+bNm5d8wulrP73kj12uB991SW/nljQ5rffIJK7c54BN89bPAB4eNrCqtlXVTFXNTE1NTeDUkqRhJlHu24HXdnfNnAscqqpHJnBcSdISjZyWSXIbcAGwPskc8HZgHUBV3QDsAF4F7Ad+ArxhpcJKksYzstyr6soR+wu4amKJJEnL5hOqktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGjRWuSe5KMm3kuxPcu2Q/a9PcjDJnu71p5OPKkka19pRA5KsAT4AvBKYA76eZHtVPbBg6Meq6s0rkFGStEjjXLmfA+yvqm9X1ePAR4HLVzaWJGk5xin3jcBD89bnum0L/XGSe5PcnmTTsAMl2ZpkNsnswYMHlxBXkjSOcco9Q7bVgvV/Bqar6neAfwFuHnagqtpWVTNVNTM1NbW4pJKksY1T7nPA/CvxM4CH5w+oqh9U1c+71b8Dfncy8SRJSzFOuX8dODPJc5M8GdgCbJ8/IMmGeauXAfsmF1GStFgj75apqiNJ3gx8DlgD3FRVe5O8E5itqu3AXyS5DDgCPAq8fgUzS5JGGFnuAFW1A9ixYNt185bfCrx1stEkSUvlE6qS1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1KCxyj3JRUm+lWR/kmuH7H9Kko91+3cmmZ50UEnS+EaWe5I1wAeAi4EXAFcmecGCYW8EflhVzwPeC7x70kElSeMb58r9HGB/VX27qh4HPgpcvmDM5cDN3fLtwIVJMrmYkqTFGKfcNwIPzVuf67YNHVNVR4BDwOmTCChJWry1Y4wZdgVeSxhDkq3A1m71cJJvjXH+YdYD31/ixy5LTjzh1FuuMazWbOaaZ8TXFzT4+Rrjv3k5VuXnK+9eVq7njDNonHKfAzbNWz8DePg4Y+aSrAWeBTy68EBVtQ3YNk6wE0kyW1Uzyz3OpK3WXLB6s5lrccy1OL/KucaZlvk6cGaS5yZ5MrAF2L5gzHbgdd3yFcAXquqYK3dJ0hNj5JV7VR1J8mbgc8Aa4Kaq2pvkncBsVW0HbgQ+kmQ/gyv2LSsZWpJ0YuNMy1BVO4AdC7ZdN2/5Z8CrJxvthJY9tbNCVmsuWL3ZzLU45lqcX9lccfZEktrj2w9IUoNOqnJPclOSA0nu7zvLfEk2Jflikn1J9ia5uu9MAEmemuRrSe7pcr2j70zzJVmT5BtJPtV3lqOSPJjkviR7ksz2neeoJKcluT3JN7uvs5etgkzP7z5PR1+PJbmm71wASf6y+5q/P8ltSZ7adyaAJFd3mfau9OfqpJqWSXI+cBi4pape1Heeo5JsADZU1e4kzwB2AX9QVQ/0nCvAqVV1OMk64KvA1VV1d5+5jkryV8AM8MyqurTvPDAod2CmqlbVvdFJbga+UlUf6u5aO6WqftR3rqO6tyn5HvDSqvpOz1k2Mvhaf0FV/TTJx4EdVfXhnnO9iMET/ucAjwOfBd5UVf+2Euc7qa7cq+rLDLl/vm9V9UhV7e6Wfwzs49ineJ9wNXC4W13XvVbFd/MkZwCXAB/qO8tql+SZwPkM7kqjqh5fTcXeuRD4976LfZ61wNO6525O4dhnc/rw28DdVfWT7kn+fwX+cKVOdlKV+8mge0fMs4Cd/SYZ6KY+9gAHgLuqalXkAv4W+Gvgf/oOskABn0+yq3uiejX4LeAg8PfdNNaHkpzad6gFtgC39R0CoKq+B/wN8F3gEeBQVX2+31QA3A+cn+T0JKcAr+L/PyA6UZb7BCV5OnAHcE1VPdZ3HoCq+mVVvYTBk8XndD8a9irJpcCBqtrVd5Yhzquqsxm8C+pV3VRg39YCZwMfrKqzgP8Gjnnr7b5000SXAf/UdxaAJL/G4M0Mnwv8JnBqkj/pNxVU1T4G75h7F4MpmXuAIyt1Pst9Qro57TuAW6vqzr7zLNT9GP8l4KKeowCcB1zWzW9/FHh5kn/oN9JAVT3c/XsA+ASD+dG+zQFz837qup1B2a8WFwO7q+q/+g7SeQXwH1V1sKp+AdwJ/F7PmQCoqhur6uyqOp/BFPOKzLeD5T4R3S8ubwT2VdV7+s5zVJKpJKd1y09j8EX/zX5TQVW9tarOqKppBj/Of6Gqer+ySnJq9wtxummP32fwo3Svquo/gYeSPL/bdCHQ6y/rF7iSVTIl0/kucG6SU7r/Ny9k8Huw3iX59e7fzcAfsYKft7GeUF0tktwGXACsTzIHvL2qbuw3FTC4En0NcF83vw3wtu7J3j5tAG7u7mR4EvDxqlo1tx2uQr8BfKL7UwRrgX+sqs/2G+n//DlwazcF8m3gDT3nAaCbO34l8Gd9ZzmqqnYmuR3YzWDa4xusnidV70hyOvAL4Kqq+uFKneikuhVSkjQep2UkqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDfpf0qsOrJmX27YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a13b65240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example histogram with matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# load arry and plot histogram\n",
    "plt.hist([1,5,5,7,7,7,9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.a Combining NLP data extraction with plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T00:49:50.240529Z",
     "start_time": "2018-11-20T00:35:39.094268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package qc to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package rslp to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/joe/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T00:53:26.504814Z",
     "start_time": "2018-11-20T00:53:26.384677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1778afd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(\"This is a pretty cool tool!\")\n",
    "# make a list of word lengths\n",
    "word_lengths = [len(w) for w in words]\n",
    "\n",
    "plt.hist(word_lengths)\n",
    "plt.show()\n",
    "\n",
    "# see 4 letter words is the most common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.a Charting practice\n",
    "Try using your new skills to find and chart the number of words per line in the script using matplotlib. The Holy Grail script is loaded for you, and you need to use regex to find the words per line.\n",
    "\n",
    "Using list comprehensions here will speed up your computations. For example: my_lines = [tokenize(l) for l in lines] will call a function tokenize on each line in the list lines. The new transformed list will be saved in the my_lines variable.\n",
    "\n",
    "You have access to the entire script in the variable holy_grail. Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T00:53:30.896938Z",
     "start_time": "2018-11-20T00:53:30.869581Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the script into lines: lines\n",
    "lines = holy_grail.split('\\n')\n",
    "\n",
    "# Replace all script lines for speaker (ie. remove ARTHUR:)\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Tokenize each line: tokenized_lines\n",
    "# Keep only words in each line\n",
    "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot (see below)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-20T00:53:43.635598Z",
     "start_time": "2018-11-20T00:53:43.607540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"428pt\" version=\"1.1\" viewBox=\"0 0 270 428\" width=\"270pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 428  L 270 428  L 270 0  L 0 0  z \" style=\"fill:#ffffff;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 36.8875 400.521875  L 255.7 400.521875  L 255.7 14.3  L 36.8875 14.3  z \" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 46.833523 400.521875  L 66.725568 400.521875  L 66.725568 32.691518  L 46.833523 32.691518  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 66.725568 400.521875  L 86.617614 400.521875  L 86.617614 329.445485  L 66.725568 329.445485  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 86.617614 400.521875  L 106.509659 400.521875  L 106.509659 379.640676  L 86.617614 379.640676  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 106.509659 400.521875  L 126.401705 400.521875  L 126.401705 391.687521  L 106.509659 391.687521  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 126.401705 400.521875  L 146.29375 400.521875  L 146.29375 396.907821  L 126.401705 396.907821  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 146.29375 400.521875  L 166.185795 400.521875  L 166.185795 398.915629  L 146.29375 398.915629  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 166.185795 400.521875  L 186.077841 400.521875  L 186.077841 398.915629  L 166.185795 398.915629  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 186.077841 400.521875  L 205.969886 400.521875  L 205.969886 398.514067  L 186.077841 398.514067  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 205.969886 400.521875  L 225.861932 400.521875  L 225.861932 400.120313  L 205.969886 400.120313  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path clip-path=\"url(#p231c1a4d4a)\" d=\"M 225.861932 400.521875  L 245.753977 400.521875  L 245.753977 399.718752  L 225.861932 399.718752  z \" style=\"fill:#1f77b4;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0  L 0 3.5  \" id=\"m22b2467620\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.833523\" xlink:href=\"#m22b2467620\" y=\"400.521875\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625  Q 24.171875 66.40625 20.328125 58.90625  Q 16.5 51.421875 16.5 36.375  Q 16.5 21.390625 20.328125 13.890625  Q 24.171875 6.390625 31.78125 6.390625  Q 39.453125 6.390625 43.28125 13.890625  Q 47.125 21.390625 47.125 36.375  Q 47.125 51.421875 43.28125 58.90625  Q 39.453125 66.40625 31.78125 66.40625  z M 31.78125 74.21875  Q 44.046875 74.21875 50.515625 64.515625  Q 56.984375 54.828125 56.984375 36.375  Q 56.984375 17.96875 50.515625 8.265625  Q 44.046875 -1.421875 31.78125 -1.421875  Q 19.53125 -1.421875 13.0625 8.265625  Q 6.59375 17.96875 6.59375 36.375  Q 6.59375 54.828125 13.0625 64.515625  Q 19.53125 74.21875 31.78125 74.21875  z \" id=\"DejaVuSans-30\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(43.652273 415.120313)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"85.458854\" xlink:href=\"#m22b2467620\" y=\"400.521875\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 20 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875  L 53.609375 8.296875  L 53.609375 0  L 7.328125 0  L 7.328125 8.296875  Q 12.9375 14.109375 22.625 23.890625  Q 32.328125 33.6875 34.8125 36.53125  Q 39.546875 41.84375 41.421875 45.53125  Q 43.3125 49.21875 43.3125 52.78125  Q 43.3125 58.59375 39.234375 62.25  Q 35.15625 65.921875 28.609375 65.921875  Q 23.96875 65.921875 18.8125 64.3125  Q 13.671875 62.703125 7.8125 59.421875  L 7.8125 69.390625  Q 13.765625 71.78125 18.9375 73  Q 24.125 74.21875 28.421875 74.21875  Q 39.75 74.21875 46.484375 68.546875  Q 53.21875 62.890625 53.21875 53.421875  Q 53.21875 48.921875 51.53125 44.890625  Q 49.859375 40.875 45.40625 35.40625  Q 44.1875 33.984375 37.640625 27.21875  Q 31.109375 20.453125 19.1875 8.296875  z \" id=\"DejaVuSans-32\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(79.096354 415.120313)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"124.084185\" xlink:href=\"#m22b2467620\" y=\"400.521875\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 40 -->\n",
       "      <defs>\n",
       "       <path d=\"M 37.796875 64.3125  L 12.890625 25.390625  L 37.796875 25.390625  z M 35.203125 72.90625  L 47.609375 72.90625  L 47.609375 25.390625  L 58.015625 25.390625  L 58.015625 17.1875  L 47.609375 17.1875  L 47.609375 0  L 37.796875 0  L 37.796875 17.1875  L 4.890625 17.1875  L 4.890625 26.703125  z \" id=\"DejaVuSans-34\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(117.721685 415.120313)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"162.709516\" xlink:href=\"#m22b2467620\" y=\"400.521875\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 60 -->\n",
       "      <defs>\n",
       "       <path d=\"M 33.015625 40.375  Q 26.375 40.375 22.484375 35.828125  Q 18.609375 31.296875 18.609375 23.390625  Q 18.609375 15.53125 22.484375 10.953125  Q 26.375 6.390625 33.015625 6.390625  Q 39.65625 6.390625 43.53125 10.953125  Q 47.40625 15.53125 47.40625 23.390625  Q 47.40625 31.296875 43.53125 35.828125  Q 39.65625 40.375 33.015625 40.375  z M 52.59375 71.296875  L 52.59375 62.3125  Q 48.875 64.0625 45.09375 64.984375  Q 41.3125 65.921875 37.59375 65.921875  Q 27.828125 65.921875 22.671875 59.328125  Q 17.53125 52.734375 16.796875 39.40625  Q 19.671875 43.65625 24.015625 45.921875  Q 28.375 48.1875 33.59375 48.1875  Q 44.578125 48.1875 50.953125 41.515625  Q 57.328125 34.859375 57.328125 23.390625  Q 57.328125 12.15625 50.6875 5.359375  Q 44.046875 -1.421875 33.015625 -1.421875  Q 20.359375 -1.421875 13.671875 8.265625  Q 6.984375 17.96875 6.984375 36.375  Q 6.984375 53.65625 15.1875 63.9375  Q 23.390625 74.21875 37.203125 74.21875  Q 40.921875 74.21875 44.703125 73.484375  Q 48.484375 72.75 52.59375 71.296875  z \" id=\"DejaVuSans-36\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(156.347016 415.120313)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"201.334847\" xlink:href=\"#m22b2467620\" y=\"400.521875\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 80 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 34.625  Q 24.75 34.625 20.71875 30.859375  Q 16.703125 27.09375 16.703125 20.515625  Q 16.703125 13.921875 20.71875 10.15625  Q 24.75 6.390625 31.78125 6.390625  Q 38.8125 6.390625 42.859375 10.171875  Q 46.921875 13.96875 46.921875 20.515625  Q 46.921875 27.09375 42.890625 30.859375  Q 38.875 34.625 31.78125 34.625  z M 21.921875 38.8125  Q 15.578125 40.375 12.03125 44.71875  Q 8.5 49.078125 8.5 55.328125  Q 8.5 64.0625 14.71875 69.140625  Q 20.953125 74.21875 31.78125 74.21875  Q 42.671875 74.21875 48.875 69.140625  Q 55.078125 64.0625 55.078125 55.328125  Q 55.078125 49.078125 51.53125 44.71875  Q 48 40.375 41.703125 38.8125  Q 48.828125 37.15625 52.796875 32.3125  Q 56.78125 27.484375 56.78125 20.515625  Q 56.78125 9.90625 50.3125 4.234375  Q 43.84375 -1.421875 31.78125 -1.421875  Q 19.734375 -1.421875 13.25 4.234375  Q 6.78125 9.90625 6.78125 20.515625  Q 6.78125 27.484375 10.78125 32.3125  Q 14.796875 37.15625 21.921875 38.8125  z M 18.3125 54.390625  Q 18.3125 48.734375 21.84375 45.5625  Q 25.390625 42.390625 31.78125 42.390625  Q 38.140625 42.390625 41.71875 45.5625  Q 45.3125 48.734375 45.3125 54.390625  Q 45.3125 60.0625 41.71875 63.234375  Q 38.140625 66.40625 31.78125 66.40625  Q 25.390625 66.40625 21.84375 63.234375  Q 18.3125 60.0625 18.3125 54.390625  z \" id=\"DejaVuSans-38\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(194.972347 415.120313)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"239.960178\" xlink:href=\"#m22b2467620\" y=\"400.521875\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 100 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875  L 28.515625 8.296875  L 28.515625 63.921875  L 10.984375 60.40625  L 10.984375 69.390625  L 28.421875 72.90625  L 38.28125 72.90625  L 38.28125 8.296875  L 54.390625 8.296875  L 54.390625 0  L 12.40625 0  z \" id=\"DejaVuSans-31\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(230.416428 415.120313)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0  L -3.5 0  \" id=\"ma0e477f1b2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.8875\" xlink:href=\"#ma0e477f1b2\" y=\"400.521875\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(23.525 404.321094)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.8875\" xlink:href=\"#ma0e477f1b2\" y=\"320.20957\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(10.8 324.008789)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.8875\" xlink:href=\"#ma0e477f1b2\" y=\"239.897265\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(10.8 243.696484)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.8875\" xlink:href=\"#ma0e477f1b2\" y=\"159.58496\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 600 -->\n",
       "      <g transform=\"translate(10.8 163.384179)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.8875\" xlink:href=\"#ma0e477f1b2\" y=\"79.272655\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 800 -->\n",
       "      <g transform=\"translate(10.8 83.071874)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 36.8875 400.521875  L 36.8875 14.3  \" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 255.7 400.521875  L 255.7 14.3  \" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 36.8875 400.521875  L 255.7 400.521875  \" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 36.8875 14.3  L 255.7 14.3  \" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p231c1a4d4a\">\n",
       "   <rect height=\"386.221875\" width=\"218.8125\" x=\"36.8875\" y=\"14.3\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image, SVG\n",
    "SVG(filename='Images/nlp_linelength.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simple topic identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Named-entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Building a \"fake news\" classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
